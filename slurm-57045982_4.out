Job Id listed below:
57045982

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

/var/spool/slurmd.spool/job57045982/slurm_script: line 36: regret_lambda: command not found
2023-08-01 23:52:39.360842: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-01 23:52:45.514299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0801 23:52:58.916460 22927420767104 rl_environment.py:187] Using game instance: bargaining
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 32
discount_factor: 0.99
double: True
epsilon_decay_duration: 1000
hidden_layers_sizes: [50, 50]
learn_every: 10
learning_rate: 0.01
min_buffer_size_to_learn: 1000
num_actions: 121
optimizer_str: adam
replay_buffer_capacity: 10000
session: <tensorflow.python.client.session.Session object at 0x14d9eb862d70>
state_representation_size: 93
symmetric: True
update_target_network_every: 1000


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.8
epochs_ppo: 5
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 20000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 1000
regret_calculation_steps: 200000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x14d9eb862d70>
sims_per_entry: 5000
state_representation_size: 93
steps_fine_tune: 2000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-08-01 23:52:59.489135: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-08-01 23:53:00.117211: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1079 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 5000 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 2.69 seconds to finish estimate with resulting utilities: [1.9416 1.9622]
Exited RRD with total regret 0.0 that was less than regret lambda 0.5 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0     1.95      

 

Player 1 Payoff matrix: 

           0      
    0     1.95      

 

Social Welfare Sum Matrix: 

           0      
    0     3.90      

 

Iteration : 0
Time so far: 0.00017881393432617188
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-08-01 23:53:03.899713: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1941 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.24566122591495515 8.889286518096924 4.781264400482177 0.0004929984497721307 20004 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.22082584784144446 9.874437786283947 4.647232378096808 0.15414047864220004 420040 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.18455611965278299 8.425616138737377 4.4200056145830855 0.3769834092404713 820077 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.15152398143879703 7.53872302321137 4.220011821340342 0.6769860364325663 1220121 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.12261537980334258 7.0705553490438575 4.062899991612375 1.0763797135227957 1620161 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.10108720200634239 6.838914830141729 3.92463116409755 1.548781122957713 2020199 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08669911195362402 6.708451561494307 3.7976165913353284 2.0524780418729955 2420254 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0763061912664285 6.670372102277499 3.6850167017456488 2.555614336456388 2820293 0


Pure best response payoff estimated to be 7.4888 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 6.14 seconds to finish estimate with resulting utilities: [7.3508 2.9476]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 9.71 seconds to finish estimate with resulting utilities: [4.5214 4.9792]
Computing meta_strategies
Exited RRD with total regret 0.4991793222504768 that was less than regret lambda 0.5 after 749 iterations 
NEW LAMBDA 0.4827586206896552
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0     1.95           2.95      
    1     7.35           4.75      

 

Player 1 Payoff matrix: 

           0              1      
    0     1.95           7.35      
    1     2.95           4.75      

 

Social Welfare Sum Matrix: 

           0              1      
    0     3.90          10.30      
    1    10.30           9.50      

 

Metagame probabilities: 
Player #0: 0.113  0.887  
Player #1: 0.113  0.887  
Iteration : 1
Time so far: 5563.2436509132385
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-08-02 01:25:47.275350: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3851 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07702442731865695 6.830236269386721 3.6807238971683343 2.5747698236078422 20003 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09010994661553406 7.404187349331232 3.6664221475153793 2.8524251469473025 420041 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09452492703612034 7.423724433615967 3.661109123125181 3.106390463878491 820092 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09320102866259541 7.452877061437852 3.6514305020322895 3.352744211741897 1220136 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08782462539856095 7.515804686632242 3.6318958621841295 3.5857268605938355 1620187 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08160068831725954 7.5576891975954545 3.5854862041709836 3.817975846375068 2020241 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07588910926340009 7.593323955645088 3.4682978253328165 4.068964431280169 2420292 0


Pure best response payoff estimated to be 6.5876 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 6.09 seconds to finish estimate with resulting utilities: [7.3282 3.0786]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 10.03 seconds to finish estimate with resulting utilities: [6.3074 4.2228]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 11.9 seconds to finish estimate with resulting utilities: [4.3886 4.0424]
Computing meta_strategies
Exited RRD with total regret 0.48245002147226934 that was less than regret lambda 0.4827586206896552 after 1157 iterations 
NEW LAMBDA 0.4655172413793104
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0     1.95           2.95           3.08      
    1     7.35           4.75           4.22      
    2     7.33           6.31           4.22      

 

Player 1 Payoff matrix: 

           0              1              2      
    0     1.95           7.35           7.33      
    1     2.95           4.75           6.31      
    2     3.08           4.22           4.22      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0     3.90          10.30          10.41      
    1    10.30           9.50          10.53      
    2    10.41          10.53           8.43      

 

Metagame probabilities: 
Player #0: 0.0353  0.3319  0.6328  
Player #1: 0.0353  0.3319  0.6328  
Iteration : 2
Time so far: 10337.269006967545
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 02:45:21.386879: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5841 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07583887253874282 7.663315376549056 3.4548606326182685 4.093162951120598 20003 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08275291113485009 7.851132837987282 3.452368284195242 4.241868874870344 420049 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08555313723958725 7.768494068164575 3.458757732965444 4.385087201919798 820090 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08487813868008362 7.671393030808296 3.4595672023885045 4.532628738566497 1220128 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08178699760243288 7.5979779827040295 3.4581848981075507 4.674166975585353 1620169 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07806092118404634 7.537194481655791 3.4532210028433536 4.80920381897999 2020209 0
Recovering previous policy with expected return of 5.227772227772228. Long term value was 4.2826 and short term was 4.237.


Pure best response payoff estimated to be 5.6758 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 6.26 seconds to finish estimate with resulting utilities: [7.4236 3.107 ]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 10.11 seconds to finish estimate with resulting utilities: [6.3354 4.1276]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 12.01 seconds to finish estimate with resulting utilities: [4.4678 4.1128]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 12.01 seconds to finish estimate with resulting utilities: [4.4692 4.0446]
Computing meta_strategies
Exited RRD with total regret 0.4653438551495359 that was less than regret lambda 0.4655172413793104 after 1068 iterations 
NEW LAMBDA 0.4482758620689656
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0     1.95           2.95           3.08           3.11      
    1     7.35           4.75           4.22           4.13      
    2     7.33           6.31           4.22           4.11      
    3     7.42           6.34           4.47           4.26      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0     1.95           7.35           7.33           7.42      
    1     2.95           4.75           6.31           6.34      
    2     3.08           4.22           4.22           4.47      
    3     3.11           4.13           4.11           4.26      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0     3.90          10.30          10.41          10.53      
    1    10.30           9.50          10.53          10.46      
    2    10.41          10.53           8.43           8.58      
    3    10.53          10.46           8.58           8.51      

 

Metagame probabilities: 
Player #0: 0.0359  0.2268  0.3405  0.3968  
Player #1: 0.0359  0.2268  0.3405  0.3968  
Iteration : 3
Time so far: 15494.459832668304
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-08-02 04:11:18.810721: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7831 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0751117676154081 7.5352450905988615 3.4370494407912093 4.939866386213494 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08028729192534804 7.650328659421146 3.4378432034265876 5.009045725160158 420053 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08240617105342231 7.588791655931833 3.4391322761211756 5.086881932995088 820096 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08218113812467194 7.538159702597437 3.445086457385673 5.166948136431542 1220139 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08001107472244194 7.507953536202168 3.4502709127705673 5.249890796980852 1620175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07719022422594313 7.479677511739337 3.447758070003888 5.334131685452889 2020218 0


Pure best response payoff estimated to be 5.8278 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 6.12 seconds to finish estimate with resulting utilities: [7.3068 3.301 ]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 9.23 seconds to finish estimate with resulting utilities: [5.9718 5.0908]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 9.21 seconds to finish estimate with resulting utilities: [5.594  5.6218]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 9.19 seconds to finish estimate with resulting utilities: [5.5828 5.5998]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 9.86 seconds to finish estimate with resulting utilities: [4.2956 4.9174]
Computing meta_strategies
Exited RRD with total regret 0.4481685243310327 that was less than regret lambda 0.4482758620689656 after 1492 iterations 
NEW LAMBDA 0.4310344827586208
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0     1.95           2.95           3.08           3.11           3.30      
    1     7.35           4.75           4.22           4.13           5.09      
    2     7.33           6.31           4.22           4.11           5.62      
    3     7.42           6.34           4.47           4.26           5.60      
    4     7.31           5.97           5.59           5.58           4.61      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0     1.95           7.35           7.33           7.42           7.31      
    1     2.95           4.75           6.31           6.34           5.97      
    2     3.08           4.22           4.22           4.47           5.59      
    3     3.11           4.13           4.11           4.26           5.58      
    4     3.30           5.09           5.62           5.60           4.61      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0     3.90          10.30          10.41          10.53          10.61      
    1    10.30           9.50          10.53          10.46          11.06      
    2    10.41          10.53           8.43           8.58          11.22      
    3    10.53          10.46           8.58           8.51          11.18      
    4    10.61          11.06          11.22          11.18           9.21      

 

Metagame probabilities: 
Player #0: 0.0101  0.1241  0.2298  0.2657  0.3703  
Player #1: 0.0101  0.1241  0.2298  0.2657  0.3703  
Iteration : 4
Time so far: 20348.89727497101
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-08-02 05:32:13.417906: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9821 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07512303738372698 7.496295448413409 3.4259030305056934 5.418846779028138 20005 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07836795240080642 7.548917583213456 3.424503489381052 5.479093202947999 420046 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07936532191200417 7.440154541022663 3.415540814927583 5.545135700471176 820085 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0786359871429117 7.3288365196078695 3.3994621621332133 5.623885169900502 1220125 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07657452899190761 7.223740702724129 3.3795641161731838 5.713073993865464 1620157 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07416478503681874 7.120738172768754 3.357162410872323 5.810280907500126 2020204 0


Pure best response payoff estimated to be 5.668 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 6.04 seconds to finish estimate with resulting utilities: [7.2702 3.5624]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 8.18 seconds to finish estimate with resulting utilities: [5.6654 5.9196]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 7.84 seconds to finish estimate with resulting utilities: [5.8904 6.5158]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 7.9 seconds to finish estimate with resulting utilities: [5.8648 6.5392]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 8.19 seconds to finish estimate with resulting utilities: [4.895  6.1142]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 7.15 seconds to finish estimate with resulting utilities: [5.6548 6.7746]
Computing meta_strategies
Exited RRD with total regret 0.4309655760875053 that was less than regret lambda 0.4310344827586208 after 1752 iterations 
NEW LAMBDA 0.41379310344827597
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0     1.95           2.95           3.08           3.11           3.30           3.56      
    1     7.35           4.75           4.22           4.13           5.09           5.92      
    2     7.33           6.31           4.22           4.11           5.62           6.52      
    3     7.42           6.34           4.47           4.26           5.60           6.54      
    4     7.31           5.97           5.59           5.58           4.61           6.11      
    5     7.27           5.67           5.89           5.86           4.89           6.21      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0     1.95           7.35           7.33           7.42           7.31           7.27      
    1     2.95           4.75           6.31           6.34           5.97           5.67      
    2     3.08           4.22           4.22           4.47           5.59           5.89      
    3     3.11           4.13           4.11           4.26           5.58           5.86      
    4     3.30           5.09           5.62           5.60           4.61           4.89      
    5     3.56           5.92           6.52           6.54           6.11           6.21      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0     3.90          10.30          10.41          10.53          10.61          10.83      
    1    10.30           9.50          10.53          10.46          11.06          11.59      
    2    10.41          10.53           8.43           8.58          11.22          12.41      
    3    10.53          10.46           8.58           8.51          11.18          12.40      
    4    10.61          11.06          11.22          11.18           9.21          11.01      
    5    10.83          11.59          12.41          12.40          11.01          12.43      

 

Metagame probabilities: 
Player #0: 0.0033  0.0787  0.1728  0.1996  0.236  0.3096  
Player #1: 0.0033  0.0787  0.1728  0.1996  0.236  0.3096  
Iteration : 5
Time so far: 25619.61122751236
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 07:00:04.204309: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11811 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07288023475484924 7.101530621303776 3.3400901293560743 5.876596057462983 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07464948350067964 7.1137788828902355 3.325521748309999 5.939403288448419 420039 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07521845409677956 7.009543768831792 3.315017411035436 5.98541712381412 820077 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07467767367114667 6.913742773621171 3.3040973774945295 6.037572988359575 1220115 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07316172951821735 6.8241930537601165 3.2906345403108666 6.102883652122329 1620155 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07131275293752071 6.738595308290495 3.2703956343244007 6.184280775701666 2020201 0
Recovering previous policy with expected return of 5.569430569430569. Long term value was 5.4338 and short term was 5.461.


Pure best response payoff estimated to be 5.8634 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 6.11 seconds to finish estimate with resulting utilities: [7.2964 3.5544]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 8.15 seconds to finish estimate with resulting utilities: [5.7116 6.0018]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 7.9 seconds to finish estimate with resulting utilities: [5.8982 6.5022]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 7.92 seconds to finish estimate with resulting utilities: [5.8302 6.6086]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 8.13 seconds to finish estimate with resulting utilities: [4.9264 6.1534]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 7.31 seconds to finish estimate with resulting utilities: [5.5544 6.7122]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 7.15 seconds to finish estimate with resulting utilities: [5.645 6.794]
Computing meta_strategies
Exited RRD with total regret 0.4137648543932908 that was less than regret lambda 0.41379310344827597 after 2510 iterations 
NEW LAMBDA 0.39655172413793116
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0     1.95           2.95           3.08           3.11           3.30           3.56           3.55      
    1     7.35           4.75           4.22           4.13           5.09           5.92           6.00      
    2     7.33           6.31           4.22           4.11           5.62           6.52           6.50      
    3     7.42           6.34           4.47           4.26           5.60           6.54           6.61      
    4     7.31           5.97           5.59           5.58           4.61           6.11           6.15      
    5     7.27           5.67           5.89           5.86           4.89           6.21           6.71      
    6     7.30           5.71           5.90           5.83           4.93           5.55           6.22      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0     1.95           7.35           7.33           7.42           7.31           7.27           7.30      
    1     2.95           4.75           6.31           6.34           5.97           5.67           5.71      
    2     3.08           4.22           4.22           4.47           5.59           5.89           5.90      
    3     3.11           4.13           4.11           4.26           5.58           5.86           5.83      
    4     3.30           5.09           5.62           5.60           4.61           4.89           4.93      
    5     3.56           5.92           6.52           6.54           6.11           6.21           5.55      
    6     3.55           6.00           6.50           6.61           6.15           6.71           6.22      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0     3.90          10.30          10.41          10.53          10.61          10.83          10.85      
    1    10.30           9.50          10.53          10.46          11.06          11.59          11.71      
    2    10.41          10.53           8.43           8.58          11.22          12.41          12.40      
    3    10.53          10.46           8.58           8.51          11.18          12.40          12.44      
    4    10.61          11.06          11.22          11.18           9.21          11.01          11.08      
    5    10.83          11.59          12.41          12.40          11.01          12.43          12.27      
    6    10.85          11.71          12.40          12.44          11.08          12.27          12.44      

 

Metagame probabilities: 
Player #0: 0.0004  0.044  0.1355  0.1683  0.1721  0.3074  0.1725  
Player #1: 0.0004  0.044  0.1355  0.1683  0.1721  0.3074  0.1725  
Iteration : 6
Time so far: 31079.49189710617
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-08-02 08:31:04.164344: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13801 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07043074533670023 6.722456248164013 3.25584672937039 6.23887186751364 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07205794493575561 6.756394510192565 3.2456531855316366 6.283116006842818 420040 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07254714197700218 6.6928343017533365 3.2359809158678314 6.318697563633225 820080 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07204534426349307 6.6343294793193195 3.2259424700367405 6.353061488372624 1220125 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07065976118690588 6.574809615762703 3.2116240109652834 6.394082032459449 1620165 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06899500188328415 6.516066324782919 3.1901990605669126 6.451774182421625 2020196 0


Pure best response payoff estimated to be 6.1288 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 7.04 seconds to finish estimate with resulting utilities: [7.315  3.6178]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 9.75 seconds to finish estimate with resulting utilities: [5.5944 5.963 ]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 9.4 seconds to finish estimate with resulting utilities: [5.5644 6.7026]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 9.35 seconds to finish estimate with resulting utilities: [5.604  6.6474]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 9.19 seconds to finish estimate with resulting utilities: [4.879  6.4814]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 7.26 seconds to finish estimate with resulting utilities: [6.139  7.0374]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 7.42 seconds to finish estimate with resulting utilities: [6.0608 6.9312]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 6.46 seconds to finish estimate with resulting utilities: [6.4434 6.8078]
Computing meta_strategies
Exited RRD with total regret 0.39649395135967325 that was less than regret lambda 0.39655172413793116 after 3254 iterations 
NEW LAMBDA 0.3793103448275863
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0     1.95           2.95           3.08           3.11           3.30           3.56           3.55           3.62      
    1     7.35           4.75           4.22           4.13           5.09           5.92           6.00           5.96      
    2     7.33           6.31           4.22           4.11           5.62           6.52           6.50           6.70      
    3     7.42           6.34           4.47           4.26           5.60           6.54           6.61           6.65      
    4     7.31           5.97           5.59           5.58           4.61           6.11           6.15           6.48      
    5     7.27           5.67           5.89           5.86           4.89           6.21           6.71           7.04      
    6     7.30           5.71           5.90           5.83           4.93           5.55           6.22           6.93      
    7     7.32           5.59           5.56           5.60           4.88           6.14           6.06           6.63      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0     1.95           7.35           7.33           7.42           7.31           7.27           7.30           7.32      
    1     2.95           4.75           6.31           6.34           5.97           5.67           5.71           5.59      
    2     3.08           4.22           4.22           4.47           5.59           5.89           5.90           5.56      
    3     3.11           4.13           4.11           4.26           5.58           5.86           5.83           5.60      
    4     3.30           5.09           5.62           5.60           4.61           4.89           4.93           4.88      
    5     3.56           5.92           6.52           6.54           6.11           6.21           5.55           6.14      
    6     3.55           6.00           6.50           6.61           6.15           6.71           6.22           6.06      
    7     3.62           5.96           6.70           6.65           6.48           7.04           6.93           6.63      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0     3.90          10.30          10.41          10.53          10.61          10.83          10.85          10.93      
    1    10.30           9.50          10.53          10.46          11.06          11.59          11.71          11.56      
    2    10.41          10.53           8.43           8.58          11.22          12.41          12.40          12.27      
    3    10.53          10.46           8.58           8.51          11.18          12.40          12.44          12.25      
    4    10.61          11.06          11.22          11.18           9.21          11.01          11.08          11.36      
    5    10.83          11.59          12.41          12.40          11.01          12.43          12.27          13.18      
    6    10.85          11.71          12.40          12.44          11.08          12.27          12.44          12.99      
    7    10.93          11.56          12.27          12.25          11.36          13.18          12.99          13.25      

 

Metagame probabilities: 
Player #0: 0.0001  0.0227  0.1097  0.136  0.1268  0.3183  0.1491  0.1373  
Player #1: 0.0001  0.0227  0.1097  0.136  0.1268  0.3183  0.1491  0.1373  
Iteration : 7
Time so far: 37939.02970170975
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-08-02 10:25:24.289190: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15791 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06799774941239826 6.509966086434127 3.17155039664823 6.502084636964285 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06907283967000792 6.5476785619361335 3.153059328557722 6.552062024464798 420036 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06943931732864952 6.487896470503964 3.1410441670325775 6.5934277660523986 820068 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06902937956858395 6.432581956854936 3.1295523276207318 6.637925049513413 1220108 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06789671118284857 6.379128287643615 3.1185149307748006 6.687277177095607 1620151 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06653521338913948 6.32557374632954 3.1082614970131206 6.735235433490763 2020186 0
Recovering previous policy with expected return of 5.7502497502497505. Long term value was 5.7824 and short term was 5.725.


Pure best response payoff estimated to be 6.31 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (8, 0)
Current player 0 and current strategies (8, 0) took 6.38 seconds to finish estimate with resulting utilities: [7.3236 3.5012]
Estimating current strategies:  (8, 1)
Current player 0 and current strategies (8, 1) took 8.79 seconds to finish estimate with resulting utilities: [5.5974 5.9562]
Estimating current strategies:  (8, 2)
Current player 0 and current strategies (8, 2) took 8.59 seconds to finish estimate with resulting utilities: [5.502  6.6772]
Estimating current strategies:  (8, 3)
Current player 0 and current strategies (8, 3) took 8.51 seconds to finish estimate with resulting utilities: [5.5828 6.6972]
Estimating current strategies:  (8, 4)
Current player 0 and current strategies (8, 4) took 8.33 seconds to finish estimate with resulting utilities: [4.8142 6.5234]
Estimating current strategies:  (8, 5)
Current player 0 and current strategies (8, 5) took 6.8 seconds to finish estimate with resulting utilities: [5.9934 7.0362]
Estimating current strategies:  (8, 6)
Current player 0 and current strategies (8, 6) took 6.72 seconds to finish estimate with resulting utilities: [6.0772 7.033 ]
Estimating current strategies:  (8, 7)
Current player 0 and current strategies (8, 7) took 6.0 seconds to finish estimate with resulting utilities: [6.4614 6.8444]
Estimating current strategies:  (8, 8)
Current player 0 and current strategies (8, 8) took 5.97 seconds to finish estimate with resulting utilities: [6.4452 6.7374]
Computing meta_strategies
Exited RRD with total regret 0.3792561489618258 that was less than regret lambda 0.3793103448275863 after 4065 iterations 
NEW LAMBDA 0.3620689655172415
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0     1.95           2.95           3.08           3.11           3.30           3.56           3.55           3.62           3.50      
    1     7.35           4.75           4.22           4.13           5.09           5.92           6.00           5.96           5.96      
    2     7.33           6.31           4.22           4.11           5.62           6.52           6.50           6.70           6.68      
    3     7.42           6.34           4.47           4.26           5.60           6.54           6.61           6.65           6.70      
    4     7.31           5.97           5.59           5.58           4.61           6.11           6.15           6.48           6.52      
    5     7.27           5.67           5.89           5.86           4.89           6.21           6.71           7.04           7.04      
    6     7.30           5.71           5.90           5.83           4.93           5.55           6.22           6.93           7.03      
    7     7.32           5.59           5.56           5.60           4.88           6.14           6.06           6.63           6.84      
    8     7.32           5.60           5.50           5.58           4.81           5.99           6.08           6.46           6.59      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0     1.95           7.35           7.33           7.42           7.31           7.27           7.30           7.32           7.32      
    1     2.95           4.75           6.31           6.34           5.97           5.67           5.71           5.59           5.60      
    2     3.08           4.22           4.22           4.47           5.59           5.89           5.90           5.56           5.50      
    3     3.11           4.13           4.11           4.26           5.58           5.86           5.83           5.60           5.58      
    4     3.30           5.09           5.62           5.60           4.61           4.89           4.93           4.88           4.81      
    5     3.56           5.92           6.52           6.54           6.11           6.21           5.55           6.14           5.99      
    6     3.55           6.00           6.50           6.61           6.15           6.71           6.22           6.06           6.08      
    7     3.62           5.96           6.70           6.65           6.48           7.04           6.93           6.63           6.46      
    8     3.50           5.96           6.68           6.70           6.52           7.04           7.03           6.84           6.59      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8      
    0     3.90          10.30          10.41          10.53          10.61          10.83          10.85          10.93          10.82      
    1    10.30           9.50          10.53          10.46          11.06          11.59          11.71          11.56          11.55      
    2    10.41          10.53           8.43           8.58          11.22          12.41          12.40          12.27          12.18      
    3    10.53          10.46           8.58           8.51          11.18          12.40          12.44          12.25          12.28      
    4    10.61          11.06          11.22          11.18           9.21          11.01          11.08          11.36          11.34      
    5    10.83          11.59          12.41          12.40          11.01          12.43          12.27          13.18          13.03      
    6    10.85          11.71          12.40          12.44          11.08          12.27          12.44          12.99          13.11      
    7    10.93          11.56          12.27          12.25          11.36          13.18          12.99          13.25          13.31      
    8    10.82          11.55          12.18          12.28          11.34          13.03          13.11          13.31          13.18      

 

Metagame probabilities: 
Player #0: 0.0001  0.0117  0.0914  0.1167  0.0966  0.3426  0.1357  0.1221  0.0832  
Player #1: 0.0001  0.0117  0.0914  0.1167  0.0966  0.3426  0.1357  0.1221  0.0832  
Iteration : 8
Time so far: 44025.59974527359
Approximating Best Response
Training best response:  True 0.06
rtg values check:  (?,) (?, 1)
2023-08-02 12:06:50.796759: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_62/weights_2/Adam_1/Assign' id:17781 op device:{requested: '', assigned: ''} def:{{{node mlp_62/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_62/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_62/weights_2/Adam_1, mlp_62/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06586543771941342 6.326393020465766 3.101747038281852 6.762269056649861 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06678529294012191 6.356677631582042 3.085346932866958 6.799549013731229 420038 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06707085614280929 6.304756034032816 3.0741657489496177 6.8275420124013975 820067 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06674567305321805 6.254816862673067 3.064394547037876 6.857305658198709 1220110 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06578898563797812 6.207925892799429 3.0533550670970904 6.894903875983753 1620138 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06460666766080427 6.1619974708738265 3.039469033089119 6.942071094896435 2020170 0
Recovering previous policy with expected return of 5.835164835164835. Long term value was 5.3426 and short term was 5.216.


Pure best response payoff estimated to be 6.4394 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (9, 0)
Current player 0 and current strategies (9, 0) took 6.26 seconds to finish estimate with resulting utilities: [7.3656 3.4958]
Estimating current strategies:  (9, 1)
Current player 0 and current strategies (9, 1) took 8.67 seconds to finish estimate with resulting utilities: [5.4996 5.9922]
Estimating current strategies:  (9, 2)
Current player 0 and current strategies (9, 2) took 8.39 seconds to finish estimate with resulting utilities: [5.589  6.7362]
Estimating current strategies:  (9, 3)
Current player 0 and current strategies (9, 3) took 8.46 seconds to finish estimate with resulting utilities: [5.5086 6.6808]
Estimating current strategies:  (9, 4)
Current player 0 and current strategies (9, 4) took 8.19 seconds to finish estimate with resulting utilities: [4.8488 6.5956]
Estimating current strategies:  (9, 5)
Current player 0 and current strategies (9, 5) took 6.78 seconds to finish estimate with resulting utilities: [6.1026 6.9422]
Estimating current strategies:  (9, 6)
Current player 0 and current strategies (9, 6) took 6.72 seconds to finish estimate with resulting utilities: [5.9326 6.9058]
Estimating current strategies:  (9, 7)
Current player 0 and current strategies (9, 7) took 6.02 seconds to finish estimate with resulting utilities: [6.4562 6.7908]
Estimating current strategies:  (9, 8)
Current player 0 and current strategies (9, 8) took 6.0 seconds to finish estimate with resulting utilities: [6.4706 6.6866]
Estimating current strategies:  (9, 9)
Current player 0 and current strategies (9, 9) took 5.94 seconds to finish estimate with resulting utilities: [6.5168 6.7738]
Computing meta_strategies
Exited RRD with total regret 0.36205308564911043 that was less than regret lambda 0.3620689655172415 after 4738 iterations 
NEW LAMBDA 0.3448275862068967
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     1.95           2.95           3.08           3.11           3.30           3.56           3.55           3.62           3.50           3.50      
    1     7.35           4.75           4.22           4.13           5.09           5.92           6.00           5.96           5.96           5.99      
    2     7.33           6.31           4.22           4.11           5.62           6.52           6.50           6.70           6.68           6.74      
    3     7.42           6.34           4.47           4.26           5.60           6.54           6.61           6.65           6.70           6.68      
    4     7.31           5.97           5.59           5.58           4.61           6.11           6.15           6.48           6.52           6.60      
    5     7.27           5.67           5.89           5.86           4.89           6.21           6.71           7.04           7.04           6.94      
    6     7.30           5.71           5.90           5.83           4.93           5.55           6.22           6.93           7.03           6.91      
    7     7.32           5.59           5.56           5.60           4.88           6.14           6.06           6.63           6.84           6.79      
    8     7.32           5.60           5.50           5.58           4.81           5.99           6.08           6.46           6.59           6.69      
    9     7.37           5.50           5.59           5.51           4.85           6.10           5.93           6.46           6.47           6.65      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     1.95           7.35           7.33           7.42           7.31           7.27           7.30           7.32           7.32           7.37      
    1     2.95           4.75           6.31           6.34           5.97           5.67           5.71           5.59           5.60           5.50      
    2     3.08           4.22           4.22           4.47           5.59           5.89           5.90           5.56           5.50           5.59      
    3     3.11           4.13           4.11           4.26           5.58           5.86           5.83           5.60           5.58           5.51      
    4     3.30           5.09           5.62           5.60           4.61           4.89           4.93           4.88           4.81           4.85      
    5     3.56           5.92           6.52           6.54           6.11           6.21           5.55           6.14           5.99           6.10      
    6     3.55           6.00           6.50           6.61           6.15           6.71           6.22           6.06           6.08           5.93      
    7     3.62           5.96           6.70           6.65           6.48           7.04           6.93           6.63           6.46           6.46      
    8     3.50           5.96           6.68           6.70           6.52           7.04           7.03           6.84           6.59           6.47      
    9     3.50           5.99           6.74           6.68           6.60           6.94           6.91           6.79           6.69           6.65      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     3.90          10.30          10.41          10.53          10.61          10.83          10.85          10.93          10.82          10.86      
    1    10.30           9.50          10.53          10.46          11.06          11.59          11.71          11.56          11.55          11.49      
    2    10.41          10.53           8.43           8.58          11.22          12.41          12.40          12.27          12.18          12.33      
    3    10.53          10.46           8.58           8.51          11.18          12.40          12.44          12.25          12.28          12.19      
    4    10.61          11.06          11.22          11.18           9.21          11.01          11.08          11.36          11.34          11.44      
    5    10.83          11.59          12.41          12.40          11.01          12.43          12.27          13.18          13.03          13.04      
    6    10.85          11.71          12.40          12.44          11.08          12.27          12.44          12.99          13.11          12.84      
    7    10.93          11.56          12.27          12.25          11.36          13.18          12.99          13.25          13.31          13.25      
    8    10.82          11.55          12.18          12.28          11.34          13.03          13.11          13.31          13.18          13.16      
    9    10.86          11.49          12.33          12.19          11.44          13.04          12.84          13.25          13.16          13.29      

 

Metagame probabilities: 
Player #0: 0.0001  0.0068  0.082  0.1036  0.0797  0.3557  0.1234  0.1112  0.0704  0.0671  
Player #1: 0.0001  0.0068  0.082  0.1036  0.0797  0.3557  0.1234  0.1112  0.0704  0.0671  
Iteration : 9
Time so far: 50113.53289294243
Approximating Best Response
Training best response:  True 0.05500000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 13:48:18.791954: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_69/weights_2/Adam_1/Assign' id:19771 op device:{requested: '', assigned: ''} def:{{{node mlp_69/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_69/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_69/weights_2/Adam_1, mlp_69/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06383389518318273 6.146085339851593 3.0255200695455744 6.985718302997701 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06464616542063381 6.1801119129009106 3.0114923060159473 7.021309850183863 420034 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06490062876363471 6.127794675749562 3.0020420973266506 7.050195931220689 820069 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06453910469002601 6.083096872534313 2.9900013488446566 7.0917971589408335 1220100 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06369473769615254 6.041920457899778 2.976250313134144 7.14478786983842 1620139 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06266742673181659 6.003160964340379 2.960954440783148 7.208298467972048 2020169 0


Pure best response payoff estimated to be 6.3724 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (10, 0)
Current player 0 and current strategies (10, 0) took 6.94 seconds to finish estimate with resulting utilities: [7.2236 3.3064]
Estimating current strategies:  (10, 1)
Current player 0 and current strategies (10, 1) took 7.76 seconds to finish estimate with resulting utilities: [5.2468 6.803 ]
Estimating current strategies:  (10, 2)
Current player 0 and current strategies (10, 2) took 6.69 seconds to finish estimate with resulting utilities: [5.7002 6.8546]
Estimating current strategies:  (10, 3)
Current player 0 and current strategies (10, 3) took 6.68 seconds to finish estimate with resulting utilities: [5.6322 6.946 ]
Estimating current strategies:  (10, 4)
Current player 0 and current strategies (10, 4) took 7.2 seconds to finish estimate with resulting utilities: [5.301  6.6818]
Estimating current strategies:  (10, 5)
Current player 0 and current strategies (10, 5) took 6.17 seconds to finish estimate with resulting utilities: [5.8656 7.1582]
Estimating current strategies:  (10, 6)
Current player 0 and current strategies (10, 6) took 6.21 seconds to finish estimate with resulting utilities: [5.8892 7.1422]
Estimating current strategies:  (10, 7)
Current player 0 and current strategies (10, 7) took 5.58 seconds to finish estimate with resulting utilities: [6.1212 7.25  ]
Estimating current strategies:  (10, 8)
Current player 0 and current strategies (10, 8) took 5.54 seconds to finish estimate with resulting utilities: [6.205  7.2524]
Estimating current strategies:  (10, 9)
Current player 0 and current strategies (10, 9) took 5.5 seconds to finish estimate with resulting utilities: [6.2074 7.2426]
Estimating current strategies:  (10, 10)
Current player 0 and current strategies (10, 10) took 5.41 seconds to finish estimate with resulting utilities: [6.4102 6.6402]
Computing meta_strategies
Exited RRD with total regret 0.3448205890628868 that was less than regret lambda 0.3448275862068967 after 5466 iterations 
NEW LAMBDA 0.3275862068965518
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     1.95           2.95           3.08           3.11           3.30           3.56           3.55           3.62           3.50           3.50           3.31      
    1     7.35           4.75           4.22           4.13           5.09           5.92           6.00           5.96           5.96           5.99           6.80      
    2     7.33           6.31           4.22           4.11           5.62           6.52           6.50           6.70           6.68           6.74           6.85      
    3     7.42           6.34           4.47           4.26           5.60           6.54           6.61           6.65           6.70           6.68           6.95      
    4     7.31           5.97           5.59           5.58           4.61           6.11           6.15           6.48           6.52           6.60           6.68      
    5     7.27           5.67           5.89           5.86           4.89           6.21           6.71           7.04           7.04           6.94           7.16      
    6     7.30           5.71           5.90           5.83           4.93           5.55           6.22           6.93           7.03           6.91           7.14      
    7     7.32           5.59           5.56           5.60           4.88           6.14           6.06           6.63           6.84           6.79           7.25      
    8     7.32           5.60           5.50           5.58           4.81           5.99           6.08           6.46           6.59           6.69           7.25      
    9     7.37           5.50           5.59           5.51           4.85           6.10           5.93           6.46           6.47           6.65           7.24      
   10     7.22           5.25           5.70           5.63           5.30           5.87           5.89           6.12           6.21           6.21           6.53      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     1.95           7.35           7.33           7.42           7.31           7.27           7.30           7.32           7.32           7.37           7.22      
    1     2.95           4.75           6.31           6.34           5.97           5.67           5.71           5.59           5.60           5.50           5.25      
    2     3.08           4.22           4.22           4.47           5.59           5.89           5.90           5.56           5.50           5.59           5.70      
    3     3.11           4.13           4.11           4.26           5.58           5.86           5.83           5.60           5.58           5.51           5.63      
    4     3.30           5.09           5.62           5.60           4.61           4.89           4.93           4.88           4.81           4.85           5.30      
    5     3.56           5.92           6.52           6.54           6.11           6.21           5.55           6.14           5.99           6.10           5.87      
    6     3.55           6.00           6.50           6.61           6.15           6.71           6.22           6.06           6.08           5.93           5.89      
    7     3.62           5.96           6.70           6.65           6.48           7.04           6.93           6.63           6.46           6.46           6.12      
    8     3.50           5.96           6.68           6.70           6.52           7.04           7.03           6.84           6.59           6.47           6.21      
    9     3.50           5.99           6.74           6.68           6.60           6.94           6.91           6.79           6.69           6.65           6.21      
   10     3.31           6.80           6.85           6.95           6.68           7.16           7.14           7.25           7.25           7.24           6.53      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     3.90          10.30          10.41          10.53          10.61          10.83          10.85          10.93          10.82          10.86          10.53      
    1    10.30           9.50          10.53          10.46          11.06          11.59          11.71          11.56          11.55          11.49          12.05      
    2    10.41          10.53           8.43           8.58          11.22          12.41          12.40          12.27          12.18          12.33          12.55      
    3    10.53          10.46           8.58           8.51          11.18          12.40          12.44          12.25          12.28          12.19          12.58      
    4    10.61          11.06          11.22          11.18           9.21          11.01          11.08          11.36          11.34          11.44          11.98      
    5    10.83          11.59          12.41          12.40          11.01          12.43          12.27          13.18          13.03          13.04          13.02      
    6    10.85          11.71          12.40          12.44          11.08          12.27          12.44          12.99          13.11          12.84          13.03      
    7    10.93          11.56          12.27          12.25          11.36          13.18          12.99          13.25          13.31          13.25          13.37      
    8    10.82          11.55          12.18          12.28          11.34          13.03          13.11          13.31          13.18          13.16          13.46      
    9    10.86          11.49          12.33          12.19          11.44          13.04          12.84          13.25          13.16          13.29          13.45      
   10    10.53          12.05          12.55          12.58          11.98          13.02          13.03          13.37          13.46          13.45          13.05      

 

Metagame probabilities: 
Player #0: 0.0001  0.0047  0.0728  0.096  0.064  0.38  0.1128  0.1108  0.0667  0.0636  0.0283  
Player #1: 0.0001  0.0047  0.0728  0.096  0.064  0.38  0.1128  0.1108  0.0667  0.0636  0.0283  
Iteration : 10
Time so far: 56502.81768369675
Approximating Best Response
Training best response:  True 0.05
rtg values check:  (?,) (?, 1)
2023-08-02 15:34:48.146002: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_76/weights_2/Adam_1/Assign' id:21761 op device:{requested: '', assigned: ''} def:{{{node mlp_76/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_76/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_76/weights_2/Adam_1, mlp_76/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0620481323333305 6.003199247821737 2.949102131364309 7.256233374796896 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06267029242410894 6.048464101007497 2.935597671308057 7.30812328218298 420039 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0628194234761392 6.012581714787772 2.9251887834794004 7.351988770913716 820069 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06250190144567301 5.97610240869476 2.914556520630196 7.39742709422721 1220107 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.061743729947315286 5.941293059627152 2.9041588990132894 7.44319600185648 1620140 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06083002577310049 5.899585195525016 2.8940571629684912 7.488560381280418 2020180 0
Recovering previous policy with expected return of 5.997002997002997. Long term value was 5.6238 and short term was 5.766.


Pure best response payoff estimated to be 6.3546 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (11, 0)
Current player 0 and current strategies (11, 0) took 6.91 seconds to finish estimate with resulting utilities: [7.1808 3.399 ]
Estimating current strategies:  (11, 1)
Current player 0 and current strategies (11, 1) took 7.75 seconds to finish estimate with resulting utilities: [5.2932 6.8506]
Estimating current strategies:  (11, 2)
Current player 0 and current strategies (11, 2) took 6.78 seconds to finish estimate with resulting utilities: [5.703 6.899]
Estimating current strategies:  (11, 3)
Current player 0 and current strategies (11, 3) took 6.72 seconds to finish estimate with resulting utilities: [5.7102 6.8822]
Estimating current strategies:  (11, 4)
Current player 0 and current strategies (11, 4) took 7.27 seconds to finish estimate with resulting utilities: [5.2294 6.611 ]
Estimating current strategies:  (11, 5)
Current player 0 and current strategies (11, 5) took 6.24 seconds to finish estimate with resulting utilities: [5.9268 7.178 ]
Estimating current strategies:  (11, 6)
Current player 0 and current strategies (11, 6) took 6.18 seconds to finish estimate with resulting utilities: [5.9226 7.1744]
Estimating current strategies:  (11, 7)
Current player 0 and current strategies (11, 7) took 5.47 seconds to finish estimate with resulting utilities: [6.1752 7.2746]
Estimating current strategies:  (11, 8)
Current player 0 and current strategies (11, 8) took 5.49 seconds to finish estimate with resulting utilities: [6.1724 7.2466]
Estimating current strategies:  (11, 9)
Current player 0 and current strategies (11, 9) took 5.49 seconds to finish estimate with resulting utilities: [6.2384 7.235 ]
Estimating current strategies:  (11, 10)
Current player 0 and current strategies (11, 10) took 5.44 seconds to finish estimate with resulting utilities: [6.3128 6.6566]
Estimating current strategies:  (11, 11)
Current player 0 and current strategies (11, 11) took 5.47 seconds to finish estimate with resulting utilities: [6.371  6.6764]
Computing meta_strategies
Exited RRD with total regret 0.3275281766959557 that was less than regret lambda 0.3275862068965518 after 6093 iterations 
NEW LAMBDA 0.31034482758620696
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11      
    0     1.95           2.95           3.08           3.11           3.30           3.56           3.55           3.62           3.50           3.50           3.31           3.40      
    1     7.35           4.75           4.22           4.13           5.09           5.92           6.00           5.96           5.96           5.99           6.80           6.85      
    2     7.33           6.31           4.22           4.11           5.62           6.52           6.50           6.70           6.68           6.74           6.85           6.90      
    3     7.42           6.34           4.47           4.26           5.60           6.54           6.61           6.65           6.70           6.68           6.95           6.88      
    4     7.31           5.97           5.59           5.58           4.61           6.11           6.15           6.48           6.52           6.60           6.68           6.61      
    5     7.27           5.67           5.89           5.86           4.89           6.21           6.71           7.04           7.04           6.94           7.16           7.18      
    6     7.30           5.71           5.90           5.83           4.93           5.55           6.22           6.93           7.03           6.91           7.14           7.17      
    7     7.32           5.59           5.56           5.60           4.88           6.14           6.06           6.63           6.84           6.79           7.25           7.27      
    8     7.32           5.60           5.50           5.58           4.81           5.99           6.08           6.46           6.59           6.69           7.25           7.25      
    9     7.37           5.50           5.59           5.51           4.85           6.10           5.93           6.46           6.47           6.65           7.24           7.24      
   10     7.22           5.25           5.70           5.63           5.30           5.87           5.89           6.12           6.21           6.21           6.53           6.66      
   11     7.18           5.29           5.70           5.71           5.23           5.93           5.92           6.18           6.17           6.24           6.31           6.52      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11      
    0     1.95           7.35           7.33           7.42           7.31           7.27           7.30           7.32           7.32           7.37           7.22           7.18      
    1     2.95           4.75           6.31           6.34           5.97           5.67           5.71           5.59           5.60           5.50           5.25           5.29      
    2     3.08           4.22           4.22           4.47           5.59           5.89           5.90           5.56           5.50           5.59           5.70           5.70      
    3     3.11           4.13           4.11           4.26           5.58           5.86           5.83           5.60           5.58           5.51           5.63           5.71      
    4     3.30           5.09           5.62           5.60           4.61           4.89           4.93           4.88           4.81           4.85           5.30           5.23      
    5     3.56           5.92           6.52           6.54           6.11           6.21           5.55           6.14           5.99           6.10           5.87           5.93      
    6     3.55           6.00           6.50           6.61           6.15           6.71           6.22           6.06           6.08           5.93           5.89           5.92      
    7     3.62           5.96           6.70           6.65           6.48           7.04           6.93           6.63           6.46           6.46           6.12           6.18      
    8     3.50           5.96           6.68           6.70           6.52           7.04           7.03           6.84           6.59           6.47           6.21           6.17      
    9     3.50           5.99           6.74           6.68           6.60           6.94           6.91           6.79           6.69           6.65           6.21           6.24      
   10     3.31           6.80           6.85           6.95           6.68           7.16           7.14           7.25           7.25           7.24           6.53           6.31      
   11     3.40           6.85           6.90           6.88           6.61           7.18           7.17           7.27           7.25           7.24           6.66           6.52      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11      
    0     3.90          10.30          10.41          10.53          10.61          10.83          10.85          10.93          10.82          10.86          10.53          10.58      
    1    10.30           9.50          10.53          10.46          11.06          11.59          11.71          11.56          11.55          11.49          12.05          12.14      
    2    10.41          10.53           8.43           8.58          11.22          12.41          12.40          12.27          12.18          12.33          12.55          12.60      
    3    10.53          10.46           8.58           8.51          11.18          12.40          12.44          12.25          12.28          12.19          12.58          12.59      
    4    10.61          11.06          11.22          11.18           9.21          11.01          11.08          11.36          11.34          11.44          11.98          11.84      
    5    10.83          11.59          12.41          12.40          11.01          12.43          12.27          13.18          13.03          13.04          13.02          13.10      
    6    10.85          11.71          12.40          12.44          11.08          12.27          12.44          12.99          13.11          12.84          13.03          13.10      
    7    10.93          11.56          12.27          12.25          11.36          13.18          12.99          13.25          13.31          13.25          13.37          13.45      
    8    10.82          11.55          12.18          12.28          11.34          13.03          13.11          13.31          13.18          13.16          13.46          13.42      
    9    10.86          11.49          12.33          12.19          11.44          13.04          12.84          13.25          13.16          13.29          13.45          13.47      
   10    10.53          12.05          12.55          12.58          11.98          13.02          13.03          13.37          13.46          13.45          13.05          12.97      
   11    10.58          12.14          12.60          12.59          11.84          13.10          13.10          13.45          13.42          13.47          12.97          13.05      

 

Metagame probabilities: 
Player #0: 0.0001  0.0035  0.0674  0.0891  0.0518  0.4043  0.1045  0.1116  0.0637  0.0608  0.021  0.0222  
Player #1: 0.0001  0.0035  0.0674  0.0891  0.0518  0.4043  0.1045  0.1116  0.0637  0.0608  0.021  0.0222  
Iteration : 11
Time so far: 62740.587996959686
Approximating Best Response
Training best response:  True 0.045
rtg values check:  (?,) (?, 1)
2023-08-02 17:18:47.107141: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_83/weights_2/Adam_1/Assign' id:23751 op device:{requested: '', assigned: ''} def:{{{node mlp_83/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_83/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_83/weights_2/Adam_1, mlp_83/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.060286763017266924 5.887656849917071 2.886976990011739 7.520068945740062 20005 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06081701483478402 5.904891473181705 2.8739560086314078 7.5658241656366 420044 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.061014585657641854 5.867880456497569 2.865881220380703 7.595702340819477 820072 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06075709305959125 5.830149680463966 2.857959971842864 7.628625919012501 1220097 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06007487964583254 5.790588743742123 2.849243168602156 7.666039420867685 1620134 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0592313662576777 5.755186267397807 2.8379163905508538 7.707545475046708 2020175 0


slurmstepd: error: *** JOB 57045982 ON gl3117 CANCELLED AT 2023-08-02T18:18:52 ***
