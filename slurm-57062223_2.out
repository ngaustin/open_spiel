Job Id listed below:
57062229

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

/var/spool/slurmd.spool/job57062229/slurm_script: line 29: regret_lambda: command not found
/var/spool/slurmd.spool/job57062229/slurm_script: line 30: regret_steps: command not found
2023-08-02 12:24:58.590290: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-02 12:25:00.365401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0802 12:25:03.701832 23421839936384 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x154d09332d70>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x154d09332d70>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-08-02 12:25:03.986470: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-08-02 12:25:04.258681: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.57 seconds to finish estimate with resulting utilities: [48.83 50.72]
Exited RRD with total regret 0.0 that was less than regret lambda 2.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    49.77      

 

Player 1 Payoff matrix: 

           0      
    0    49.77      

 

Social Welfare Sum Matrix: 

           0      
    0    99.55      

 

Iteration : 0
Time so far: 0.00018024444580078125
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-08-02 12:25:23.711169: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.11679944917559623 29.313785552978516 2.06161630153656 0.0013951495464425534 10881 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.100006303191185 13.450551891326905 1.8992271542549133 0.1934519663453102 218457 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09819706305861473 14.493696022033692 1.8761809110641479 0.2710838258266449 421928 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08919580280780792 16.854555511474608 1.8325143218040467 0.4009512960910797 624474 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07698702439665794 15.3541748046875 1.793606948852539 0.480095711350441 827053 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07478132843971252 16.84296932220459 1.8117356419563293 0.49129638969898226 1029332 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06747591197490692 24.098179054260253 1.7410715222358704 0.6139158070087433 1233071 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0604338139295578 16.880418968200683 1.7230151295661926 0.704172819852829 1437809 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.052912260964512826 15.974484539031982 1.640644609928131 0.9112319111824035 1645402 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04516366682946682 17.852603340148924 1.598869252204895 1.018726658821106 1853473 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03964475356042385 18.336935424804686 1.5548704743385315 1.1231589555740356 2062517 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034206503070890903 20.808782386779786 1.4653577089309693 1.329782521724701 2271527 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030634135380387305 22.221570205688476 1.447132694721222 1.4445083379745483 2482328 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02357592899352312 23.888282203674315 1.3750337600708007 1.599058735370636 2693533 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01976345470175147 19.629548835754395 1.3303673624992371 1.6577301383018495 2905189 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015286855772137642 25.010771560668946 1.2473924994468688 1.9079390168190002 3115907 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012577610928565264 25.523616600036622 1.174557864665985 2.0856404423713686 3326649 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00740194357931614 25.021538734436035 1.128697669506073 2.190226984024048 3538154 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004017710592597723 19.518741416931153 1.0957558870315551 2.5072335958480836 3751226 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0023863584763603287 29.552480506896973 0.9956668674945831 2.6813289165496825 3965395 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022916779475053773 24.85248966217041 0.8983271777629852 3.0921001195907594 4180245 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017734406486852095 26.71504154205322 0.8576992690563202 3.2968708992004396 4397207 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00125224388029892 27.694289207458496 0.7635690450668335 3.5696184635162354 4612840 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016784253146397532 22.515138244628908 0.7562138736248016 3.633633327484131 4827114 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002329406709759496 29.05903854370117 0.7015066385269165 3.9511931419372557 5041392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019133937938022427 26.30482883453369 0.6217330694198608 4.330424070358276 5254782 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009607830256754823 22.184541511535645 0.6377190709114074 4.430917119979858 5472003 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001636769055039622 23.96307430267334 0.6167284846305847 4.673871850967407 5687794 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004442375120561337 27.43484287261963 0.5813310265541076 5.115693807601929 5906411 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008591686360887252 20.10166015625 0.5609986484050751 5.496492099761963 6125322 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001745515683433041 21.59446620941162 0.5193262100219727 5.96326847076416 6342095 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001538173179142177 23.438533210754393 0.5085354954004287 6.247984600067139 6560007 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021692651091143487 22.96397132873535 0.47444185316562654 6.393208837509155 6777613 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011421763661928707 28.873550796508788 0.4817176192998886 6.437318134307861 6995109 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0023866278839705045 23.554589462280273 0.4621947228908539 6.728355264663696 7214386 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010524725974391913 28.755097579956054 0.4649831235408783 6.934819269180298 7432248 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00023954877397045494 21.29541358947754 0.45391889810562136 7.167458248138428 7651131 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000471466012095334 24.209160614013673 0.4247305870056152 7.186433601379394 7868514 0


Pure best response payoff estimated to be 192.37 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 79.39 seconds to finish estimate with resulting utilities: [189.44    4.485]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 132.72 seconds to finish estimate with resulting utilities: [92.93  94.255]
Computing meta_strategies
Exited RRD with total regret 1.9766496112068808 that was less than regret lambda 2.0 after 44 iterations 
NEW LAMBDA 1.9310344827586208
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    49.77           4.49      
    1    189.44          93.59      

 

Player 1 Payoff matrix: 

           0              1      
    0    49.77          189.44      
    1     4.49          93.59      

 

Social Welfare Sum Matrix: 

           0              1      
    0    99.55          193.93      
    1    193.93          187.19      

 

Metagame probabilities: 
Player #0: 0.011  0.989  
Player #1: 0.011  0.989  
Iteration : 1
Time so far: 6249.4694945812225
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-08-02 14:09:33.314511: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.033940657414495944 65.73750190734863 0.6712324887514114 8.036443758010865 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035671422444283965 16.434339332580567 0.724754810333252 7.265228748321533 230348 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03404661100357771 18.018490409851076 0.7287876844406128 6.74030818939209 449187 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03418482765555382 15.850761032104492 0.7834675431251525 6.320935583114624 667390 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028167390264570713 28.30561218261719 0.6990742444992065 6.821197938919068 887210 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028307181037962436 26.77352180480957 0.7458455681800842 6.204669857025147 1105520 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026597657613456248 22.498068809509277 0.7492107629776001 5.970775604248047 1325057 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028912542387843132 20.810563945770262 0.8806108474731446 5.356106567382812 1545057 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027114031836390495 16.760782337188722 0.9344078719615936 5.189487504959106 1764408 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029119970090687276 14.513197898864746 1.012889379262924 5.126550197601318 1983566 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022004730254411697 15.811903095245361 0.923037838935852 5.060405683517456 2201916 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020828089490532876 19.815395832061768 0.9624984920024872 5.224142122268677 2420849 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01688347030431032 17.914250564575195 0.9029334127902985 5.216067171096801 2638310 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015837769024074078 20.174271202087404 0.9236821472644806 5.0566168308258055 2856079 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010235702898353339 17.685282707214355 0.8721910655498505 5.247018432617187 3073518 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007991212652996182 17.96290788650513 0.8660857796669006 5.261408090591431 3289760 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006854261551052332 14.608269691467285 0.8475730061531067 5.443266105651856 3508803 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0033103607478551567 18.82814121246338 0.8497624397277832 5.382548236846924 3728803 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010494373214896767 16.217521381378173 0.8078194916248321 5.386614847183227 3946449 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011463442495369236 16.212699794769286 0.6627610921859741 5.812159252166748 4163350 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010547699618655316 16.020226573944093 0.6459209203720093 5.966122674942016 4381084 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008107531753921649 17.03357629776001 0.6483801245689392 5.938079166412353 4597425 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005005874310882064 19.0594783782959 0.5744464874267579 6.067235898971558 4814465 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006226608529686928 15.03988790512085 0.5232838183641434 6.292285680770874 5033426 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007489407173125074 18.56145887374878 0.4520791471004486 6.504517984390259 5251640 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006965529537410475 16.255804824829102 0.37938250601291656 6.663940572738648 5468598 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008312866688356735 17.120295238494872 0.40150841772556306 7.05085301399231 5686242 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00030909896886441854 17.108902072906496 0.4053919643163681 6.933582735061646 5904941 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  2.908220048993826e-05 14.643126106262207 0.4266044914722443 6.938922500610351 6123244 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00057786651304923 21.934188079833984 0.408340585231781 7.516792488098145 6341841 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003020647331140935 19.81961269378662 0.4441856384277344 7.392975759506226 6561068 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001050629919336643 18.236455535888673 0.4136601358652115 7.468600416183472 6781068 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006275791326629587 15.779162788391114 0.4165277063846588 7.525255298614502 7000555 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0005540199766983278 16.97693614959717 0.33018586933612826 8.187617206573487 7219571 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013587679248303175 21.111112785339355 0.3205439388751984 8.40214729309082 7438125 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  3.5481208760757e-05 16.437842464447023 0.27210205644369123 8.544091129302979 7658125 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  4.2285754170734434e-05 19.670408630371092 0.2301938369870186 8.760038661956788 7877354 0


Pure best response payoff estimated to be 124.15 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 82.02 seconds to finish estimate with resulting utilities: [173.46   2.18]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 136.71 seconds to finish estimate with resulting utilities: [123.775  56.15 ]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 137.24 seconds to finish estimate with resulting utilities: [36.68 35.25]
Computing meta_strategies
Exited RRD with total regret 1.913942724792463 that was less than regret lambda 1.9310344827586208 after 89 iterations 
NEW LAMBDA 1.8620689655172415
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    49.77           4.49           2.18      
    1    189.44          93.59          56.15      
    2    173.46          123.78          35.97      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    49.77          189.44          173.46      
    1     4.49          93.59          123.78      
    2     2.18          56.15          35.97      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    99.55          193.93          175.64      
    1    193.93          187.19          179.93      
    2    175.64          179.93          71.93      

 

Metagame probabilities: 
Player #0: 0.0004  0.4423  0.5573  
Player #1: 0.0004  0.4423  0.5573  
Iteration : 2
Time so far: 15485.106102228165
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 16:43:29.199572: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017891112715005875 72.66836090087891 0.33777487874031065 10.174059772491455 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03308953884989023 16.784800052642822 0.6932628214359283 7.856283903121948 230703 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030635287053883076 18.237605381011964 0.6531542420387269 7.604209232330322 448520 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028191249631345273 19.652757740020753 0.6526971757411957 7.723931503295899 665866 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03163325283676386 16.122461128234864 0.7915580928325653 6.892486095428467 883295 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03026320394128561 19.682351303100585 0.8104324758052825 6.786299180984497 1102763 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029268776625394823 16.641690921783447 0.8447570919990539 6.828764295578003 1321467 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02698234301060438 19.71940040588379 0.8656204462051391 6.5094703197479244 1541467 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024912617355585098 14.568183612823486 0.8435729384422302 6.8604480743408205 1760743 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02227341029793024 18.187158393859864 0.8221203923225403 6.826059436798095 1979291 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0204979307949543 14.090840435028076 0.8309349536895752 6.446672630310059 2197568 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018087512254714964 14.558673477172851 0.8476567566394806 6.6083251953125 2417339 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015186937525868416 14.593542671203613 0.7812860667705536 6.777000856399536 2634215 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013465896341949701 18.37255048751831 0.826561713218689 6.66675500869751 2852205 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011335570365190506 20.221111679077147 0.8116456925868988 6.520674371719361 3069589 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008320251246914268 16.659894371032713 0.7943178236484527 6.850398063659668 3288164 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005221376661211252 18.16875476837158 0.779501986503601 6.707551574707031 3507330 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004164692154154182 16.877095890045165 0.7353850364685058 6.892206382751465 3725708 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021795667067635804 21.510720443725585 0.6862785279750824 7.463755798339844 3944274 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00015032482697279191 14.972067070007324 0.708171945810318 7.005509614944458 4164274 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006249160513107199 19.83034439086914 0.6669493675231933 7.623741912841797 4382553 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010969748225761577 18.33689193725586 0.6097110986709595 6.952051973342895 4601506 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011638133131782525 19.246867370605468 0.5214137554168701 7.899315118789673 4821285 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001091979367629392 16.384217166900633 0.5558975487947464 7.498419141769409 5041285 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004944514625094598 16.418069076538085 0.5201289296150208 7.701162672042846 5260604 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -8.40044900542125e-05 16.48562660217285 0.503337424993515 8.348437976837157 5480604 0
