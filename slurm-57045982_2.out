Job Id listed below:
57045986

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

/var/spool/slurmd.spool/job57045986/slurm_script: line 36: regret_lambda: command not found
2023-08-01 23:52:39.361337: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-01 23:52:45.511350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0801 23:52:58.915755 22742212545408 rl_environment.py:187] Using game instance: bargaining
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 32
discount_factor: 0.99
double: True
epsilon_decay_duration: 1000
hidden_layers_sizes: [50, 50]
learn_every: 10
learning_rate: 0.01
min_buffer_size_to_learn: 1000
num_actions: 121
optimizer_str: adam
replay_buffer_capacity: 10000
session: <tensorflow.python.client.session.Session object at 0x14aecc406d70>
state_representation_size: 93
symmetric: True
update_target_network_every: 1000


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.8
epochs_ppo: 5
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 20000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 1000
regret_calculation_steps: 200000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x14aecc406d70>
sims_per_entry: 5000
state_representation_size: 93
steps_fine_tune: 2000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-08-01 23:52:59.491186: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-08-01 23:53:00.114357: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1079 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 5000 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 2.74 seconds to finish estimate with resulting utilities: [1.8936 1.8604]
Exited RRD with total regret 0.0 that was less than regret lambda 0.5 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0     1.88      

 

Player 1 Payoff matrix: 

           0      
    0     1.88      

 

Social Welfare Sum Matrix: 

           0      
    0     3.75      

 

Iteration : 0
Time so far: 0.0001766681671142578
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-08-01 23:53:03.914325: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1941 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.24448949992656707 8.849095439910888 4.784422302246094 0.0006166701496113092 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.22045612207480839 9.806701056162517 4.662207108452207 0.15221447841447247 420054 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.1856301793238012 8.338949791978045 4.473230598031021 0.36961180703876334 820097 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.15238340334569822 7.443910536218862 4.259185840262742 0.6755054669657944 1220145 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.12326706891396531 6.950935541553262 4.098407271467609 1.0613413363595257 1620184 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.1016276713552894 6.673737894662536 3.9458800325299253 1.51603800945732 2020230 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08700826187456442 6.511322908559121 3.8072096292637596 1.9981974731971537 2420273 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07655254486710467 6.429660366612969 3.6875688258637775 2.490607174083876 2820318 0


Pure best response payoff estimated to be 7.437 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 5.91 seconds to finish estimate with resulting utilities: [7.2982 2.9814]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 8.99 seconds to finish estimate with resulting utilities: [4.8054 4.9288]
Computing meta_strategies
Exited RRD with total regret 0.4989016040415031 that was less than regret lambda 0.5 after 745 iterations 
NEW LAMBDA 0.4827586206896552
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0     1.88           2.98      
    1     7.30           4.87      

 

Player 1 Payoff matrix: 

           0              1      
    0     1.88           7.30      
    1     2.98           4.87      

 

Social Welfare Sum Matrix: 

           0              1      
    0     3.75          10.28      
    1    10.28           9.73      

 

Metagame probabilities: 
Player #0: 0.1097  0.8903  
Player #1: 0.1097  0.8903  
Iteration : 1
Time so far: 4979.7947199344635
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-08-02 01:16:04.020293: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3851 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0772827388167801 6.615198427858487 3.6828470273756646 2.5103058986413753 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08912614260650711 7.293581464849872 3.652465858871554 2.8312054101465756 420052 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09279622223105405 7.306639505480672 3.6297306996125442 3.118670628370959 820103 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09102906753668691 7.291970265029681 3.596118752319034 3.3898949852191653 1220150 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0854721659857377 7.283138344953726 3.556579242955457 3.648636336594129 1620191 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07907492387921969 7.273219589162464 3.473368646487717 3.924868469207845 2020232 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07342310158953178 7.261199272133922 3.3007280817468656 4.258504296267403 2420276 0


Pure best response payoff estimated to be 6.6626 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 5.75 seconds to finish estimate with resulting utilities: [7.4768 3.299 ]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 9.49 seconds to finish estimate with resulting utilities: [6.4996 4.2742]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 12.52 seconds to finish estimate with resulting utilities: [3.7654 3.2962]
Computing meta_strategies
Exited RRD with total regret 0.4823179906581583 that was less than regret lambda 0.4827586206896552 after 857 iterations 
NEW LAMBDA 0.4655172413793104
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0     1.88           2.98           3.30      
    1     7.30           4.87           4.27      
    2     7.48           6.50           3.53      

 

Player 1 Payoff matrix: 

           0              1              2      
    0     1.88           7.30           7.48      
    1     2.98           4.87           6.50      
    2     3.30           4.27           3.53      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0     3.75          10.28          10.78      
    1    10.28           9.73          10.77      
    2    10.78          10.77           7.06      

 

Metagame probabilities: 
Player #0: 0.0686  0.4011  0.5302  
Player #1: 0.0686  0.4011  0.5302  
Iteration : 2
Time so far: 9788.605114936829
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 02:36:12.767828: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5841 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07180941108862292 7.319876713557758 3.2312276273633467 4.386821496716314 20004 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07687566598843977 7.52248216863322 3.2073327075445115 4.55190595789278 420054 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0788393889993181 7.402569485482275 3.206189644240253 4.6822616205316585 820085 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07789857120396541 7.2591577371927745 3.2049479430388534 4.808376004854674 1220117 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07473679931473169 7.136014246735669 3.200238211260826 4.937701939449684 1620158 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07099507713793644 7.013499339292366 3.186387443364797 5.078888568909902 2020202 0


Pure best response payoff estimated to be 6.761 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 5.58 seconds to finish estimate with resulting utilities: [7.246 3.694]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 7.63 seconds to finish estimate with resulting utilities: [6.4152 5.5522]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 6.39 seconds to finish estimate with resulting utilities: [6.7926 6.6612]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 5.52 seconds to finish estimate with resulting utilities: [7.0856 6.6216]
Computing meta_strategies
Exited RRD with total regret 0.46510307918455496 that was less than regret lambda 0.4655172413793104 after 1627 iterations 
NEW LAMBDA 0.4482758620689656
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0     1.88           2.98           3.30           3.69      
    1     7.30           4.87           4.27           5.55      
    2     7.48           6.50           3.53           6.66      
    3     7.25           6.42           6.79           6.85      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0     1.88           7.30           7.48           7.25      
    1     2.98           4.87           6.50           6.42      
    2     3.30           4.27           3.53           6.79      
    3     3.69           5.55           6.66           6.85      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0     3.75          10.28          10.78          10.94      
    1    10.28           9.73          10.77          11.97      
    2    10.78          10.77           7.06          13.45      
    3    10.94          11.97          13.45          13.71      

 

Metagame probabilities: 
Player #0: 0.0029  0.0597  0.1807  0.7568  
Player #1: 0.0029  0.0597  0.1807  0.7568  
Iteration : 3
Time so far: 15135.936800479889
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-08-02 04:05:20.312972: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7831 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06825283834204521 6.992735316525442 3.1559333224167196 5.219336779671496 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07224969030112266 7.087291252408039 3.149201243354296 5.3189099095137795 420041 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07341843011742472 6.879008540224973 3.1291101340248098 5.436065397893018 820075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07269392525480649 6.669148907832118 3.1001175474667177 5.57974174476529 1220107 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.070623993245156 6.468934183253466 3.0677751170108354 5.744555169157148 1620134 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06787508594824668 6.284071494030022 3.0239142220245494 5.926386328844616 2020165 0


Pure best response payoff estimated to be 7.5064 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 5.63 seconds to finish estimate with resulting utilities: [7.1586 3.9226]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 8.04 seconds to finish estimate with resulting utilities: [6.3148 5.618 ]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 5.84 seconds to finish estimate with resulting utilities: [6.8794 6.8722]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 4.96 seconds to finish estimate with resulting utilities: [7.5756 6.7916]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 5.33 seconds to finish estimate with resulting utilities: [7.4956 6.7428]
Computing meta_strategies
Exited RRD with total regret 0.4480309881894957 that was less than regret lambda 0.4482758620689656 after 2014 iterations 
NEW LAMBDA 0.4310344827586208
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0     1.88           2.98           3.30           3.69           3.92      
    1     7.30           4.87           4.27           5.55           5.62      
    2     7.48           6.50           3.53           6.66           6.87      
    3     7.25           6.42           6.79           6.85           6.79      
    4     7.16           6.31           6.88           7.58           7.12      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0     1.88           7.30           7.48           7.25           7.16      
    1     2.98           4.87           6.50           6.42           6.31      
    2     3.30           4.27           3.53           6.79           6.88      
    3     3.69           5.55           6.66           6.85           7.58      
    4     3.92           5.62           6.87           6.79           7.12      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0     3.75          10.28          10.78          10.94          11.08      
    1    10.28           9.73          10.77          11.97          11.93      
    2    10.78          10.77           7.06          13.45          13.75      
    3    10.94          11.97          13.45          13.71          14.37      
    4    11.08          11.93          13.75          14.37          14.24      

 

Metagame probabilities: 
Player #0: 0.0005  0.0167  0.0984  0.2921  0.5924  
Player #1: 0.0005  0.0167  0.0984  0.2921  0.5924  
Iteration : 4
Time so far: 21970.252218961716
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-08-02 05:59:14.740863: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9821 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06593885416783146 6.227804443755633 2.9811461965914514 6.085701074945812 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06704655678625579 6.2668084320101185 2.949853913015891 6.225513489346252 420035 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06687096602055063 6.112360780410345 2.9164353930159828 6.337875610078065 820069 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06573716859602621 5.9617617731195995 2.8818099287962196 6.454219063395127 1220102 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06382731373130585 5.798627974034174 2.8453742345549027 6.583000291481274 1620135 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.061717368323078996 5.641364420409226 2.8080682468355 6.723363006256946 2020166 0
Recovering previous policy with expected return of 7.4245754245754245. Long term value was 7.2528 and short term was 7.247.


Pure best response payoff estimated to be 7.6512 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 5.71 seconds to finish estimate with resulting utilities: [7.1568 3.8638]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 7.97 seconds to finish estimate with resulting utilities: [6.3452 5.6614]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 5.87 seconds to finish estimate with resulting utilities: [6.9124 6.8896]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 4.94 seconds to finish estimate with resulting utilities: [7.5954 6.7878]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 5.26 seconds to finish estimate with resulting utilities: [7.4982 6.7994]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 5.29 seconds to finish estimate with resulting utilities: [7.4582 6.8384]
Computing meta_strategies
Exited RRD with total regret 0.4309469304784326 that was less than regret lambda 0.4310344827586208 after 2411 iterations 
NEW LAMBDA 0.41379310344827597
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0     1.88           2.98           3.30           3.69           3.92           3.86      
    1     7.30           4.87           4.27           5.55           5.62           5.66      
    2     7.48           6.50           3.53           6.66           6.87           6.89      
    3     7.25           6.42           6.79           6.85           6.79           6.79      
    4     7.16           6.31           6.88           7.58           7.12           6.80      
    5     7.16           6.35           6.91           7.60           7.50           7.15      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0     1.88           7.30           7.48           7.25           7.16           7.16      
    1     2.98           4.87           6.50           6.42           6.31           6.35      
    2     3.30           4.27           3.53           6.79           6.88           6.91      
    3     3.69           5.55           6.66           6.85           7.58           7.60      
    4     3.92           5.62           6.87           6.79           7.12           7.50      
    5     3.86           5.66           6.89           6.79           6.80           7.15      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0     3.75          10.28          10.78          10.94          11.08          11.02      
    1    10.28           9.73          10.77          11.97          11.93          12.01      
    2    10.78          10.77           7.06          13.45          13.75          13.80      
    3    10.94          11.97          13.45          13.71          14.37          14.38      
    4    11.08          11.93          13.75          14.37          14.24          14.30      
    5    11.02          12.01          13.80          14.38          14.30          14.30      

 

Metagame probabilities: 
Player #0: 0.0001  0.0063  0.0662  0.1602  0.2811  0.486  
Player #1: 0.0001  0.0063  0.0662  0.1602  0.2811  0.486  
Iteration : 5
Time so far: 28836.722649097443
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 07:53:41.214401: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11811 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05998909901808016 5.567622933836229 2.7712087276760022 6.864702672517225 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06090116555206213 5.615391376215349 2.7485780070225396 6.954067863708384 420033 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06094769706081489 5.507742415292025 2.7274435700336612 7.032556332205396 820070 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06024262974911238 5.397591324163672 2.7070245800217574 7.112710863617196 1220101 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05883364431333387 5.296609640953547 2.6858756040321117 7.2016071711541265 1620133 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.057206767008789634 5.197892097010176 2.6632306514106627 7.300430306374237 2020166 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.055666426512777215 5.106655078547984 2.639916227617354 7.405374223719744 2420194 0
Recovering previous policy with expected return of 7.425574425574426. Long term value was 7.137 and short term was 7.186.


Pure best response payoff estimated to be 7.6902 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 5.83 seconds to finish estimate with resulting utilities: [7.1118 4.015 ]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 8.21 seconds to finish estimate with resulting utilities: [6.3262 5.6282]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 6.13 seconds to finish estimate with resulting utilities: [6.8906 6.8754]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 5.18 seconds to finish estimate with resulting utilities: [7.5946 6.8298]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 5.44 seconds to finish estimate with resulting utilities: [7.546  6.7936]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 5.47 seconds to finish estimate with resulting utilities: [7.488  6.7808]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 5.55 seconds to finish estimate with resulting utilities: [7.5074 6.7638]
Computing meta_strategies
Exited RRD with total regret 0.41375731121691395 that was less than regret lambda 0.41379310344827597 after 3011 iterations 
NEW LAMBDA 0.39655172413793116
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0     1.88           2.98           3.30           3.69           3.92           3.86           4.01      
    1     7.30           4.87           4.27           5.55           5.62           5.66           5.63      
    2     7.48           6.50           3.53           6.66           6.87           6.89           6.88      
    3     7.25           6.42           6.79           6.85           6.79           6.79           6.83      
    4     7.16           6.31           6.88           7.58           7.12           6.80           6.79      
    5     7.16           6.35           6.91           7.60           7.50           7.15           6.78      
    6     7.11           6.33           6.89           7.59           7.55           7.49           7.14      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0     1.88           7.30           7.48           7.25           7.16           7.16           7.11      
    1     2.98           4.87           6.50           6.42           6.31           6.35           6.33      
    2     3.30           4.27           3.53           6.79           6.88           6.91           6.89      
    3     3.69           5.55           6.66           6.85           7.58           7.60           7.59      
    4     3.92           5.62           6.87           6.79           7.12           7.50           7.55      
    5     3.86           5.66           6.89           6.79           6.80           7.15           7.49      
    6     4.01           5.63           6.88           6.83           6.79           6.78           7.14      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0     3.75          10.28          10.78          10.94          11.08          11.02          11.13      
    1    10.28           9.73          10.77          11.97          11.93          12.01          11.95      
    2    10.78          10.77           7.06          13.45          13.75          13.80          13.77      
    3    10.94          11.97          13.45          13.71          14.37          14.38          14.42      
    4    11.08          11.93          13.75          14.37          14.24          14.30          14.34      
    5    11.02          12.01          13.80          14.38          14.30          14.30          14.27      
    6    11.13          11.95          13.77          14.42          14.34          14.27          14.27      

 

Metagame probabilities: 
Player #0: 0.0001  0.0019  0.0443  0.0992  0.1591  0.2509  0.4444  
Player #1: 0.0001  0.0019  0.0443  0.0992  0.1591  0.2509  0.4444  
Iteration : 6
Time so far: 35962.356291770935
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-08-02 09:52:27.144764: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13801 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05557049179095405 5.15806717864619 2.6363574625581703 7.420699643451609 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.056518733973384826 5.2035475742427355 2.62229637118726 7.475778007323167 420038 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05679653672236408 5.0945075566070095 2.6100356352632974 7.504719949044728 820069 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.056371763369943186 4.985604263314549 2.5967028481989916 7.532179237332879 1220097 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.055372827236163616 4.882774077249295 2.585915625348236 7.555976798609273 1620133 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.054196370009244835 4.785433952970617 2.5781395151840862 7.586439383751995 2020176 0


Pure best response payoff estimated to be 7.682 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 6.05 seconds to finish estimate with resulting utilities: [7.4016 3.7622]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 9.2 seconds to finish estimate with resulting utilities: [6.2612 5.1726]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 7.22 seconds to finish estimate with resulting utilities: [6.6672 6.46  ]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 5.25 seconds to finish estimate with resulting utilities: [7.72   6.7232]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 5.59 seconds to finish estimate with resulting utilities: [7.6104 6.6374]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 5.6 seconds to finish estimate with resulting utilities: [7.6086 6.6128]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 5.54 seconds to finish estimate with resulting utilities: [7.6166 6.6076]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 5.7 seconds to finish estimate with resulting utilities: [7.703  6.7116]
Computing meta_strategies
Exited RRD with total regret 0.39649660403572184 that was less than regret lambda 0.39655172413793116 after 4293 iterations 
NEW LAMBDA 0.3793103448275863
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0     1.88           2.98           3.30           3.69           3.92           3.86           4.01           3.76      
    1     7.30           4.87           4.27           5.55           5.62           5.66           5.63           5.17      
    2     7.48           6.50           3.53           6.66           6.87           6.89           6.88           6.46      
    3     7.25           6.42           6.79           6.85           6.79           6.79           6.83           6.72      
    4     7.16           6.31           6.88           7.58           7.12           6.80           6.79           6.64      
    5     7.16           6.35           6.91           7.60           7.50           7.15           6.78           6.61      
    6     7.11           6.33           6.89           7.59           7.55           7.49           7.14           6.61      
    7     7.40           6.26           6.67           7.72           7.61           7.61           7.62           7.21      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0     1.88           7.30           7.48           7.25           7.16           7.16           7.11           7.40      
    1     2.98           4.87           6.50           6.42           6.31           6.35           6.33           6.26      
    2     3.30           4.27           3.53           6.79           6.88           6.91           6.89           6.67      
    3     3.69           5.55           6.66           6.85           7.58           7.60           7.59           7.72      
    4     3.92           5.62           6.87           6.79           7.12           7.50           7.55           7.61      
    5     3.86           5.66           6.89           6.79           6.80           7.15           7.49           7.61      
    6     4.01           5.63           6.88           6.83           6.79           6.78           7.14           7.62      
    7     3.76           5.17           6.46           6.72           6.64           6.61           6.61           7.21      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0     3.75          10.28          10.78          10.94          11.08          11.02          11.13          11.16      
    1    10.28           9.73          10.77          11.97          11.93          12.01          11.95          11.43      
    2    10.78          10.77           7.06          13.45          13.75          13.80          13.77          13.13      
    3    10.94          11.97          13.45          13.71          14.37          14.38          14.42          14.44      
    4    11.08          11.93          13.75          14.37          14.24          14.30          14.34          14.25      
    5    11.02          12.01          13.80          14.38          14.30          14.30          14.27          14.22      
    6    11.13          11.95          13.77          14.42          14.34          14.27          14.27          14.22      
    7    11.16          11.43          13.13          14.44          14.25          14.22          14.22          14.41      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0144  0.0457  0.063  0.0919  0.1508  0.6341  
Player #1: 0.0001  0.0001  0.0144  0.0457  0.063  0.0919  0.1508  0.6341  
Iteration : 7
Time so far: 42780.86544537544
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-08-02 11:46:05.861172: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15791 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05324542464771027 4.745707814409225 2.568492740371989 7.625406562456684 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05467647115110638 4.786772527144506 2.5671204382275565 7.633530723080135 420033 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.055027433351570676 4.699636352998493 2.559151144767493 7.653671572748831 820070 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.054672686320522274 4.613898765073194 2.549236769187244 7.68424675562916 1220106 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05377253208460244 4.531190145817601 2.5383564327796133 7.723959600776208 1620146 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05268185530444878 4.451174968221376 2.5257715782498424 7.7749542909561615 2020170 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05162310397014133 4.372780181582623 2.511316203616741 7.834133015853871 2420203 0


Pure best response payoff estimated to be 7.7316 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (8, 0)
Current player 0 and current strategies (8, 0) took 6.21 seconds to finish estimate with resulting utilities: [7.3952 3.4894]
Estimating current strategies:  (8, 1)
Current player 0 and current strategies (8, 1) took 9.66 seconds to finish estimate with resulting utilities: [6.189 4.879]
Estimating current strategies:  (8, 2)
Current player 0 and current strategies (8, 2) took 7.9 seconds to finish estimate with resulting utilities: [6.4326 6.0702]
Estimating current strategies:  (8, 3)
Current player 0 and current strategies (8, 3) took 5.42 seconds to finish estimate with resulting utilities: [7.6826 6.6996]
Estimating current strategies:  (8, 4)
Current player 0 and current strategies (8, 4) took 5.49 seconds to finish estimate with resulting utilities: [7.6334 6.802 ]
Estimating current strategies:  (8, 5)
Current player 0 and current strategies (8, 5) took 5.36 seconds to finish estimate with resulting utilities: [7.6994 6.7576]
Estimating current strategies:  (8, 6)
Current player 0 and current strategies (8, 6) took 5.45 seconds to finish estimate with resulting utilities: [7.6392 6.7868]
Estimating current strategies:  (8, 7)
Current player 0 and current strategies (8, 7) took 5.63 seconds to finish estimate with resulting utilities: [7.6806 6.7992]
Estimating current strategies:  (8, 8)
Current player 0 and current strategies (8, 8) took 5.36 seconds to finish estimate with resulting utilities: [7.704  6.7442]
Computing meta_strategies
Exited RRD with total regret 0.379253695608158 that was less than regret lambda 0.3793103448275863 after 4577 iterations 
NEW LAMBDA 0.3620689655172415
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0     1.88           2.98           3.30           3.69           3.92           3.86           4.01           3.76           3.49      
    1     7.30           4.87           4.27           5.55           5.62           5.66           5.63           5.17           4.88      
    2     7.48           6.50           3.53           6.66           6.87           6.89           6.88           6.46           6.07      
    3     7.25           6.42           6.79           6.85           6.79           6.79           6.83           6.72           6.70      
    4     7.16           6.31           6.88           7.58           7.12           6.80           6.79           6.64           6.80      
    5     7.16           6.35           6.91           7.60           7.50           7.15           6.78           6.61           6.76      
    6     7.11           6.33           6.89           7.59           7.55           7.49           7.14           6.61           6.79      
    7     7.40           6.26           6.67           7.72           7.61           7.61           7.62           7.21           6.80      
    8     7.40           6.19           6.43           7.68           7.63           7.70           7.64           7.68           7.22      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0     1.88           7.30           7.48           7.25           7.16           7.16           7.11           7.40           7.40      
    1     2.98           4.87           6.50           6.42           6.31           6.35           6.33           6.26           6.19      
    2     3.30           4.27           3.53           6.79           6.88           6.91           6.89           6.67           6.43      
    3     3.69           5.55           6.66           6.85           7.58           7.60           7.59           7.72           7.68      
    4     3.92           5.62           6.87           6.79           7.12           7.50           7.55           7.61           7.63      
    5     3.86           5.66           6.89           6.79           6.80           7.15           7.49           7.61           7.70      
    6     4.01           5.63           6.88           6.83           6.79           6.78           7.14           7.62           7.64      
    7     3.76           5.17           6.46           6.72           6.64           6.61           6.61           7.21           7.68      
    8     3.49           4.88           6.07           6.70           6.80           6.76           6.79           6.80           7.22      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8      
    0     3.75          10.28          10.78          10.94          11.08          11.02          11.13          11.16          10.88      
    1    10.28           9.73          10.77          11.97          11.93          12.01          11.95          11.43          11.07      
    2    10.78          10.77           7.06          13.45          13.75          13.80          13.77          13.13          12.50      
    3    10.94          11.97          13.45          13.71          14.37          14.38          14.42          14.44          14.38      
    4    11.08          11.93          13.75          14.37          14.24          14.30          14.34          14.25          14.44      
    5    11.02          12.01          13.80          14.38          14.30          14.30          14.27          14.22          14.46      
    6    11.13          11.95          13.77          14.42          14.34          14.27          14.27          14.22          14.43      
    7    11.16          11.43          13.13          14.44          14.25          14.22          14.22          14.41          14.48      
    8    10.88          11.07          12.50          14.38          14.44          14.46          14.43          14.48          14.45      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0043  0.0258  0.0405  0.0514  0.0782  0.1997  0.6  
Player #1: 0.0001  0.0001  0.0043  0.0258  0.0405  0.0514  0.0782  0.1997  0.6  
Iteration : 8
Time so far: 49699.64486837387
Approximating Best Response
Training best response:  True 0.06
rtg values check:  (?,) (?, 1)
2023-08-02 13:41:24.826818: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_62/weights_2/Adam_1/Assign' id:17781 op device:{requested: '', assigned: ''} def:{{{node mlp_62/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_62/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_62/weights_2/Adam_1, mlp_62/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05160889339797226 4.41072457189734 2.5098165841724036 7.8401243500509095 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05301098473858478 4.46376203626809 2.5135866963661924 7.839168056818199 420035 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.053728330044133625 4.391020601215186 2.5170030342266108 7.836846641027696 820063 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05373722087285213 4.320710257732846 2.517467570880403 7.839037188747214 1220102 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05308921895686709 4.252351100119372 2.512474201706292 7.849504892242135 1620139 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05223386719113299 4.189026926987518 2.506323903727707 7.867533693306816 2020173 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.051390846565216454 4.1304070841608596 2.498107179234084 7.889592779870298 2420215 0


Pure best response payoff estimated to be 7.7606 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (9, 0)
Current player 0 and current strategies (9, 0) took 6.42 seconds to finish estimate with resulting utilities: [7.4394 3.4046]
Estimating current strategies:  (9, 1)
Current player 0 and current strategies (9, 1) took 9.99 seconds to finish estimate with resulting utilities: [6.1404 4.6742]
Estimating current strategies:  (9, 2)
Current player 0 and current strategies (9, 2) took 9.4 seconds to finish estimate with resulting utilities: [5.7038 5.2308]
Estimating current strategies:  (9, 3)
Current player 0 and current strategies (9, 3) took 5.37 seconds to finish estimate with resulting utilities: [7.763  6.5752]
Estimating current strategies:  (9, 4)
Current player 0 and current strategies (9, 4) took 5.52 seconds to finish estimate with resulting utilities: [7.7356 6.585 ]
Estimating current strategies:  (9, 5)
Current player 0 and current strategies (9, 5) took 5.59 seconds to finish estimate with resulting utilities: [7.6996 6.616 ]
Estimating current strategies:  (9, 6)
Current player 0 and current strategies (9, 6) took 5.49 seconds to finish estimate with resulting utilities: [7.7762 6.5352]
Estimating current strategies:  (9, 7)
Current player 0 and current strategies (9, 7) took 5.53 seconds to finish estimate with resulting utilities: [7.7808 6.6172]
Estimating current strategies:  (9, 8)
Current player 0 and current strategies (9, 8) took 5.94 seconds to finish estimate with resulting utilities: [7.564  6.4964]
Estimating current strategies:  (9, 9)
Current player 0 and current strategies (9, 9) took 9.1 seconds to finish estimate with resulting utilities: [5.7326 5.3786]
Computing meta_strategies
Exited RRD with total regret 0.36201447494719474 that was less than regret lambda 0.3620689655172415 after 3596 iterations 
NEW LAMBDA 0.3448275862068967
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     1.88           2.98           3.30           3.69           3.92           3.86           4.01           3.76           3.49           3.40      
    1     7.30           4.87           4.27           5.55           5.62           5.66           5.63           5.17           4.88           4.67      
    2     7.48           6.50           3.53           6.66           6.87           6.89           6.88           6.46           6.07           5.23      
    3     7.25           6.42           6.79           6.85           6.79           6.79           6.83           6.72           6.70           6.58      
    4     7.16           6.31           6.88           7.58           7.12           6.80           6.79           6.64           6.80           6.58      
    5     7.16           6.35           6.91           7.60           7.50           7.15           6.78           6.61           6.76           6.62      
    6     7.11           6.33           6.89           7.59           7.55           7.49           7.14           6.61           6.79           6.54      
    7     7.40           6.26           6.67           7.72           7.61           7.61           7.62           7.21           6.80           6.62      
    8     7.40           6.19           6.43           7.68           7.63           7.70           7.64           7.68           7.22           6.50      
    9     7.44           6.14           5.70           7.76           7.74           7.70           7.78           7.78           7.56           5.56      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     1.88           7.30           7.48           7.25           7.16           7.16           7.11           7.40           7.40           7.44      
    1     2.98           4.87           6.50           6.42           6.31           6.35           6.33           6.26           6.19           6.14      
    2     3.30           4.27           3.53           6.79           6.88           6.91           6.89           6.67           6.43           5.70      
    3     3.69           5.55           6.66           6.85           7.58           7.60           7.59           7.72           7.68           7.76      
    4     3.92           5.62           6.87           6.79           7.12           7.50           7.55           7.61           7.63           7.74      
    5     3.86           5.66           6.89           6.79           6.80           7.15           7.49           7.61           7.70           7.70      
    6     4.01           5.63           6.88           6.83           6.79           6.78           7.14           7.62           7.64           7.78      
    7     3.76           5.17           6.46           6.72           6.64           6.61           6.61           7.21           7.68           7.78      
    8     3.49           4.88           6.07           6.70           6.80           6.76           6.79           6.80           7.22           7.56      
    9     3.40           4.67           5.23           6.58           6.58           6.62           6.54           6.62           6.50           5.56      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     3.75          10.28          10.78          10.94          11.08          11.02          11.13          11.16          10.88          10.84      
    1    10.28           9.73          10.77          11.97          11.93          12.01          11.95          11.43          11.07          10.81      
    2    10.78          10.77           7.06          13.45          13.75          13.80          13.77          13.13          12.50          10.93      
    3    10.94          11.97          13.45          13.71          14.37          14.38          14.42          14.44          14.38          14.34      
    4    11.08          11.93          13.75          14.37          14.24          14.30          14.34          14.25          14.44          14.32      
    5    11.02          12.01          13.80          14.38          14.30          14.30          14.27          14.22          14.46          14.32      
    6    11.13          11.95          13.77          14.42          14.34          14.27          14.27          14.22          14.43          14.31      
    7    11.16          11.43          13.13          14.44          14.25          14.22          14.22          14.41          14.48          14.40      
    8    10.88          11.07          12.50          14.38          14.44          14.46          14.43          14.48          14.45          14.06      
    9    10.84          10.81          10.93          14.34          14.32          14.32          14.31          14.40          14.06          11.11      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0061  0.0413  0.0575  0.0725  0.093  0.1845  0.3102  0.2347  
Player #1: 0.0001  0.0001  0.0061  0.0413  0.0575  0.0725  0.093  0.1845  0.3102  0.2347  
Iteration : 9
Time so far: 56687.802198410034
Approximating Best Response
Training best response:  True 0.05500000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 15:37:53.076406: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_69/weights_2/Adam_1/Assign' id:19771 op device:{requested: '', assigned: ''} def:{{{node mlp_69/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_69/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_69/weights_2/Adam_1, mlp_69/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05129269056753197 4.151325438867057 2.4959130574571025 7.895684602646675 20005 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.052374140969506947 4.230443868950128 2.491328679856529 7.924064646393109 420041 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05295407301454261 4.191982442420219 2.485321404191331 7.959020438515157 820074 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05302484678354303 4.151547934828814 2.4797444870948384 7.9952740517381535 1220116 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05253880326158405 4.112210923683533 2.474291361294105 8.029449555177054 1620144 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05176867088185932 4.075101861753905 2.46926427781828 8.065906500601573 2020181 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05098342149320503 4.041994456994059 2.460851719730952 8.11036698417468 2420213 0


Pure best response payoff estimated to be 7.6264 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (10, 0)
Current player 0 and current strategies (10, 0) took 6.24 seconds to finish estimate with resulting utilities: [7.415  3.4344]
Estimating current strategies:  (10, 1)
Current player 0 and current strategies (10, 1) took 9.82 seconds to finish estimate with resulting utilities: [6.2584 4.6156]
Estimating current strategies:  (10, 2)
Current player 0 and current strategies (10, 2) took 8.54 seconds to finish estimate with resulting utilities: [6.2096 5.9238]
Estimating current strategies:  (10, 3)
Current player 0 and current strategies (10, 3) took 5.02 seconds to finish estimate with resulting utilities: [7.7402 6.627 ]
Estimating current strategies:  (10, 4)
Current player 0 and current strategies (10, 4) took 5.23 seconds to finish estimate with resulting utilities: [7.6236 6.5642]
Estimating current strategies:  (10, 5)
Current player 0 and current strategies (10, 5) took 5.13 seconds to finish estimate with resulting utilities: [7.6756 6.5324]
Estimating current strategies:  (10, 6)
Current player 0 and current strategies (10, 6) took 5.19 seconds to finish estimate with resulting utilities: [7.624  6.6002]
Estimating current strategies:  (10, 7)
Current player 0 and current strategies (10, 7) took 5.28 seconds to finish estimate with resulting utilities: [7.6842 6.6302]
Estimating current strategies:  (10, 8)
Current player 0 and current strategies (10, 8) took 5.23 seconds to finish estimate with resulting utilities: [7.7168 6.6258]
Estimating current strategies:  (10, 9)
Current player 0 and current strategies (10, 9) took 7.03 seconds to finish estimate with resulting utilities: [6.858 6.255]
Estimating current strategies:  (10, 10)
Current player 0 and current strategies (10, 10) took 5.88 seconds to finish estimate with resulting utilities: [7.584  6.6504]
Computing meta_strategies
Exited RRD with total regret 0.3447210214530241 that was less than regret lambda 0.3448275862068967 after 5872 iterations 
NEW LAMBDA 0.3275862068965518
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     1.88           2.98           3.30           3.69           3.92           3.86           4.01           3.76           3.49           3.40           3.43      
    1     7.30           4.87           4.27           5.55           5.62           5.66           5.63           5.17           4.88           4.67           4.62      
    2     7.48           6.50           3.53           6.66           6.87           6.89           6.88           6.46           6.07           5.23           5.92      
    3     7.25           6.42           6.79           6.85           6.79           6.79           6.83           6.72           6.70           6.58           6.63      
    4     7.16           6.31           6.88           7.58           7.12           6.80           6.79           6.64           6.80           6.58           6.56      
    5     7.16           6.35           6.91           7.60           7.50           7.15           6.78           6.61           6.76           6.62           6.53      
    6     7.11           6.33           6.89           7.59           7.55           7.49           7.14           6.61           6.79           6.54           6.60      
    7     7.40           6.26           6.67           7.72           7.61           7.61           7.62           7.21           6.80           6.62           6.63      
    8     7.40           6.19           6.43           7.68           7.63           7.70           7.64           7.68           7.22           6.50           6.63      
    9     7.44           6.14           5.70           7.76           7.74           7.70           7.78           7.78           7.56           5.56           6.25      
   10     7.42           6.26           6.21           7.74           7.62           7.68           7.62           7.68           7.72           6.86           7.12      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     1.88           7.30           7.48           7.25           7.16           7.16           7.11           7.40           7.40           7.44           7.42      
    1     2.98           4.87           6.50           6.42           6.31           6.35           6.33           6.26           6.19           6.14           6.26      
    2     3.30           4.27           3.53           6.79           6.88           6.91           6.89           6.67           6.43           5.70           6.21      
    3     3.69           5.55           6.66           6.85           7.58           7.60           7.59           7.72           7.68           7.76           7.74      
    4     3.92           5.62           6.87           6.79           7.12           7.50           7.55           7.61           7.63           7.74           7.62      
    5     3.86           5.66           6.89           6.79           6.80           7.15           7.49           7.61           7.70           7.70           7.68      
    6     4.01           5.63           6.88           6.83           6.79           6.78           7.14           7.62           7.64           7.78           7.62      
    7     3.76           5.17           6.46           6.72           6.64           6.61           6.61           7.21           7.68           7.78           7.68      
    8     3.49           4.88           6.07           6.70           6.80           6.76           6.79           6.80           7.22           7.56           7.72      
    9     3.40           4.67           5.23           6.58           6.58           6.62           6.54           6.62           6.50           5.56           6.86      
   10     3.43           4.62           5.92           6.63           6.56           6.53           6.60           6.63           6.63           6.25           7.12      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     3.75          10.28          10.78          10.94          11.08          11.02          11.13          11.16          10.88          10.84          10.85      
    1    10.28           9.73          10.77          11.97          11.93          12.01          11.95          11.43          11.07          10.81          10.87      
    2    10.78          10.77           7.06          13.45          13.75          13.80          13.77          13.13          12.50          10.93          12.13      
    3    10.94          11.97          13.45          13.71          14.37          14.38          14.42          14.44          14.38          14.34          14.37      
    4    11.08          11.93          13.75          14.37          14.24          14.30          14.34          14.25          14.44          14.32          14.19      
    5    11.02          12.01          13.80          14.38          14.30          14.30          14.27          14.22          14.46          14.32          14.21      
    6    11.13          11.95          13.77          14.42          14.34          14.27          14.27          14.22          14.43          14.31          14.22      
    7    11.16          11.43          13.13          14.44          14.25          14.22          14.22          14.41          14.48          14.40          14.31      
    8    10.88          11.07          12.50          14.38          14.44          14.46          14.43          14.48          14.45          14.06          14.34      
    9    10.84          10.81          10.93          14.34          14.32          14.32          14.31          14.40          14.06          11.11          13.11      
   10    10.85          10.87          12.13          14.37          14.19          14.21          14.22          14.31          14.34          13.11          14.23      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0004  0.0136  0.0172  0.0204  0.0307  0.0698  0.1304  0.0518  0.6655  
Player #1: 0.0001  0.0001  0.0004  0.0136  0.0172  0.0204  0.0307  0.0698  0.1304  0.0518  0.6655  
Iteration : 10
Time so far: 63738.84110569954
Approximating Best Response
Training best response:  True 0.05
rtg values check:  (?,) (?, 1)
2023-08-02 17:35:25.009958: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_76/weights_2/Adam_1/Assign' id:21761 op device:{requested: '', assigned: ''} def:{{{node mlp_76/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_76/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_76/weights_2/Adam_1, mlp_76/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05090692709366785 4.0696053665997045 2.4589928786522948 8.119597134958704 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05172282056776159 4.149129794836044 2.45517827137058 8.141045949299327 420042 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05212628298564034 4.107277021814795 2.453571047619277 8.159256016639468 820077 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05207826589731159 4.065121161528076 2.45456557325875 8.1739690869396 1220116 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05154659534019136 4.022394466023934 2.4556687586797055 8.18885052514332 1620149 0
slurmstepd: error: *** JOB 57045986 ON gl3116 CANCELLED AT 2023-08-02T18:18:52 ***
