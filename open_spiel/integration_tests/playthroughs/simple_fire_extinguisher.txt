game: simple_fire_extinguisher

GameType.chance_mode = ChanceMode.EXPLICIT_STOCHASTIC
GameType.dynamics = Dynamics.SIMULTANEOUS
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "Simple fire extinguisher game"
GameType.max_num_players = 2
GameType.min_num_players = 2
GameType.parameter_specification = ["grid_size", "max_game_length"]
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = False
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = False
GameType.reward_model = RewardModel.REWARDS
GameType.short_name = "simple_fire_extinguisher"
GameType.utility = Utility.GENERAL_SUM

NumDistinctActions() = 4
PolicyTensorShape() = [4]
MaxChanceOutcomes() = 2
GetParameters() = {grid_size=5,max_game_length=40}
NumPlayers() = 2
MinUtility() = -461.32
MaxUtility() = 10.0
UtilitySum() = 0.0
InformationStateTensorShape() = [4]
InformationStateTensorLayout() = TensorLayout.CHW
InformationStateTensorSize() = 4
ObservationTensorShape() = [4]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 4
MaxGameLength() = 40
ToString() = "simple_fire_extinguisher(grid_size=5,max_game_length=40)"

# State 0
# p0: p1:
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = PlayerId.SIMULTANEOUS
InformationStateString(0) = "us: op:"
InformationStateString(1) = "us: op:"
InformationStateTensor(0).observation = [0, 0, 4, 4, 0]
InformationStateTensor(1).observation = [0, 0, 4, 4, 0]
ObservationString(0) = "us: op:"
ObservationString(1) = "us: op:"
ObservationTensor(0) = [0, 0, 4, 4, 0]
ObservationTensor(1) = [0, 0, 4, 4, 0]
Rewards() = [0, 0]
Returns() = [0, 0]
LegalActions(0) = [0, 1, 2, 3]
LegalActions(1) = [0, 1, 2, 3]
StringLegalActions(0) = ["LEFT", "RIGHT", "UP", "DOWN"]
StringLegalActions(1) = ["LEFT", "RIGHT", "UP", "DOWN"]

# Apply joint action ["RIGHT", "UP"]
actions: [1, 2]

# State 1
# p0:R p1:U
IsTerminal() = False
History() = [1, 2]
HistoryString() = "1, 2"
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = PlayerId.SIMULTANEOUS
InformationStateString(0) = "us:R op:U"
InformationStateString(1) = "us:U op:R"
InformationStateTensor(0).observation = [-1, 1, 3, 4, 0]
InformationStateTensor(1).observation = [1, -1, 4, 3, 0]
ObservationString(0) = "us:R op:U"
ObservationString(1) = "us:U op:R"
ObservationTensor(0) = [-1, 1, 3, 4, 0]
ObservationTensor(1) = [1, -1, 4, 3, 0]
Rewards() = [-0.45, -0.45]
Returns() = [-0.45, -0.45]
LegalActions(0) = [0, 1, 2, 3]
LegalActions(1) = [0, 1, 2, 3]
StringLegalActions(0) = ["LEFT", "RIGHT", "UP", "DOWN"]
StringLegalActions(1) = ["LEFT", "RIGHT", "UP", "DOWN"]

# Apply joint action ["LEFT", "DOWN"]
actions: [0, 3]

# State 2
# p0:RL p1:UD
IsTerminal() = False
History() = [1, 2, 0, 3]
HistoryString() = "1, 2, 0, 3"
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = PlayerId.SIMULTANEOUS
InformationStateString(0) = "us:RL op:UD"
InformationStateString(1) = "us:UD op:RL"
InformationStateTensor(0).observation = [0, 0, 4, 4, 0]
InformationStateTensor(1).observation = [0, 0, 4, 4, 0]
ObservationString(0) = "us:RL op:UD"
ObservationString(1) = "us:UD op:RL"
ObservationTensor(0) = [0, 0, 4, 4, 0]
ObservationTensor(1) = [0, 0, 4, 4, 0]
Rewards() = [-0.3, -0.3]
Returns() = [-0.75, -0.75]
LegalActions(0) = [0, 1, 2, 3]
LegalActions(1) = [0, 1, 2, 3]
StringLegalActions(0) = ["LEFT", "RIGHT", "UP", "DOWN"]
StringLegalActions(1) = ["LEFT", "RIGHT", "UP", "DOWN"]

# Apply joint action ["RIGHT", "DOWN"]
actions: [1, 3]

# State 3
# Apply joint action ["DOWN", "RIGHT"]
actions: [3, 1]

# State 4
# Apply joint action ["UP", "DOWN"]
actions: [2, 3]

# State 5
# Apply joint action ["DOWN", "LEFT"]
actions: [3, 0]

# State 6
# Apply joint action ["LEFT", "DOWN"]
actions: [0, 3]

# State 7
# Apply joint action ["DOWN", "RIGHT"]
actions: [3, 1]

# State 8
# Apply joint action ["DOWN", "DOWN"]
actions: [3, 3]

# State 9
# Apply joint action ["DOWN", "UP"]
actions: [3, 2]

# State 10
# p0:RLRDUDLDDD p1:UDDRDLDRDU
IsTerminal() = False
History() = [1, 2, 0, 3, 1, 3, 3, 1, 2, 3, 3, 0, 0, 3, 3, 1, 3, 3, 3, 2]
HistoryString() = "1, 2, 0, 3, 1, 3, 3, 1, 2, 3, 3, 0, 0, 3, 3, 1, 3, 3, 3, 2"
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = PlayerId.SIMULTANEOUS
InformationStateString(0) = "us:RLRDUDLDDD op:UDDRDLDRDU"
InformationStateString(1) = "us:UDDRDLDRDU op:RLRDUDLDDD"
InformationStateTensor(0).observation = [1, 1, 4, 4, 0]
InformationStateTensor(1).observation = [-1, -1, 3, 3, 0]
ObservationString(0) = "us:RLRDUDLDDD op:UDDRDLDRDU"
ObservationString(1) = "us:UDDRDLDRDU op:RLRDUDLDDD"
ObservationTensor(0) = [1, 1, 4, 4, 0]
ObservationTensor(1) = [-1, -1, 3, 3, 0]
Rewards() = [-0.3, -0.675]
Returns() = [-3.975, -4.125]
LegalActions(0) = [0, 1, 2, 3]
LegalActions(1) = [0, 1, 2, 3]
StringLegalActions(0) = ["LEFT", "RIGHT", "UP", "DOWN"]
StringLegalActions(1) = ["LEFT", "RIGHT", "UP", "DOWN"]

# Apply joint action ["LEFT", "RIGHT"]
actions: [0, 1]

# State 11
# Apply joint action ["RIGHT", "LEFT"]
actions: [1, 0]

# State 12
# Apply joint action ["LEFT", "DOWN"]
actions: [0, 3]

# State 13
# Apply joint action ["RIGHT", "UP"]
actions: [1, 2]

# State 14
# Apply joint action ["DOWN", "RIGHT"]
actions: [3, 1]

# State 15
# Apply joint action ["UP", "UP"]
actions: [2, 2]

# State 16
# Apply joint action ["UP", "UP"]
actions: [2, 2]

# State 17
# Apply joint action ["LEFT", "UP"]
actions: [0, 2]

# State 18
# Apply joint action ["RIGHT", "LEFT"]
actions: [1, 0]

# State 19
# Apply joint action ["UP", "LEFT"]
actions: [2, 0]

# State 20
# p0:RLRDUDLDDDLRLRDUULRU p1:UDDRDLDRDURLDURUUULL
IsTerminal() = False
History() = [1, 2, 0, 3, 1, 3, 3, 1, 2, 3, 3, 0, 0, 3, 3, 1, 3, 3, 3, 2, 0, 1, 1, 0, 0, 3, 1, 2, 3, 1, 2, 2, 2, 2, 0, 2, 1, 0, 2, 0]
HistoryString() = "1, 2, 0, 3, 1, 3, 3, 1, 2, 3, 3, 0, 0, 3, 3, 1, 3, 3, 3, 2, 0, 1, 1, 0, 0, 3, 1, 2, 3, 1, 2, 2, 2, 2, 0, 2, 1, 0, 2, 0"
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = PlayerId.SIMULTANEOUS
InformationStateString(0) = "us:RLRDUDLDDDLRLRDUULRU op:UDDRDLDRDURLDURUUULL"
InformationStateString(1) = "us:UDDRDLDRDURLDURUUULL op:RLRDUDLDDDLRLRDUULRU"
InformationStateTensor(0).observation = [-1, 1, 3, 1, 0]
InformationStateTensor(1).observation = [1, -1, 4, 0, 0]
ObservationString(0) = "us:RLRDUDLDDDLRLRDUULRU op:UDDRDLDRDURLDURUUULL"
ObservationString(1) = "us:UDDRDLDRDURLDURUUULL op:RLRDUDLDDDLRLRDUULRU"
ObservationTensor(0) = [-1, 1, 3, 1, 0]
ObservationTensor(1) = [1, -1, 4, 0, 0]
Rewards() = [-1.51875, -1.51875]
Returns() = [-10.81875, -18.9609375]
LegalActions(0) = [0, 1, 2, 3]
LegalActions(1) = [0, 1, 2, 3]
StringLegalActions(0) = ["LEFT", "RIGHT", "UP", "DOWN"]
StringLegalActions(1) = ["LEFT", "RIGHT", "UP", "DOWN"]

# Apply joint action ["UP", "RIGHT"]
actions: [2, 1]

# State 21
# Apply joint action ["RIGHT", "DOWN"]
actions: [1, 3]

# State 22
# Apply joint action ["DOWN", "UP"]
actions: [3, 2]

# State 23
# Apply joint action ["DOWN", "DOWN"]
actions: [3, 3]

# State 24
# Apply joint action ["DOWN", "RIGHT"]
actions: [3, 1]

# State 25
# Apply joint action ["DOWN", "UP"]
actions: [3, 2]

# State 26
# Apply joint action ["DOWN", "UP"]
actions: [3, 2]

# State 27
# Apply joint action ["UP", "LEFT"]
actions: [2, 0]

# State 28
# Apply joint action ["LEFT", "RIGHT"]
actions: [0, 1]

# State 29
# Apply joint action ["RIGHT", "DOWN"]
actions: [1, 3]

# State 30
# p0:RLRDUDLDDDLRLRDUULRUURDDDDDULR p1:UDDRDLDRDURLDURUUULLRDUDRUULRD
IsTerminal() = False
History() = [1, 2, 0, 3, 1, 3, 3, 1, 2, 3, 3, 0, 0, 3, 3, 1, 3, 3, 3, 2, 0, 1, 1, 0, 0, 3, 1, 2, 3, 1, 2, 2, 2, 2, 0, 2, 1, 0, 2, 0, 2, 1, 1, 3, 3, 2, 3, 3, 3, 1, 3, 2, 3, 2, 2, 0, 0, 1, 1, 3]
HistoryString() = "1, 2, 0, 3, 1, 3, 3, 1, 2, 3, 3, 0, 0, 3, 3, 1, 3, 3, 3, 2, 0, 1, 1, 0, 0, 3, 1, 2, 3, 1, 2, 2, 2, 2, 0, 2, 1, 0, 2, 0, 2, 1, 1, 3, 3, 2, 3, 3, 3, 1, 3, 2, 3, 2, 2, 0, 0, 1, 1, 3"
IsChanceNode() = False
IsSimultaneousNode() = True
CurrentPlayer() = PlayerId.SIMULTANEOUS
InformationStateString(0) = "us:RLRDUDLDDDLRLRDUULRUURDDDDDULR op:UDDRDLDRDURLDURUUULLRDUDRUULRD"
InformationStateString(1) = "us:UDDRDLDRDURLDURUUULLRDUDRUULRD op:RLRDUDLDDDLRLRDUULRUURDDDDDULR"
InformationStateTensor(0).observation = [0, 2, 2, 3, 0]
InformationStateTensor(1).observation = [0, -2, 2, 1, 0]
ObservationString(0) = "us:RLRDUDLDDDLRLRDUULRUURDDDDDULR op:UDDRDLDRDURLDURUUULLRDUDRUULRD"
ObservationString(1) = "us:UDDRDLDRDURLDURUUULLRDUDRUULRD op:RLRDUDLDDDLRLRDUULRUURDDDDDULR"
ObservationTensor(0) = [0, 2, 2, 3, 0]
ObservationTensor(1) = [0, -2, 2, 1, 0]
Rewards() = [-1.0125, -2.278125]
Returns() = [-25.3734375, -43.640625]
LegalActions(0) = [0, 1, 2, 3]
LegalActions(1) = [0, 1, 2, 3]
StringLegalActions(0) = ["LEFT", "RIGHT", "UP", "DOWN"]
StringLegalActions(1) = ["LEFT", "RIGHT", "UP", "DOWN"]

# Apply joint action ["LEFT", "DOWN"]
actions: [0, 3]

# State 31
# Apply joint action ["RIGHT", "RIGHT"]
actions: [1, 1]

# State 32
# Apply joint action ["LEFT", "UP"]
actions: [0, 2]

# State 33
# Apply joint action ["DOWN", "LEFT"]
actions: [3, 0]

# State 34
# Apply joint action ["LEFT", "DOWN"]
actions: [0, 3]

# State 35
# Apply joint action ["LEFT", "UP"]
actions: [0, 2]

# State 36
# Apply joint action ["LEFT", "LEFT"]
actions: [0, 0]

# State 37
# Apply joint action ["RIGHT", "DOWN"]
actions: [1, 3]

# State 38
# Apply joint action ["UP", "DOWN"]
actions: [2, 3]

# State 39
# Apply joint action ["DOWN", "DOWN"]
actions: [3, 3]

# State 40
# p0:RLRDUDLDDDLRLRDUULRUURDDDDDULRLRLDLLLRUD p1:UDDRDLDRDURLDURUUULLRDUDRUULRDDRULDULDDD
IsTerminal() = True
History() = [1, 2, 0, 3, 1, 3, 3, 1, 2, 3, 3, 0, 0, 3, 3, 1, 3, 3, 3, 2, 0, 1, 1, 0, 0, 3, 1, 2, 3, 1, 2, 2, 2, 2, 0, 2, 1, 0, 2, 0, 2, 1, 1, 3, 3, 2, 3, 3, 3, 1, 3, 2, 3, 2, 2, 0, 0, 1, 1, 3, 0, 3, 1, 1, 0, 2, 3, 0, 0, 3, 0, 2, 0, 0, 1, 3, 2, 3, 3, 3]
HistoryString() = "1, 2, 0, 3, 1, 3, 3, 1, 2, 3, 3, 0, 0, 3, 3, 1, 3, 3, 3, 2, 0, 1, 1, 0, 0, 3, 1, 2, 3, 1, 2, 2, 2, 2, 0, 2, 1, 0, 2, 0, 2, 1, 1, 3, 3, 2, 3, 3, 3, 1, 3, 2, 3, 2, 2, 0, 0, 1, 1, 3, 0, 3, 1, 1, 0, 2, 3, 0, 0, 3, 0, 2, 0, 0, 1, 3, 2, 3, 3, 3"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.TERMINAL
InformationStateString(0) = "us:RLRDUDLDDDLRLRDUULRUURDDDDDULRLRLDLLLRUD op:UDDRDLDRDURLDURUUULLRDUDRUULRDDRULDULDDD"
InformationStateString(1) = "us:UDDRDLDRDURLDURUUULLRDUDRUULRDDRULDULDDD op:RLRDUDLDDDLRLRDUULRUURDDDDDULRLRLDLLLRUD"
InformationStateTensor(0).observation = [0, 0, 3, 4, 0]
InformationStateTensor(1).observation = [0, 0, 3, 4, 0]
ObservationString(0) = "us:RLRDUDLDDDLRLRDUULRUURDDDDDULRLRLDLLLRUD op:UDDRDLDRDURLDURUUULLRDUDRUULRDDRULDULDDD"
ObservationString(1) = "us:UDDRDLDRDURLDURUUULLRDUDRUULRDDRULDULDDD op:RLRDUDLDDDLRLRDUULRUURDDDDDULRLRLDLLLRUD"
ObservationTensor(0) = [0, 0, 3, 4, 0]
ObservationTensor(1) = [0, 0, 3, 4, 0]
Rewards() = [-0.45, -0.45]
Returns() = [-30.6609375, -60.5859375]
