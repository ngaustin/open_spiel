Job Id listed below:
57046047

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

/var/spool/slurmd.spool/job57046047/slurm_script: line 36: regret_lambda: command not found
2023-08-01 23:54:29.982658: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-01 23:54:30.868840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0801 23:54:32.297402 22577536793472 rl_environment.py:187] Using game instance: bargaining
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 32
discount_factor: 0.99
double: True
epsilon_decay_duration: 1000
hidden_layers_sizes: [50, 50]
learn_every: 10
learning_rate: 0.01
min_buffer_size_to_learn: 1000
num_actions: 121
optimizer_str: adam
replay_buffer_capacity: 10000
session: <tensorflow.python.client.session.Session object at 0x148874d06d70>
state_representation_size: 93
symmetric: True
update_target_network_every: 1000


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.8
epochs_ppo: 5
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 20000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 1000
regret_calculation_steps: 200000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x148874d06d70>
sims_per_entry: 5000
state_representation_size: 93
steps_fine_tune: 2000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-08-01 23:54:32.614591: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-08-01 23:54:32.906137: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1079 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 5000 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 2.74 seconds to finish estimate with resulting utilities: [1.8724 1.9002]
Exited RRD with total regret 0.0 that was less than regret lambda 0.2 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0     1.89      

 

Player 1 Payoff matrix: 

           0      
    0     1.89      

 

Social Welfare Sum Matrix: 

           0      
    0     3.77      

 

Iteration : 0
Time so far: 0.00017833709716796875
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-08-01 23:54:36.754492: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1941 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.24682229459285737 8.34248161315918 4.773425197601318 0.0006121622587670572 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.21968629757563274 9.760121045793806 4.645713892437163 0.15442553394386777 420050 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.1844137198677877 8.39291907985036 4.462358026969723 0.35684609008522095 820090 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.15221882049666077 7.520344260481537 4.268191106202172 0.6324552971972047 1220132 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.12347534376621982 6.975988274444768 4.110535710534932 1.0069878276606574 1620171 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.10196519621859977 6.682332609667636 3.9685160707719254 1.459816149329622 2020211 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0874926583779868 6.527362576397983 3.829693719375232 1.961477549911887 2420263 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07705417231702846 6.436826174648096 3.7078881037150713 2.460846886658805 2820298 0


Pure best response payoff estimated to be 7.3928 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 6.07 seconds to finish estimate with resulting utilities: [7.3236 2.8732]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 9.64 seconds to finish estimate with resulting utilities: [4.7464 5.0042]
Computing meta_strategies
Exited RRD with total regret 0.19957251128223774 that was less than regret lambda 0.2 after 1135 iterations 
NEW LAMBDA 0.19310344827586207
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0     1.89           2.87      
    1     7.32           4.88      

 

Player 1 Payoff matrix: 

           0              1      
    0     1.89           7.32      
    1     2.87           4.88      

 

Social Welfare Sum Matrix: 

           0              1      
    0     3.77          10.20      
    1    10.20           9.75      

 

Metagame probabilities: 
Player #0: 0.0462  0.9538  
Player #1: 0.0462  0.9538  
Iteration : 1
Time so far: 5509.437175273895
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-08-02 01:26:26.577998: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3851 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07691131598872339 6.599938337339295 3.692331110768848 2.5299483404460563 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08897803924103245 7.113573684634232 3.6563204983385598 2.8567241458691925 420042 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09365705962282726 6.930502074438593 3.6410278807515684 3.162458957737141 820087 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09277765998826819 6.786275453661002 3.6247217374689438 3.440582340785313 1220131 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08766145844087338 6.663859595571245 3.5948086468236786 3.693412027991436 1620178 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08155522340427718 6.55464343399298 3.53378469279555 3.946124288639261 2020228 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07593302146734927 6.462264642932198 3.424706888740713 4.216527672567314 2420268 0


Pure best response payoff estimated to be 6.2966 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 5.77 seconds to finish estimate with resulting utilities: [7.3978 3.2266]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 9.27 seconds to finish estimate with resulting utilities: [6.109  4.4648]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 10.83 seconds to finish estimate with resulting utilities: [4.7498 4.8872]
Computing meta_strategies
Exited RRD with total regret 0.19309511485734276 that was less than regret lambda 0.19310344827586207 after 2537 iterations 
NEW LAMBDA 0.18620689655172412
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0     1.89           2.87           3.23      
    1     7.32           4.88           4.46      
    2     7.40           6.11           4.82      

 

Player 1 Payoff matrix: 

           0              1              2      
    0     1.89           7.32           7.40      
    1     2.87           4.88           6.11      
    2     3.23           4.46           4.82      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0     3.77          10.20          10.62      
    1    10.20           9.75          10.57      
    2    10.62          10.57           9.64      

 

Metagame probabilities: 
Player #0: 0.0023  0.1803  0.8174  
Player #1: 0.0023  0.1803  0.8174  
Iteration : 2
Time so far: 10813.279792308807
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 02:54:50.433307: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5841 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07455762411432076 6.493855383475328 3.380203605226045 4.309553864568961 20005 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08041555151326144 6.562820697404265 3.3758002606454176 4.438284143102276 420045 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0826290511005293 6.374326291498264 3.375711265714222 4.567648412295232 820082 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0818439915464112 6.210577292024549 3.37135300139286 4.712551930270656 1220126 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07866617018196724 6.063827393061755 3.365230254705815 4.865203658776529 1620166 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07487576135801029 5.928600761125672 3.3559505492850454 5.02073776005682 2020216 0


Pure best response payoff estimated to be 6.26 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 5.63 seconds to finish estimate with resulting utilities: [7.2246 3.5764]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 8.34 seconds to finish estimate with resulting utilities: [6.2508 5.217 ]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 7.1 seconds to finish estimate with resulting utilities: [6.0454 6.6146]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 7.29 seconds to finish estimate with resulting utilities: [5.9988 5.6082]
Computing meta_strategies
Exited RRD with total regret 0.18613260024182576 that was less than regret lambda 0.18620689655172412 after 2179 iterations 
NEW LAMBDA 0.1793103448275862
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0     1.89           2.87           3.23           3.58      
    1     7.32           4.88           4.46           5.22      
    2     7.40           6.11           4.82           6.61      
    3     7.22           6.25           6.05           5.80      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0     1.89           7.32           7.40           7.22      
    1     2.87           4.88           6.11           6.25      
    2     3.23           4.46           4.82           6.05      
    3     3.58           5.22           6.61           5.80      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0     3.77          10.20          10.62          10.80      
    1    10.20           9.75          10.57          11.47      
    2    10.62          10.57           9.64          12.66      
    3    10.80          11.47          12.66          11.61      

 

Metagame probabilities: 
Player #0: 0.0013  0.0549  0.4112  0.5327  
Player #1: 0.0013  0.0549  0.4112  0.5327  
Iteration : 3
Time so far: 16633.21294426918
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-08-02 04:31:50.417386: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7831 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07187176082871885 5.880704558657868 3.340090927443541 5.179128640481078 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0757389303741647 5.9624007063770526 3.3303872167048953 5.320719411920852 420040 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07758754578073565 5.8485605117063235 3.3216385870687923 5.460069529337662 820075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07753324520898344 5.751974772032507 3.312396763539367 5.591616980281046 1220104 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07577831743690659 5.659661007332447 3.30341188067337 5.713821484811838 1620142 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07337807635849014 5.57458552925747 3.294793571959688 5.826355540026075 2020177 0
Recovering previous policy with expected return of 6.084915084915085. Long term value was 5.9636 and short term was 5.891.


Pure best response payoff estimated to be 6.544 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 5.75 seconds to finish estimate with resulting utilities: [7.2284 3.5054]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 8.49 seconds to finish estimate with resulting utilities: [6.2174 5.3144]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 7.33 seconds to finish estimate with resulting utilities: [5.9882 6.5202]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 7.35 seconds to finish estimate with resulting utilities: [6.0076 5.6634]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 7.39 seconds to finish estimate with resulting utilities: [5.9956 5.5878]
Computing meta_strategies
Exited RRD with total regret 0.17925664492380644 that was less than regret lambda 0.1793103448275862 after 1822 iterations 
NEW LAMBDA 0.1724137931034483
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0     1.89           2.87           3.23           3.58           3.51      
    1     7.32           4.88           4.46           5.22           5.31      
    2     7.40           6.11           4.82           6.61           6.52      
    3     7.22           6.25           6.05           5.80           5.66      
    4     7.23           6.22           5.99           6.01           5.79      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0     1.89           7.32           7.40           7.22           7.23      
    1     2.87           4.88           6.11           6.25           6.22      
    2     3.23           4.46           4.82           6.05           5.99      
    3     3.58           5.22           6.61           5.80           6.01      
    4     3.51           5.31           6.52           5.66           5.79      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0     3.77          10.20          10.62          10.80          10.73      
    1    10.20           9.75          10.57          11.47          11.53      
    2    10.62          10.57           9.64          12.66          12.51      
    3    10.80          11.47          12.66          11.61          11.67      
    4    10.73          11.53          12.51          11.67          11.58      

 

Metagame probabilities: 
Player #0: 0.0022  0.0575  0.3346  0.2838  0.3218  
Player #1: 0.0022  0.0575  0.3346  0.2838  0.3218  
Iteration : 4
Time so far: 22676.499614953995
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-08-02 06:12:33.763007: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9821 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07155559787422866 5.544227951367696 3.2812714774468366 5.9292900221741345 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07454705745971296 5.62641736228511 3.276250217590692 6.016955376010652 420040 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07593039630358743 5.552984076673335 3.271296233914115 6.106774038468353 820069 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07576364712108523 5.490162763344614 3.2641688072890567 6.198435684653408 1220112 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07428694145411639 5.439606410931733 3.2582066978438426 6.285589915758141 1620156 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07230789274440079 5.38742181434006 3.2519380533890647 6.371389229866005 2020200 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07047535185662417 5.330261738943675 3.2436524195519705 6.45430888340745 2420236 0


Pure best response payoff estimated to be 6.7406 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 5.92 seconds to finish estimate with resulting utilities: [7.2362 3.5842]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 8.11 seconds to finish estimate with resulting utilities: [5.8466 5.9426]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 7.29 seconds to finish estimate with resulting utilities: [6.0068 7.2882]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 6.55 seconds to finish estimate with resulting utilities: [6.648  6.7746]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 6.66 seconds to finish estimate with resulting utilities: [6.4798 6.7616]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 6.19 seconds to finish estimate with resulting utilities: [6.5688 7.3048]
Computing meta_strategies
Exited RRD with total regret 0.1723969625459585 that was less than regret lambda 0.1724137931034483 after 5196 iterations 
NEW LAMBDA 0.16551724137931034
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0     1.89           2.87           3.23           3.58           3.51           3.58      
    1     7.32           4.88           4.46           5.22           5.31           5.94      
    2     7.40           6.11           4.82           6.61           6.52           7.29      
    3     7.22           6.25           6.05           5.80           5.66           6.77      
    4     7.23           6.22           5.99           6.01           5.79           6.76      
    5     7.24           5.85           6.01           6.65           6.48           6.94      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0     1.89           7.32           7.40           7.22           7.23           7.24      
    1     2.87           4.88           6.11           6.25           6.22           5.85      
    2     3.23           4.46           4.82           6.05           5.99           6.01      
    3     3.58           5.22           6.61           5.80           6.01           6.65      
    4     3.51           5.31           6.52           5.66           5.79           6.48      
    5     3.58           5.94           7.29           6.77           6.76           6.94      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0     3.77          10.20          10.62          10.80          10.73          10.82      
    1    10.20           9.75          10.57          11.47          11.53          11.79      
    2    10.62          10.57           9.64          12.66          12.51          13.29      
    3    10.80          11.47          12.66          11.61          11.67          13.42      
    4    10.73          11.53          12.51          11.67          11.58          13.24      
    5    10.82          11.79          13.29          13.42          13.24          13.87      

 

Metagame probabilities: 
Player #0: 0.0001  0.0009  0.2471  0.1037  0.1243  0.524  
Player #1: 0.0001  0.0009  0.2471  0.1037  0.1243  0.524  
Iteration : 5
Time so far: 28723.89579629898
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 07:53:21.443178: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11811 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07061270600434418 5.374651226491067 3.243261700802484 6.458050850247876 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07267730079378293 5.434645299354822 3.2350225866481823 6.520947637116371 420035 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07329419077934951 5.38132779829193 3.2238356776813104 6.577488993160835 820067 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07266675245597878 5.352270929375192 3.212031693223934 6.631553339769865 1220103 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07106147606017327 5.31687131920612 3.19569806237764 6.690309885870784 1620135 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06916021587640536 5.267189657704448 3.176834749197014 6.763993640785563 2020175 0
Recovering previous policy with expected return of 6.504495504495504. Long term value was 5.1418 and short term was 5.118.


Pure best response payoff estimated to be 6.6178 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 6.05 seconds to finish estimate with resulting utilities: [7.22  3.672]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 8.1 seconds to finish estimate with resulting utilities: [5.8792 5.9238]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 7.41 seconds to finish estimate with resulting utilities: [6.0142 7.3038]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 6.8 seconds to finish estimate with resulting utilities: [6.6012 6.7634]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 6.76 seconds to finish estimate with resulting utilities: [6.5626 6.7466]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 6.13 seconds to finish estimate with resulting utilities: [6.5952 7.3438]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 6.24 seconds to finish estimate with resulting utilities: [6.5822 7.3406]
Computing meta_strategies
Exited RRD with total regret 0.1654880640348786 that was less than regret lambda 0.16551724137931034 after 6764 iterations 
NEW LAMBDA 0.1586206896551724
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0     1.89           2.87           3.23           3.58           3.51           3.58           3.67      
    1     7.32           4.88           4.46           5.22           5.31           5.94           5.92      
    2     7.40           6.11           4.82           6.61           6.52           7.29           7.30      
    3     7.22           6.25           6.05           5.80           5.66           6.77           6.76      
    4     7.23           6.22           5.99           6.01           5.79           6.76           6.75      
    5     7.24           5.85           6.01           6.65           6.48           6.94           7.34      
    6     7.22           5.88           6.01           6.60           6.56           6.60           6.96      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0     1.89           7.32           7.40           7.22           7.23           7.24           7.22      
    1     2.87           4.88           6.11           6.25           6.22           5.85           5.88      
    2     3.23           4.46           4.82           6.05           5.99           6.01           6.01      
    3     3.58           5.22           6.61           5.80           6.01           6.65           6.60      
    4     3.51           5.31           6.52           5.66           5.79           6.48           6.56      
    5     3.58           5.94           7.29           6.77           6.76           6.94           6.60      
    6     3.67           5.92           7.30           6.76           6.75           7.34           6.96      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0     3.77          10.20          10.62          10.80          10.73          10.82          10.89      
    1    10.20           9.75          10.57          11.47          11.53          11.79          11.80      
    2    10.62          10.57           9.64          12.66          12.51          13.29          13.32      
    3    10.80          11.47          12.66          11.61          11.67          13.42          13.36      
    4    10.73          11.53          12.51          11.67          11.58          13.24          13.31      
    5    10.82          11.79          13.29          13.42          13.24          13.87          13.94      
    6    10.89          11.80          13.32          13.36          13.31          13.94          13.92      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.2191  0.0547  0.0609  0.5168  0.1483  
Player #1: 0.0001  0.0001  0.2191  0.0547  0.0609  0.5168  0.1483  
Iteration : 6
Time so far: 35105.47458052635
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-08-02 09:39:43.185554: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13801 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06785433983771419 5.269041955981025 3.1604564171742626 6.833755386562329 20005 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06963928644266465 5.342726002074778 3.155957460186134 6.874979148666741 420039 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07036258927487347 5.31839099166357 3.151246255031092 6.918675295578701 820076 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07004222606163528 5.312856014530257 3.146639791544121 6.966339726009635 1220116 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06880360331781514 5.305897709655301 3.1395361604034036 7.016928169679579 1620151 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06725476754204293 5.299625683224426 3.1315190859279545 7.070084268853963 2020194 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06577037416510992 5.292326375361411 3.121123208840322 7.128270188112462 2420233 0


Pure best response payoff estimated to be 6.8958 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 6.21 seconds to finish estimate with resulting utilities: [7.2802 3.4084]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 9.99 seconds to finish estimate with resulting utilities: [6.0298 4.8504]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 10.46 seconds to finish estimate with resulting utilities: [5.421  5.9698]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 9.54 seconds to finish estimate with resulting utilities: [5.6658 5.5468]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 9.67 seconds to finish estimate with resulting utilities: [5.631 5.453]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 7.07 seconds to finish estimate with resulting utilities: [7.4056 5.5878]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 7.1 seconds to finish estimate with resulting utilities: [7.3432 5.6126]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 7.35 seconds to finish estimate with resulting utilities: [6.8626 6.1848]
Computing meta_strategies
Exited RRD with total regret 0.15857321681846237 that was less than regret lambda 0.1586206896551724 after 11898 iterations 
NEW LAMBDA 0.15172413793103448
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0     1.89           2.87           3.23           3.58           3.51           3.58           3.67           3.41      
    1     7.32           4.88           4.46           5.22           5.31           5.94           5.92           4.85      
    2     7.40           6.11           4.82           6.61           6.52           7.29           7.30           5.97      
    3     7.22           6.25           6.05           5.80           5.66           6.77           6.76           5.55      
    4     7.23           6.22           5.99           6.01           5.79           6.76           6.75           5.45      
    5     7.24           5.85           6.01           6.65           6.48           6.94           7.34           5.59      
    6     7.22           5.88           6.01           6.60           6.56           6.60           6.96           5.61      
    7     7.28           6.03           5.42           5.67           5.63           7.41           7.34           6.52      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0     1.89           7.32           7.40           7.22           7.23           7.24           7.22           7.28      
    1     2.87           4.88           6.11           6.25           6.22           5.85           5.88           6.03      
    2     3.23           4.46           4.82           6.05           5.99           6.01           6.01           5.42      
    3     3.58           5.22           6.61           5.80           6.01           6.65           6.60           5.67      
    4     3.51           5.31           6.52           5.66           5.79           6.48           6.56           5.63      
    5     3.58           5.94           7.29           6.77           6.76           6.94           6.60           7.41      
    6     3.67           5.92           7.30           6.76           6.75           7.34           6.96           7.34      
    7     3.41           4.85           5.97           5.55           5.45           5.59           5.61           6.52      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0     3.77          10.20          10.62          10.80          10.73          10.82          10.89          10.69      
    1    10.20           9.75          10.57          11.47          11.53          11.79          11.80          10.88      
    2    10.62          10.57           9.64          12.66          12.51          13.29          13.32          11.39      
    3    10.80          11.47          12.66          11.61          11.67          13.42          13.36          11.21      
    4    10.73          11.53          12.51          11.67          11.58          13.24          13.31          11.08      
    5    10.82          11.79          13.29          13.42          13.24          13.87          13.94          12.99      
    6    10.89          11.80          13.32          13.36          13.31          13.94          13.92          12.96      
    7    10.69          10.88          11.39          11.21          11.08          12.99          12.96          13.05      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0572  0.0036  0.0025  0.0412  0.0127  0.8825  
Player #1: 0.0001  0.0001  0.0572  0.0036  0.0025  0.0412  0.0127  0.8825  
Iteration : 7
Time so far: 41352.29465150833
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-08-02 11:23:49.954942: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15791 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06556671807598292 5.322979262495095 3.1181706986476465 7.143032433802096 20004 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06684458949713012 5.391178986526951 3.1067218986363856 7.200897418412538 420049 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06728006415222472 5.34910298417458 3.095664770135744 7.254312269150801 820084 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06699276494984602 5.3062113129185455 3.0850825383093325 7.306612378851823 1220131 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06604060987284573 5.263300207354215 3.0746331404288694 7.3587966388455675 1620172 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0648213341469309 5.217150063651929 3.060026462154055 7.414642516670879 2020202 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06357596893209241 5.167514884820037 3.0362571376087923 7.479953840000795 2420237 0


Pure best response payoff estimated to be 7.3862 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (8, 0)
Current player 0 and current strategies (8, 0) took 6.56 seconds to finish estimate with resulting utilities: [7.298  3.1352]
Estimating current strategies:  (8, 1)
Current player 0 and current strategies (8, 1) took 10.69 seconds to finish estimate with resulting utilities: [5.8742 4.4446]
Estimating current strategies:  (8, 2)
Current player 0 and current strategies (8, 2) took 11.44 seconds to finish estimate with resulting utilities: [4.9476 5.593 ]
Estimating current strategies:  (8, 3)
Current player 0 and current strategies (8, 3) took 10.58 seconds to finish estimate with resulting utilities: [5.1578 4.8816]
Estimating current strategies:  (8, 4)
Current player 0 and current strategies (8, 4) took 10.44 seconds to finish estimate with resulting utilities: [5.196  4.8156]
Estimating current strategies:  (8, 5)
Current player 0 and current strategies (8, 5) took 5.8 seconds to finish estimate with resulting utilities: [7.8756 5.8302]
Estimating current strategies:  (8, 6)
Current player 0 and current strategies (8, 6) took 5.67 seconds to finish estimate with resulting utilities: [7.904  5.8974]
Estimating current strategies:  (8, 7)
Current player 0 and current strategies (8, 7) took 6.33 seconds to finish estimate with resulting utilities: [7.4066 6.481 ]
Estimating current strategies:  (8, 8)
Current player 0 and current strategies (8, 8) took 7.46 seconds to finish estimate with resulting utilities: [6.7482 6.0316]
Computing meta_strategies
Exited RRD with total regret 0.15164821696025577 that was less than regret lambda 0.15172413793103448 after 6769 iterations 
NEW LAMBDA 0.14482758620689656
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0     1.89           2.87           3.23           3.58           3.51           3.58           3.67           3.41           3.14      
    1     7.32           4.88           4.46           5.22           5.31           5.94           5.92           4.85           4.44      
    2     7.40           6.11           4.82           6.61           6.52           7.29           7.30           5.97           5.59      
    3     7.22           6.25           6.05           5.80           5.66           6.77           6.76           5.55           4.88      
    4     7.23           6.22           5.99           6.01           5.79           6.76           6.75           5.45           4.82      
    5     7.24           5.85           6.01           6.65           6.48           6.94           7.34           5.59           5.83      
    6     7.22           5.88           6.01           6.60           6.56           6.60           6.96           5.61           5.90      
    7     7.28           6.03           5.42           5.67           5.63           7.41           7.34           6.52           6.48      
    8     7.30           5.87           4.95           5.16           5.20           7.88           7.90           7.41           6.39      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0     1.89           7.32           7.40           7.22           7.23           7.24           7.22           7.28           7.30      
    1     2.87           4.88           6.11           6.25           6.22           5.85           5.88           6.03           5.87      
    2     3.23           4.46           4.82           6.05           5.99           6.01           6.01           5.42           4.95      
    3     3.58           5.22           6.61           5.80           6.01           6.65           6.60           5.67           5.16      
    4     3.51           5.31           6.52           5.66           5.79           6.48           6.56           5.63           5.20      
    5     3.58           5.94           7.29           6.77           6.76           6.94           6.60           7.41           7.88      
    6     3.67           5.92           7.30           6.76           6.75           7.34           6.96           7.34           7.90      
    7     3.41           4.85           5.97           5.55           5.45           5.59           5.61           6.52           7.41      
    8     3.14           4.44           5.59           4.88           4.82           5.83           5.90           6.48           6.39      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8      
    0     3.77          10.20          10.62          10.80          10.73          10.82          10.89          10.69          10.43      
    1    10.20           9.75          10.57          11.47          11.53          11.79          11.80          10.88          10.32      
    2    10.62          10.57           9.64          12.66          12.51          13.29          13.32          11.39          10.54      
    3    10.80          11.47          12.66          11.61          11.67          13.42          13.36          11.21          10.04      
    4    10.73          11.53          12.51          11.67          11.58          13.24          13.31          11.08          10.01      
    5    10.82          11.79          13.29          13.42          13.24          13.87          13.94          12.99          13.71      
    6    10.89          11.80          13.32          13.36          13.31          13.94          13.92          12.96          13.80      
    7    10.69          10.88          11.39          11.21          11.08          12.99          12.96          13.05          13.89      
    8    10.43          10.32          10.54          10.04          10.01          13.71          13.80          13.89          12.78      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0111  0.0005  0.0004  0.0189  0.0146  0.2345  0.7197  
Player #1: 0.0001  0.0001  0.0111  0.0005  0.0004  0.0189  0.0146  0.2345  0.7197  
Iteration : 8
Time so far: 48140.370869874954
Approximating Best Response
Training best response:  True 0.06
rtg values check:  (?,) (?, 1)
2023-08-02 13:16:58.155584: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_62/weights_2/Adam_1/Assign' id:17781 op device:{requested: '', assigned: ''} def:{{{node mlp_62/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_62/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_62/weights_2/Adam_1, mlp_62/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0631235450556227 5.182431527050193 3.024059893580492 7.510191688222129 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06375862100376786 5.219601754456351 3.006054741505774 7.567893868452458 420044 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06388591301827777 5.142458692461881 2.9901183881137285 7.624259537047521 820076 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06348397068400571 5.069449582746474 2.973686832133435 7.683469038484277 1220112 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06256335769415133 5.0018726455636475 2.95370611560543 7.751818474814143 1620145 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06148143749240513 4.940952670898715 2.9328786440639876 7.826177457532991 2020180 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0604313355966572 4.884761886248189 2.9123180203471803 7.90176534328745 2420214 0


Pure best response payoff estimated to be 7.1562 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (9, 0)
Current player 0 and current strategies (9, 0) took 6.78 seconds to finish estimate with resulting utilities: [7.3798 3.0546]
Estimating current strategies:  (9, 1)
Current player 0 and current strategies (9, 1) took 10.88 seconds to finish estimate with resulting utilities: [6.0322 4.4022]
Estimating current strategies:  (9, 2)
Current player 0 and current strategies (9, 2) took 11.97 seconds to finish estimate with resulting utilities: [4.6908 4.8386]
Estimating current strategies:  (9, 3)
Current player 0 and current strategies (9, 3) took 11.26 seconds to finish estimate with resulting utilities: [5.1064 4.7046]
Estimating current strategies:  (9, 4)
Current player 0 and current strategies (9, 4) took 11.18 seconds to finish estimate with resulting utilities: [5.18   4.7508]
Estimating current strategies:  (9, 5)
Current player 0 and current strategies (9, 5) took 7.42 seconds to finish estimate with resulting utilities: [7.3726 5.4928]
Estimating current strategies:  (9, 6)
Current player 0 and current strategies (9, 6) took 7.45 seconds to finish estimate with resulting utilities: [7.3076 5.5024]
Estimating current strategies:  (9, 7)
Current player 0 and current strategies (9, 7) took 8.51 seconds to finish estimate with resulting utilities: [6.5612 5.7244]
Estimating current strategies:  (9, 8)
Current player 0 and current strategies (9, 8) took 7.11 seconds to finish estimate with resulting utilities: [7.2654 5.8716]
Estimating current strategies:  (9, 9)
Current player 0 and current strategies (9, 9) took 12.33 seconds to finish estimate with resulting utilities: [4.0962 4.4164]
Computing meta_strategies
Exited RRD with total regret 0.14480619911488546 that was less than regret lambda 0.14482758620689656 after 8779 iterations 
NEW LAMBDA 0.13793103448275862
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     1.89           2.87           3.23           3.58           3.51           3.58           3.67           3.41           3.14           3.05      
    1     7.32           4.88           4.46           5.22           5.31           5.94           5.92           4.85           4.44           4.40      
    2     7.40           6.11           4.82           6.61           6.52           7.29           7.30           5.97           5.59           4.84      
    3     7.22           6.25           6.05           5.80           5.66           6.77           6.76           5.55           4.88           4.70      
    4     7.23           6.22           5.99           6.01           5.79           6.76           6.75           5.45           4.82           4.75      
    5     7.24           5.85           6.01           6.65           6.48           6.94           7.34           5.59           5.83           5.49      
    6     7.22           5.88           6.01           6.60           6.56           6.60           6.96           5.61           5.90           5.50      
    7     7.28           6.03           5.42           5.67           5.63           7.41           7.34           6.52           6.48           5.72      
    8     7.30           5.87           4.95           5.16           5.20           7.88           7.90           7.41           6.39           5.87      
    9     7.38           6.03           4.69           5.11           5.18           7.37           7.31           6.56           7.27           4.26      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     1.89           7.32           7.40           7.22           7.23           7.24           7.22           7.28           7.30           7.38      
    1     2.87           4.88           6.11           6.25           6.22           5.85           5.88           6.03           5.87           6.03      
    2     3.23           4.46           4.82           6.05           5.99           6.01           6.01           5.42           4.95           4.69      
    3     3.58           5.22           6.61           5.80           6.01           6.65           6.60           5.67           5.16           5.11      
    4     3.51           5.31           6.52           5.66           5.79           6.48           6.56           5.63           5.20           5.18      
    5     3.58           5.94           7.29           6.77           6.76           6.94           6.60           7.41           7.88           7.37      
    6     3.67           5.92           7.30           6.76           6.75           7.34           6.96           7.34           7.90           7.31      
    7     3.41           4.85           5.97           5.55           5.45           5.59           5.61           6.52           7.41           6.56      
    8     3.14           4.44           5.59           4.88           4.82           5.83           5.90           6.48           6.39           7.27      
    9     3.05           4.40           4.84           4.70           4.75           5.49           5.50           5.72           5.87           4.26      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     3.77          10.20          10.62          10.80          10.73          10.82          10.89          10.69          10.43          10.43      
    1    10.20           9.75          10.57          11.47          11.53          11.79          11.80          10.88          10.32          10.43      
    2    10.62          10.57           9.64          12.66          12.51          13.29          13.32          11.39          10.54           9.53      
    3    10.80          11.47          12.66          11.61          11.67          13.42          13.36          11.21          10.04           9.81      
    4    10.73          11.53          12.51          11.67          11.58          13.24          13.31          11.08          10.01           9.93      
    5    10.82          11.79          13.29          13.42          13.24          13.87          13.94          12.99          13.71          12.87      
    6    10.89          11.80          13.32          13.36          13.31          13.94          13.92          12.96          13.80          12.81      
    7    10.69          10.88          11.39          11.21          11.08          12.99          12.96          13.05          13.89          12.29      
    8    10.43          10.32          10.54          10.04          10.01          13.71          13.80          13.89          12.78          13.14      
    9    10.43          10.43           9.53           9.81           9.93          12.87          12.81          12.29          13.14           8.51      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0007  0.0001  0.0001  0.0034  0.003  0.1513  0.6238  0.2174  
Player #1: 0.0001  0.0001  0.0007  0.0001  0.0001  0.0034  0.003  0.1513  0.6238  0.2174  
Iteration : 9
Time so far: 54991.89343857765
Approximating Best Response
Training best response:  True 0.05500000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 15:11:09.792761: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_69/weights_2/Adam_1/Assign' id:19771 op device:{requested: '', assigned: ''} def:{{{node mlp_69/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_69/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_69/weights_2/Adam_1, mlp_69/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05969685688696939 4.868467326815812 2.8945401836572926 7.963987555770794 20004 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0603492897262883 4.931367126184424 2.8814919561096217 8.022816407650488 420045 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06051968318538541 4.904591639327275 2.8693755951316327 8.080666107304468 820083 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06020950894656901 4.880481154164408 2.85587202236927 8.141732530791728 1220123 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05947915128300937 4.8556392731435585 2.8422985173798545 8.199969225750804 1620161 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.058605536329277876 4.832582675572936 2.828996205599512 8.256941276751 2020199 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.057735242704387964 4.809965777804628 2.81429518950194 8.315817710431123 2420232 0


Pure best response payoff estimated to be 6.876 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (10, 0)
Current player 0 and current strategies (10, 0) took 7.26 seconds to finish estimate with resulting utilities: [7.2886 2.7494]
Estimating current strategies:  (10, 1)
Current player 0 and current strategies (10, 1) took 10.97 seconds to finish estimate with resulting utilities: [5.7406 4.8494]
Estimating current strategies:  (10, 2)
Current player 0 and current strategies (10, 2) took 12.04 seconds to finish estimate with resulting utilities: [4.448 5.452]
Estimating current strategies:  (10, 3)
Current player 0 and current strategies (10, 3) took 11.19 seconds to finish estimate with resulting utilities: [5.1176 5.0424]
Estimating current strategies:  (10, 4)
Current player 0 and current strategies (10, 4) took 11.1 seconds to finish estimate with resulting utilities: [5.1552 5.0186]
Estimating current strategies:  (10, 5)
Current player 0 and current strategies (10, 5) took 6.85 seconds to finish estimate with resulting utilities: [7.5308 5.849 ]
Estimating current strategies:  (10, 6)
Current player 0 and current strategies (10, 6) took 6.91 seconds to finish estimate with resulting utilities: [7.4938 5.856 ]
Estimating current strategies:  (10, 7)
Current player 0 and current strategies (10, 7) took 7.29 seconds to finish estimate with resulting utilities: [7.0838 6.7538]
Estimating current strategies:  (10, 8)
Current player 0 and current strategies (10, 8) took 6.85 seconds to finish estimate with resulting utilities: [7.4014 6.0696]
Estimating current strategies:  (10, 9)
Current player 0 and current strategies (10, 9) took 10.72 seconds to finish estimate with resulting utilities: [4.5308 5.6046]
Estimating current strategies:  (10, 10)
Current player 0 and current strategies (10, 10) took 11.47 seconds to finish estimate with resulting utilities: [5.1252 5.4954]
Computing meta_strategies
Exited RRD with total regret 0.1379064448898477 that was less than regret lambda 0.13793103448275862 after 8219 iterations 
NEW LAMBDA 0.13103448275862067
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     1.89           2.87           3.23           3.58           3.51           3.58           3.67           3.41           3.14           3.05           2.75      
    1     7.32           4.88           4.46           5.22           5.31           5.94           5.92           4.85           4.44           4.40           4.85      
    2     7.40           6.11           4.82           6.61           6.52           7.29           7.30           5.97           5.59           4.84           5.45      
    3     7.22           6.25           6.05           5.80           5.66           6.77           6.76           5.55           4.88           4.70           5.04      
    4     7.23           6.22           5.99           6.01           5.79           6.76           6.75           5.45           4.82           4.75           5.02      
    5     7.24           5.85           6.01           6.65           6.48           6.94           7.34           5.59           5.83           5.49           5.85      
    6     7.22           5.88           6.01           6.60           6.56           6.60           6.96           5.61           5.90           5.50           5.86      
    7     7.28           6.03           5.42           5.67           5.63           7.41           7.34           6.52           6.48           5.72           6.75      
    8     7.30           5.87           4.95           5.16           5.20           7.88           7.90           7.41           6.39           5.87           6.07      
    9     7.38           6.03           4.69           5.11           5.18           7.37           7.31           6.56           7.27           4.26           5.60      
   10     7.29           5.74           4.45           5.12           5.16           7.53           7.49           7.08           7.40           4.53           5.31      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     1.89           7.32           7.40           7.22           7.23           7.24           7.22           7.28           7.30           7.38           7.29      
    1     2.87           4.88           6.11           6.25           6.22           5.85           5.88           6.03           5.87           6.03           5.74      
    2     3.23           4.46           4.82           6.05           5.99           6.01           6.01           5.42           4.95           4.69           4.45      
    3     3.58           5.22           6.61           5.80           6.01           6.65           6.60           5.67           5.16           5.11           5.12      
    4     3.51           5.31           6.52           5.66           5.79           6.48           6.56           5.63           5.20           5.18           5.16      
    5     3.58           5.94           7.29           6.77           6.76           6.94           6.60           7.41           7.88           7.37           7.53      
    6     3.67           5.92           7.30           6.76           6.75           7.34           6.96           7.34           7.90           7.31           7.49      
    7     3.41           4.85           5.97           5.55           5.45           5.59           5.61           6.52           7.41           6.56           7.08      
    8     3.14           4.44           5.59           4.88           4.82           5.83           5.90           6.48           6.39           7.27           7.40      
    9     3.05           4.40           4.84           4.70           4.75           5.49           5.50           5.72           5.87           4.26           4.53      
   10     2.75           4.85           5.45           5.04           5.02           5.85           5.86           6.75           6.07           5.60           5.31      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     3.77          10.20          10.62          10.80          10.73          10.82          10.89          10.69          10.43          10.43          10.04      
    1    10.20           9.75          10.57          11.47          11.53          11.79          11.80          10.88          10.32          10.43          10.59      
    2    10.62          10.57           9.64          12.66          12.51          13.29          13.32          11.39          10.54           9.53           9.90      
    3    10.80          11.47          12.66          11.61          11.67          13.42          13.36          11.21          10.04           9.81          10.16      
    4    10.73          11.53          12.51          11.67          11.58          13.24          13.31          11.08          10.01           9.93          10.17      
    5    10.82          11.79          13.29          13.42          13.24          13.87          13.94          12.99          13.71          12.87          13.38      
    6    10.89          11.80          13.32          13.36          13.31          13.94          13.92          12.96          13.80          12.81          13.35      
    7    10.69          10.88          11.39          11.21          11.08          12.99          12.96          13.05          13.89          12.29          13.84      
    8    10.43          10.32          10.54          10.04          10.01          13.71          13.80          13.89          12.78          13.14          13.47      
    9    10.43          10.43           9.53           9.81           9.93          12.87          12.81          12.29          13.14           8.51          10.14      
   10    10.04          10.59           9.90          10.16          10.17          13.38          13.35          13.84          13.47          10.14          10.62      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0008  0.0001  0.0001  0.0029  0.0026  0.2349  0.4363  0.0793  0.2427  
Player #1: 0.0001  0.0001  0.0008  0.0001  0.0001  0.0029  0.0026  0.2349  0.4363  0.0793  0.2427  
Iteration : 10
Time so far: 61448.55019235611
Approximating Best Response
Training best response:  True 0.05
rtg values check:  (?,) (?, 1)
2023-08-02 16:58:46.938705: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_76/weights_2/Adam_1/Assign' id:21761 op device:{requested: '', assigned: ''} def:{{{node mlp_76/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_76/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_76/weights_2/Adam_1, mlp_76/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05733290683691511 4.824285479161192 2.8050853905290762 8.349582386985208 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05786657243619357 4.898012817238911 2.792168432950419 8.4108435464246 420042 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.058087169487245356 4.875660313013586 2.780471897325596 8.47024871455233 820080 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05785394238025498 4.845323594330486 2.767263827833401 8.530821371249669 1220123 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.057252908010638905 4.817250734324628 2.7532239683121555 8.591961528382571 1620160 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05652790399529343 4.794707408492978 2.7378904529519983 8.654153557317319 2020203 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05582286831828802 4.7747990646750225 2.717942449426891 8.719590583364973 2420245 0


slurmstepd: error: *** JOB 57046047 ON gl3049 CANCELLED AT 2023-08-02T18:18:41 ***
