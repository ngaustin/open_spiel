Job Id listed below:
56105357

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

2023-07-19 16:06:29.523524: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-19 16:06:30.113429: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-19 16:06:32.441915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0719 16:06:37.594051 22726780930944 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x14ab3474ad10>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x14ab3474ad10>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-07-19 16:06:37.916311: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-07-19 16:06:38.236616: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 19.11 seconds to finish estimate with resulting utilities: [50.055 49.165]
Exited RRD with total regret 0.0 that was less than regret lambda 2.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    49.61      

 

Player 1 Payoff matrix: 

           0      
    0    49.61      

 

Social Welfare Sum Matrix: 

           0      
    0    99.22      

 

Iteration : 0
Time so far: 0.00012755393981933594
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-07-19 16:06:58.206995: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.12265938743948937 23.155233955383302 2.0698732852935793 0.001217056470341049 10089 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.10091072544455529 16.774170875549316 1.9047209739685058 0.23919936418533325 217191 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09310681000351906 13.28086576461792 1.857519268989563 0.3539880633354187 419049 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0862712800502777 13.998045253753663 1.8203909635543822 0.45625576972961424 621373 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08340119868516922 16.58674020767212 1.7522547125816346 0.5879178404808044 822936 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06790580898523331 14.330165386199951 1.705252730846405 0.6753224074840546 1025525 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06223361156880856 15.608568668365479 1.6812250375747682 0.8680338084697723 1227789 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06342878006398678 16.34850130081177 1.684006130695343 0.8583388149738311 1429730 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05418363511562348 20.155132675170897 1.619197428226471 1.0754857182502746 1634384 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04782401248812675 18.04278697967529 1.556584084033966 1.3297630190849303 1839437 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03737042658030987 21.265057373046876 1.501869547367096 1.422753393650055 2045980 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035901297442615034 21.058490753173828 1.4420871138572693 1.666356384754181 2253714 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027463805116713046 20.84542064666748 1.3683430194854735 1.6980373501777648 2462589 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026363736018538475 22.549174308776855 1.3488766312599183 1.8194271087646485 2671216 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019948774203658103 15.67909517288208 1.260595941543579 2.0606292128562926 2880359 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017421772237867117 26.448040008544922 1.245115911960602 2.1902310609817506 3089129 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01122995838522911 21.204373168945313 1.189765250682831 2.335272932052612 3300814 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008338697487488389 22.33960132598877 1.1107768058776855 2.729006624221802 3513865 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005825621704570949 23.453110122680663 1.0829540491104126 2.8176169395446777 3722285 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022368357167579234 25.25679111480713 0.9916825950145721 3.0051109313964846 3933070 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002282262413064018 24.261168479919434 0.911190789937973 3.2219998836517334 4148981 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0026931273168884218 25.40190906524658 0.7889509081840516 3.533069920539856 4362694 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017365715684718452 26.540958595275878 0.7242049932479858 3.8667135000228883 4575960 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013009820453589783 21.460768508911134 0.6742963314056396 4.317106914520264 4791703 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003295343485660851 20.200395107269287 0.6075352489948272 4.5758734226226805 5008620 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000870221428340301 23.85675506591797 0.5854344248771668 4.7913415908813475 5224791 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004628928581951186 22.383798599243164 0.5517647206783295 4.950242424011231 5441414 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007220440152195806 24.103105354309083 0.5535391092300415 5.188504362106324 5658848 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004922925966639013 25.866481018066406 0.5429021000862122 5.77760682106018 5876990 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001120998697297182 25.350521659851076 0.5411265105009079 5.832284879684448 6094273 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021984201055602172 24.344532775878907 0.499216291308403 6.023222398757935 6310351 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008664966990181711 23.9023006439209 0.49715224504470823 6.064825534820557 6525970 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014575986919226126 27.20563907623291 0.44389096796512606 6.250062799453735 6742023 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000983816635562107 28.264010620117187 0.42734919786453246 6.183497142791748 6959064 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002471220033476129 25.390052795410156 0.40710438787937164 6.454203128814697 7177659 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009965898978407494 21.792292976379393 0.4046419322490692 6.573564958572388 7393582 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013086328748613597 25.988243865966798 0.39910970330238343 6.586555337905883 7613582 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017144372774055227 26.89273910522461 0.38810276985168457 6.712199306488037 7831031 0


Pure best response payoff estimated to be 191.455 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 93.17 seconds to finish estimate with resulting utilities: [189.075   4.625]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 154.12 seconds to finish estimate with resulting utilities: [96.67 99.77]
Computing meta_strategies
Exited RRD with total regret 1.919057080617904 that was less than regret lambda 2.0 after 43 iterations 
REGRET STEPS:  20
NEW LAMBDA 1.8947368421052633
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    49.61           4.62      
    1    189.07          98.22      

 

Player 1 Payoff matrix: 

           0              1      
    0    49.61          189.07      
    1     4.62          98.22      

 

Social Welfare Sum Matrix: 

           0              1      
    0    99.22          193.70      
    1    193.70          196.44      

 

Metagame probabilities: 
Player #0: 0.0102  0.9898  
Player #1: 0.0102  0.9898  
Iteration : 1
Time so far: 6610.688790798187
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-07-19 17:57:09.024059: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012930537108331919 75.60751800537109 0.2350759744644165 9.467302989959716 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03320012707263231 13.982073020935058 0.6836768686771393 7.710654306411743 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0300436994060874 13.872782230377197 0.6741451025009155 6.52374300956726 449039 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03883596770465374 15.896615695953368 0.901355504989624 5.708778285980225 664053 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03623757809400559 18.05784168243408 0.8725260436534882 5.591600656509399 879889 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0351277194917202 15.005308055877686 0.9199239492416382 5.266993379592895 1095436 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032141261361539365 15.471176433563233 0.8934345364570617 5.462686204910279 1311814 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02817079536616802 21.36011028289795 0.8813386201858521 5.346899938583374 1525749 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022600946389138697 26.033179664611815 0.8153563678264618 5.5653914451599125 1740693 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02359181419014931 17.130718421936034 0.8984011888504029 5.031054401397705 1958038 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02170998081564903 19.962889862060546 0.8626788675785064 5.283445024490357 2171783 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02153836004436016 17.558927154541017 0.9471614003181458 5.197872638702393 2385396 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016588693391531704 19.123473358154296 0.8294802188873291 5.46935510635376 2602595 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014517712686210871 19.65371952056885 0.9599396526813507 5.111124658584595 2821250 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012693520169705153 13.87284231185913 0.9268721282482147 4.899003696441651 3037167 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009435395291075111 18.417571830749512 0.8629171669483184 5.191311073303223 3253217 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005810871254652739 18.525518703460694 0.8140429198741913 5.050976514816284 3470375 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005177914863452315 17.664611339569092 0.8536079525947571 5.16649694442749 3689826 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021750096988398583 19.048467350006103 0.7899242401123047 5.387109661102295 3908360 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012423251726431772 15.909891700744629 0.7705644905567169 5.498068761825562 4128360 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005470675881952047 18.382524394989012 0.6742928564548493 5.692720031738281 4347793 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012489458837080746 15.411857509613037 0.5687343180179596 6.091973543167114 4565113 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007816320037818513 21.92009391784668 0.5343084812164307 6.2223145961761475 4784768 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011022088321624324 21.41202621459961 0.5783696472644806 6.465992450714111 5004768 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00046698163514520276 17.612331676483155 0.5272488474845887 6.986529350280762 5224355 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014475473479251377 18.030428409576416 0.4513987898826599 7.109496116638184 5444355 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004814492873265408 20.932989883422852 0.43644990026950836 7.358997011184693 5663376 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004907872673356906 15.332055473327637 0.4675235837697983 7.693773317337036 5882892 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015734983870061114 19.218987274169923 0.41797866523265836 8.136313724517823 6102892 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -1.2991641415283085e-05 19.103389263153076 0.4711024105548859 8.077389526367188 6321601 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001606602454558015 17.297309494018556 0.4206206530332565 8.259791278839112 6540869 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00036908338079229 15.051850318908691 0.38802128434181216 8.291187858581543 6760869 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000739301745488774 16.143743419647215 0.3678410917520523 8.658545017242432 6979985 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004981710168067366 17.002614212036132 0.23570616543293 8.830496120452882 7199985 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010075313461129554 15.7319504737854 0.2277154490351677 8.640291309356689 7419859 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011163558272528462 18.323003196716307 0.19659982323646547 8.857362842559814 7639859 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014505423663649709 18.47742290496826 0.24846854358911513 8.735479927062988 7859586 0


Pure best response payoff estimated to be 134.445 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 94.8 seconds to finish estimate with resulting utilities: [182.355   2.615]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 157.12 seconds to finish estimate with resulting utilities: [133.37   49.385]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 156.83 seconds to finish estimate with resulting utilities: [39.175 38.46 ]
Computing meta_strategies
Exited RRD with total regret 1.8812741330276879 that was less than regret lambda 1.8947368421052633 after 136 iterations 
REGRET STEPS:  20
NEW LAMBDA 1.7894736842105263
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    49.61           4.62           2.62      
    1    189.07          98.22          49.38      
    2    182.35          133.37          38.82      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    49.61          189.07          182.35      
    1     4.62          98.22          133.37      
    2     2.62          49.38          38.82      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    99.22          193.70          184.97      
    1    193.70          196.44          182.75      
    2    184.97          182.75          77.63      

 

Metagame probabilities: 
Player #0: 0.0001  0.2994  0.7005  
Player #1: 0.0001  0.2994  0.7005  
Iteration : 2
Time so far: 15819.591816425323
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-07-19 20:30:38.021502: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016681736428290607 58.92018089294434 0.3244757533073425 10.521595668792724 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031127208098769187 20.660787200927736 0.6194124579429626 9.416207218170166 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02870322745293379 23.37406978607178 0.633406400680542 7.1164721012115475 449037 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030817458033561708 16.980030059814453 0.736222904920578 6.21217360496521 668148 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02632881198078394 16.5775053024292 0.6419670403003692 6.648124361038208 886724 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028745499439537525 17.62874355316162 0.77000772356987 6.289643001556397 1103150 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029639429226517678 15.957439613342284 0.8464332818984985 5.771199512481689 1318694 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02667457442730665 14.280691814422607 0.854776781797409 5.7894700050354 1529982 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02536563891917467 19.482404518127442 0.8719339191913604 5.664643049240112 1744311 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020566713623702527 18.138751316070557 0.7289133191108703 6.212588977813721 1957318 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01948158722370863 19.445785331726075 0.8105682492256164 5.87380952835083 2173893 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017488340102136134 17.227488231658935 0.8360152423381806 5.462045669555664 2388649 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01582514364272356 18.236477088928222 0.8074862241744996 5.866207456588745 2603609 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013030279148370027 18.862041473388672 0.8938789606094361 5.5493940830230715 2816371 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009702965198084713 23.030511093139648 0.7687212526798248 6.331703090667725 3030967 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008716357592493295 11.524071407318115 0.8105805277824402 5.876432657241821 3240936 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008047999907284975 18.28920383453369 0.7569341540336609 6.198213386535644 3452833 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003265052486676723 16.539511299133302 0.8242847084999084 6.176086950302124 3666860 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0027654840843752025 23.287563514709472 0.6685492515563964 6.703265380859375 3879210 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014019209673278965 12.948922443389893 0.6646039605140686 6.65230507850647 4094986 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000667099173006136 19.650656509399415 0.7245644092559814 5.641762018203735 4309918 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009692155348602682 19.800107765197755 0.6485381782054901 6.377352333068847 4526154 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011334843095937685 16.57169418334961 0.607084596157074 7.069827365875244 4743080 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00024921446820371786 19.519340324401856 0.45650831460952757 7.537375211715698 4960368 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005432971520349383 18.325800704956055 0.4832311898469925 7.198312664031983 5180368 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007847556029446423 13.424871349334717 0.6045846045017242 7.702937841415405 5398984 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000512430936214514 24.660167503356934 0.46140327453613283 7.4351929187774655 5617193 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004695642273873091 18.640965080261232 0.3715256631374359 7.929479932785034 5837009 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000768199993763119 23.91026611328125 0.3611970990896225 8.372829246520997 6056061 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00019455743167782203 13.690828609466553 0.48115027248859404 8.65643606185913 6276061 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001197672763009905 18.005448055267333 0.44846161007881163 8.674460220336915 6496061 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013061032653240545 17.023793411254882 0.40756352841854093 8.795088291168213 6716061 0
slurmstepd: error: *** JOB 56105357 ON gl3435 CANCELLED AT 2023-07-19T22:40:10 ***
