Job Id listed below:
56042731

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

2023-07-18 13:25:32.268763: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-18 13:25:32.312336: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 13:25:33.295082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0718 13:25:35.028060 23376354990976 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x15427215ad40>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x15427215ad40>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-07-18 13:25:35.323383: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-07-18 13:25:35.595468: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.86 seconds to finish estimate with resulting utilities: [48.465 48.81 ]
Exited RRD with total regret 0.0 that was less than regret lambda 10.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    48.64      

 

Player 1 Payoff matrix: 

           0      
    0    48.64      

 

Social Welfare Sum Matrix: 

           0      
    0    97.28      

 

Iteration : 0
Time so far: 0.00012826919555664062
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-07-18 13:25:55.358261: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.12189034521579742 21.397715377807618 2.0681934356689453 0.002405294997151941 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09939372316002845 14.911907768249511 1.878815245628357 0.2521119982004166 216746 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09438719153404236 14.570837688446044 1.8454463481903076 0.33196359872817993 418960 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08662864416837693 16.31405076980591 1.8000943064689636 0.4578799486160278 620920 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0813281662762165 13.925327491760253 1.7826372265815735 0.519935342669487 822294 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07433904558420182 16.782409381866454 1.7612789750099183 0.676171875 1024750 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06892401352524757 13.291503143310546 1.7381731033325196 0.7219267904758453 1226646 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05705796033143997 17.077541446685792 1.6439948201179504 0.896720415353775 1429289 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05336364582180977 20.11014633178711 1.6296989917755127 1.0027891635894775 1632738 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04588879868388176 17.54922389984131 1.547275996208191 1.089992594718933 1837155 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.040611902996897695 19.216546249389648 1.5795162916183472 1.25921550989151 2042920 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03523109182715416 16.857046699523927 1.524947500228882 1.3460384607315063 2250154 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02838496621698141 21.105062675476074 1.407283103466034 1.4382780075073243 2458043 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025868264213204383 20.29691162109375 1.3894951343536377 1.6754656195640565 2668343 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020145415514707565 18.174104309082033 1.2954362750053405 1.932930338382721 2878403 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015258182398974895 19.165774726867674 1.2139633655548097 2.095187783241272 3087086 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011883612815290689 20.44515438079834 1.128657615184784 2.3325313329696655 3298158 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009000567812472582 20.659778022766112 1.0311166882514953 2.5543335676193237 3510340 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006160681089386344 27.327829360961914 0.9943786323070526 2.776213550567627 3721345 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003501005034195259 21.71970100402832 0.9337102949619294 2.9904014587402346 3935171 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001549600290309172 22.836208724975585 0.8821839392185211 3.338575530052185 4151476 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015061656624311582 26.37070713043213 0.7976991653442382 3.5612942457199095 4365342 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -3.8335238059516996e-05 28.26492233276367 0.7188075900077819 3.9364381551742555 4580907 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016316104112775065 22.146630859375 0.7138664841651916 4.018068385124207 4795568 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010411083669168875 20.394746780395508 0.6671930074691772 4.244583368301392 5007537 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000956508397939615 20.898570823669434 0.613003796339035 4.394946432113647 5223229 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002046644504298456 22.38838882446289 0.55970339179039 4.928552722930908 5441907 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -8.608562857261858e-05 23.129975509643554 0.5260396420955658 5.268535995483399 5658642 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005070092593086883 24.784610557556153 0.4914312928915024 5.580264282226563 5876193 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0020126169540162662 27.998984146118165 0.47835242450237275 5.733115386962891 6094619 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006536412096465938 22.12015438079834 0.4443460524082184 6.228297138214112 6311704 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003648633432021597 23.297498512268067 0.45129139721393585 6.331702470779419 6528709 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000710771602098248 23.61482810974121 0.4443362593650818 6.391048288345337 6743965 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00019778705536737106 27.97123279571533 0.4265740722417831 6.7310418605804445 6962758 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005043381039286033 19.861513137817383 0.40081809759140014 7.31033148765564 7181357 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00036469940678216515 21.851400756835936 0.4070573717355728 7.431637382507324 7399227 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011553989046660718 25.181949043273924 0.37579222321510314 7.526579093933106 7616082 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019917983503546567 26.748208045959473 0.36158725023269656 7.76654748916626 7832955 0


Pure best response payoff estimated to be 191.865 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 98.16 seconds to finish estimate with resulting utilities: [190.35    4.465]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 162.57 seconds to finish estimate with resulting utilities: [94.855 95.91 ]
Computing meta_strategies
Exited RRD with total regret 9.388486866425382 that was less than regret lambda 10.0 after 27 iterations 
REGRET STEPS:  25
NEW LAMBDA 9.583333333333334
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    48.64           4.46      
    1    190.35          95.38      

 

Player 1 Payoff matrix: 

           0              1      
    0    48.64          190.35      
    1     4.46          95.38      

 

Social Welfare Sum Matrix: 

           0              1      
    0    97.28          194.81      
    1    194.81          190.76      

 

Metagame probabilities: 
Player #0: 0.0502  0.9498  
Player #1: 0.0502  0.9498  
Iteration : 1
Time so far: 7266.78363776207
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-07-18 15:27:02.347691: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022733776178210974 83.08894805908203 0.4282948225736618 8.908053779602051 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028987129405140876 13.470553207397462 0.6019078671932221 7.374750804901123 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02951571512967348 15.357655429840088 0.6484893143177033 6.629841995239258 449041 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.039115052297711374 12.400026035308837 0.8965369164943695 4.979269504547119 664527 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03603283874690533 21.238203907012938 0.9055228054523468 4.856610345840454 882908 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03552442900836468 16.464931392669676 0.9235498070716858 4.902612590789795 1098465 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03083598781377077 19.333163833618165 0.8847114741802216 4.839502906799316 1316644 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029376534186303614 17.82468738555908 0.9243454039096832 4.52583909034729 1531867 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02877802327275276 15.224765205383301 0.9740727365016937 4.522985935211182 1747141 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026812104508280754 19.810059547424316 0.9991882622241974 4.409356641769409 1962447 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023371682874858378 15.474011421203613 0.9351266205310822 4.599846076965332 2177337 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018350687623023988 20.711572074890135 0.8750719666481018 4.829412364959717 2390374 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017471747938543558 15.687570095062256 0.9452109634876251 4.536098623275757 2601154 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015790916327387095 13.952770137786866 0.9481240153312683 4.540284442901611 2812915 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01221769517287612 19.814031219482423 0.9075145423412323 4.655865430831909 3026411 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009857371728867293 19.258705139160156 0.8864914774894714 5.228873300552368 3241299 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006050564162433147 19.7386682510376 0.8161683440208435 5.091700458526612 3458030 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003616531938314438 20.823896217346192 0.7166620910167694 5.254129791259766 3674664 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0023726820247247813 18.321997261047365 0.7012248635292053 5.599750709533692 3891447 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011781993278418667 19.7254545211792 0.6785161375999451 5.610196304321289 4107762 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009998538414947688 15.531517124176025 0.5483212053775788 5.687931680679322 4323969 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006634621822740883 15.515739822387696 0.4864658355712891 6.054203462600708 4542650 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007713090570177882 19.97460479736328 0.5426951020956039 6.121107482910157 4758231 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00044385023356880994 18.669865608215332 0.5441028058528901 6.441458702087402 4975116 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016130096977576614 22.809166145324706 0.5104731440544128 6.548970460891724 5194907 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000371253106277436 18.30594367980957 0.5205789297819138 6.463274955749512 5413107 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014644629962276667 20.041992568969725 0.4792033970355988 6.66014347076416 5632345 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -7.341408636420965e-05 17.4954665184021 0.3991031140089035 7.107096099853516 5849921 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021409942215541377 23.222784805297852 0.3835628181695938 7.327748775482178 6069392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002979812885314459 23.42740249633789 0.44728252589702605 7.094405317306519 6286721 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00038549075034097766 17.994078350067138 0.4203339427709579 7.287383556365967 6505980 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00030097456328803673 17.314407730102538 0.41469735503196714 7.430168485641479 6725980 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009419879622555527 19.967729759216308 0.36904813051223756 7.691588735580444 6945980 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006032939874785371 19.192972946166993 0.3439860463142395 7.562969589233399 7164391 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010453289010911248 21.154201126098634 0.3949278652667999 7.514332294464111 7381779 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003091095868512639 16.454306888580323 0.4168125420808792 7.958041477203369 7598940 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006016436924255686 23.315114974975586 0.3664578914642334 8.36244068145752 7815397 0


Pure best response payoff estimated to be 125.47 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 96.12 seconds to finish estimate with resulting utilities: [170.545   2.27 ]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 161.8 seconds to finish estimate with resulting utilities: [123.325  53.29 ]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 161.28 seconds to finish estimate with resulting utilities: [42.445 42.815]
Computing meta_strategies
Exited RRD with total regret 9.43081467990109 that was less than regret lambda 9.583333333333334 after 35 iterations 
REGRET STEPS:  25
NEW LAMBDA 9.166666666666668
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    48.64           4.46           2.27      
    1    190.35          95.38          53.29      
    2    170.54          123.33          42.63      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    48.64          190.35          170.54      
    1     4.46          95.38          123.33      
    2     2.27          53.29          42.63      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    97.28          194.81          172.81      
    1    194.81          190.76          176.62      
    2    172.81          176.62          85.26      

 

Metagame probabilities: 
Player #0: 0.0236  0.4485  0.5279  
Player #1: 0.0236  0.4485  0.5279  
Iteration : 2
Time so far: 16945.385586738586
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-07-18 18:08:21.020119: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0209305202588439 61.07499961853027 0.3599592685699463 8.933289813995362 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03663306944072246 18.508660125732423 0.7638826847076416 6.54737343788147 225803 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03555595874786377 16.60988759994507 0.6808635234832764 6.5335917472839355 438714 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0333802230656147 19.234580421447752 0.7903655529022217 6.077624273300171 651612 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02886772472411394 30.332852935791017 0.7086472988128663 6.303468847274781 863575 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028716687113046646 23.43872833251953 0.7490886688232422 6.0976855754852295 1078153 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02707503866404295 29.97919445037842 0.7900832414627075 5.770910310745239 1289487 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026716293580830098 16.731089782714843 0.8387345135211944 6.020001840591431 1504877 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0246434198692441 15.929708480834961 0.8204315185546875 5.954946851730346 1719501 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023537067882716655 17.43586082458496 0.8763718903064728 5.7158373355865475 1932995 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020395776070654393 17.171903800964355 0.8146789908409119 6.034400177001953 2148357 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018423554208129644 14.21906623840332 0.7787059724330903 6.180943632125855 2360341 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015520413126796483 17.246520900726317 0.8848756611347198 5.49699182510376 2574971 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017524081747978926 16.86540288925171 0.8398738503456116 5.687687253952026 2789886 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010767315793782473 19.255112648010254 0.8545087277889252 5.474900913238526 3002289 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009568852558732032 18.938126945495604 0.8470006227493286 5.437314176559449 3217291 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007679698243737221 20.815430450439454 0.868125855922699 5.768525743484497 3429705 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008439388824626803 18.701675987243654 0.914154839515686 5.120214557647705 3644887 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00503658780362457 20.031715965270998 0.768435025215149 5.9222077369689945 3856973 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021865718925255353 25.718578338623047 0.7043736934661865 6.02474889755249 4068718 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0029537395806983114 15.969105434417724 0.7617056965827942 5.84375786781311 4280106 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003151774738216773 13.968706226348877 0.7279273211956024 6.094981384277344 4490764 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003687209589406848 18.6662446975708 0.6916275084018707 5.791612386703491 4704199 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00019856374347000384 22.68960361480713 0.7263426899909973 5.779853248596192 4917756 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00147655452019535 16.171741580963136 0.7235988974571228 5.693818140029907 5130727 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006327197863720357 39.85928802490234 0.6039272010326385 6.924403667449951 5345275 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003065105032874271 15.96979808807373 0.6343484997749329 6.209307527542114 5560100 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0034001002029981466 17.698499488830567 0.6625904440879822 6.161244487762451 5773296 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004501768900081515 22.521694374084472 0.6832492709159851 5.575816202163696 5985719 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  1.166034780908376e-05 21.05635013580322 0.6036894142627716 6.210158395767212 6199417 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005187191636650823 24.70660266876221 0.5783735275268554 6.400709199905395 6414445 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00810643145814538 17.08916368484497 0.6572601556777954 6.1192058563232425 6628660 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002315633706166409 24.381457710266112 0.5516803741455079 7.118557691574097 6843064 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001776349484862294 27.22678871154785 0.4758635640144348 7.2661436080932615 7055621 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009864161271252669 20.098086166381837 0.5569707304239273 7.1447686672210695 7267241 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016468854309550807 17.53793296813965 0.6148746252059937 6.5447736263275145 7480119 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009039732027304126 21.35507164001465 0.5893733143806458 6.656114339828491 7694571 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019461267977021635 20.603000259399415 0.6620355606079101 6.296199941635132 7909831 0


Pure best response payoff estimated to be 96.28 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 87.3 seconds to finish estimate with resulting utilities: [145.385   2.225]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 158.55 seconds to finish estimate with resulting utilities: [114.15  55.08]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 162.7 seconds to finish estimate with resulting utilities: [75.76  84.505]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 155.74 seconds to finish estimate with resulting utilities: [74.6  72.87]
Computing meta_strategies
Exited RRD with total regret 9.107346797760783 that was less than regret lambda 9.166666666666668 after 80 iterations 
REGRET STEPS:  25
NEW LAMBDA 8.750000000000002
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0    48.64           4.46           2.27           2.23      
    1    190.35          95.38          53.29          55.08      
    2    170.54          123.33          42.63          84.50      
    3    145.38          114.15          75.76          73.73      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0    48.64          190.35          170.54          145.38      
    1     4.46          95.38          123.33          114.15      
    2     2.27          53.29          42.63          75.76      
    3     2.23          55.08          84.50          73.73      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0    97.28          194.81          172.81          147.61      
    1    194.81          190.76          176.62          169.23      
    2    172.81          176.62          85.26          160.26      
    3    147.61          169.23          160.26          147.47      

 

Metagame probabilities: 
Player #0: 0.0006  0.1258  0.3652  0.5085  
Player #1: 0.0006  0.1258  0.3652  0.5085  
Iteration : 3
Time so far: 26859.428449630737
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-07-18 20:53:35.201161: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7779 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03762206491082907 53.220863342285156 0.5623736560344696 7.505975580215454 10193 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035354070365428925 15.67699375152588 0.6839610636234283 6.932167291641235 223662 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028303584083914757 22.724670219421387 0.6018798232078553 7.795606279373169 434827 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030615631304681302 15.115954494476318 0.7017814338207244 6.756685495376587 643864 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03334192130714655 17.624839115142823 0.8243533074855804 6.112911176681519 852661 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02719931360334158 16.96259250640869 0.6663023889064789 7.396494197845459 1061341 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028127346746623517 21.68188533782959 0.7758914291858673 6.805743646621704 1269325 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027733150124549865 27.892507553100586 0.6295332789421082 7.401318883895874 1477505 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025227667205035685 25.844889068603514 0.6523360848426819 7.330642557144165 1687830 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01710949055850506 24.720011520385743 0.44140630662441255 7.960389280319214 1897485 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009464733675122261 21.43603973388672 0.328440797328949 7.710513067245484 2106741 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017005209531635045 14.006484603881836 0.6376527965068817 7.163255882263184 2318105 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01580181494355202 20.60775833129883 0.46742006242275236 7.433130550384521 2528897 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009174253325909376 20.20303592681885 0.46607851386070254 7.622052621841431 2739564 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010592774860560894 19.82954387664795 0.41285237669944763 8.015475034713745 2948937 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006203822768293321 18.48437738418579 0.5898579359054565 6.830964183807373 3158890 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007308719307184219 23.219148445129395 0.44316632449626925 8.426983547210693 3370387 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017750201048329472 26.63111743927002 0.186684051156044 9.273865222930908 3582638 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007777835970045998 16.167048072814943 0.22544841915369035 8.61048641204834 3791822 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004375853459350765 29.523977851867677 0.17866167724132537 9.085035419464111 4003990 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002867313116439618 23.87792625427246 0.17205379605293275 9.443518924713135 4214934 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00032170343620236964 25.220633506774902 0.1542971044778824 9.627953243255615 4429726 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007112144259735942 14.3936185836792 0.12102420404553413 10.497323703765868 4644498 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015338470253482227 19.83841953277588 0.14886471778154373 11.069810009002685 4860294 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000637860700226156 20.650909423828125 0.11467518955469132 11.750467205047608 5076477 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000323298175499076 28.111078453063964 0.10385820344090461 11.419723796844483 5292455 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00032388027320848776 18.80151309967041 0.09833471179008484 11.596590042114258 5511447 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004816260690859053 26.227950668334962 0.13818541318178176 11.21991949081421 5730169 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00176910747832153 28.234708404541017 0.07381169125437737 12.726810836791993 5947633 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008674002059706254 24.333846473693846 0.0880917839705944 12.082586288452148 6164786 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010044709197245538 23.785103225708006 0.10483927056193351 12.129766273498536 6383866 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000554950644436758 17.741412734985353 0.09787740409374238 12.092939567565917 6601385 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00022088029709266266 24.552080726623537 0.09558065161108971 11.961389350891114 6819380 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010651538643287495 36.261962890625 0.07434816248714923 12.79036054611206 7035327 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000723358424147591 17.40762367248535 0.08570544607937336 12.606019878387452 7253414 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00026510678580962124 30.088132286071776 0.07552786134183406 13.138880920410156 7471770 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018407780400593766 17.1299747467041 0.11665256693959236 12.051249694824218 7691328 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001314116558205569 18.113371086120605 0.08017434775829316 12.741453456878663 7909011 0


Pure best response payoff estimated to be 93.86 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 92.86 seconds to finish estimate with resulting utilities: [144.135   4.245]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 162.45 seconds to finish estimate with resulting utilities: [114.655  48.7  ]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 161.44 seconds to finish estimate with resulting utilities: [68.705 48.565]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 162.06 seconds to finish estimate with resulting utilities: [96.525 53.66 ]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 163.13 seconds to finish estimate with resulting utilities: [64.595 63.87 ]
Computing meta_strategies
Exited RRD with total regret 8.683885471186414 that was less than regret lambda 8.750000000000002 after 142 iterations 
REGRET STEPS:  25
NEW LAMBDA 8.333333333333336
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0    48.64           4.46           2.27           2.23           4.25      
    1    190.35          95.38          53.29          55.08          48.70      
    2    170.54          123.33          42.63          84.50          48.56      
    3    145.38          114.15          75.76          73.73          53.66      
    4    144.13          114.66          68.70          96.53          64.23      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0    48.64          190.35          170.54          145.38          144.13      
    1     4.46          95.38          123.33          114.15          114.66      
    2     2.27          53.29          42.63          75.76          68.70      
    3     2.23          55.08          84.50          73.73          96.53      
    4     4.25          48.70          48.56          53.66          64.23      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0    97.28          194.81          172.81          147.61          148.38      
    1    194.81          190.76          176.62          169.23          163.36      
    2    172.81          176.62          85.26          160.26          117.27      
    3    147.61          169.23          160.26          147.47          150.19      
    4    148.38          163.36          117.27          150.19          128.47      

 

Metagame probabilities: 
Player #0: 0.0001  0.0337  0.104  0.1916  0.6706  
Player #1: 0.0001  0.0337  0.104  0.1916  0.6706  
Iteration : 4
Time so far: 37142.949602365494
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-07-18 23:44:58.858868: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9757 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004949400131590664 45.491064453125 0.08399427607655525 12.57698621749878 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014077778998762369 21.003708267211913 0.27017742246389387 11.904216194152832 226565 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016743055544793606 15.508938980102538 0.3062972664833069 12.137888145446777 438690 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01584802381694317 16.903097343444824 0.34663696587085724 11.417663192749023 652827 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014235062710940837 15.902191638946533 0.303324031829834 11.478952503204345 861673 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015858690440654754 18.87304630279541 0.3433342844247818 10.895497226715088 1069929 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012841574102640151 13.612717342376708 0.3137326717376709 11.782869434356689 1276827 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012358425371348857 16.24934539794922 0.2583252742886543 11.431272411346436 1483084 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012455120868980884 14.027864360809327 0.3880439937114716 11.029316997528076 1693123 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01117406291887164 15.131089115142823 0.3313512772321701 11.301359462738038 1903744 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009210648899897932 16.844046115875244 0.22409874349832534 11.278139781951904 2113946 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009184800321236253 14.68981761932373 0.3136706858873367 11.516523361206055 2321756 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005746468272991479 18.643264293670654 0.2843763381242752 11.262250804901123 2529541 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010642409464344383 24.394666862487792 0.2786464601755142 11.755683422088623 2741810 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006265167752280832 14.43230562210083 0.35341936349868774 11.28236427307129 2952144 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004718704172410071 20.56453628540039 0.33625341653823854 11.429717063903809 3161532 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0037326807738281786 15.365835189819336 0.1766830176115036 11.728226566314698 3369569 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00490869355853647 14.735021686553955 0.32398285567760465 10.957580471038819 3580400 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0048934152233414355 11.78715124130249 0.35257708132266996 10.985602188110352 3788011 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00431739583145827 24.49797420501709 0.2004833534359932 11.629855728149414 3998296 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0002516465086955577 13.180138397216798 0.15207559466362 11.689955234527588 4207736 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0025646439520642162 23.3380916595459 0.13211768493056297 12.087847900390624 4415405 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009323352831415832 18.290401649475097 0.2662794843316078 11.79918909072876 4621306 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002599924529204145 24.49569454193115 0.24510911405086516 12.028883266448975 4830026 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021953823234071024 20.9950044631958 0.11764539405703545 11.914348793029784 5037752 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018182666462962516 15.940096855163574 0.1589553698897362 12.011503219604492 5244967 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003011168614574444 18.233010482788085 0.2679269820451736 11.764262580871582 5453039 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009721041133161634 26.375508117675782 0.2791860356926918 11.737100601196289 5663976 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0041515206976328045 11.661283397674561 0.3397796809673309 12.048885726928711 5874580 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007698034262284636 13.924405479431153 0.28752721548080445 12.161657238006592 6084149 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00027401993284001944 15.86289873123169 0.10048913881182671 12.073209381103515 6293463 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008066936061823071 25.5499568939209 0.09602732509374619 12.5313551902771 6502303 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0020062012612470427 12.316058158874512 0.14663368165493013 12.078282165527344 6713026 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004405404010321945 14.614004707336425 0.2389148458838463 12.230849933624267 6920145 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006814090302214026 19.744221687316895 0.09366120249032975 12.493842315673827 7129015 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000621449675236363 14.447635746002197 0.08445778042078018 12.632120895385743 7337086 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -1.3875386503059418e-05 22.112988090515138 0.06511761471629143 13.201982879638672 7546444 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019758183392696083 12.070363426208496 0.10534240230917931 12.420201396942138 7757556 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005059851083206013 17.694643592834474 0.11788098365068436 12.244599056243896 7968225 0
Recovering previous policy with expected return of 70.1542288557214. Long term value was 56.354 and short term was 56.23.


Pure best response payoff estimated to be 71.28 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 92.59 seconds to finish estimate with resulting utilities: [141.59    4.405]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 163.2 seconds to finish estimate with resulting utilities: [114.235  48.715]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 165.0 seconds to finish estimate with resulting utilities: [73.335 49.37 ]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 165.95 seconds to finish estimate with resulting utilities: [97.275 54.29 ]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 165.22 seconds to finish estimate with resulting utilities: [61.115 62.085]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 163.72 seconds to finish estimate with resulting utilities: [59.58  63.425]
Computing meta_strategies
Exited RRD with total regret 8.282220299247243 that was less than regret lambda 8.333333333333336 after 100 iterations 
REGRET STEPS:  25
NEW LAMBDA 7.916666666666669
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0    48.64           4.46           2.27           2.23           4.25           4.41      
    1    190.35          95.38          53.29          55.08          48.70          48.72      
    2    170.54          123.33          42.63          84.50          48.56          49.37      
    3    145.38          114.15          75.76          73.73          53.66          54.29      
    4    144.13          114.66          68.70          96.53          64.23          62.09      
    5    141.59          114.23          73.33          97.28          61.12          61.50      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0    48.64          190.35          170.54          145.38          144.13          141.59      
    1     4.46          95.38          123.33          114.15          114.66          114.23      
    2     2.27          53.29          42.63          75.76          68.70          73.33      
    3     2.23          55.08          84.50          73.73          96.53          97.28      
    4     4.25          48.70          48.56          53.66          64.23          61.12      
    5     4.41          48.72          49.37          54.29          62.09          61.50      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0    97.28          194.81          172.81          147.61          148.38          146.00      
    1    194.81          190.76          176.62          169.23          163.36          162.95      
    2    172.81          176.62          85.26          160.26          117.27          122.70      
    3    147.61          169.23          160.26          147.47          150.19          151.56      
    4    148.38          163.36          117.27          150.19          128.47          123.20      
    5    146.00          162.95          122.70          151.56          123.20          123.00      

 

Metagame probabilities: 
Player #0: 0.0002  0.0533  0.1036  0.1567  0.3474  0.3389  
Player #1: 0.0002  0.0533  0.1036  0.1567  0.3474  0.3389  
Iteration : 5
Time so far: 47614.83625841141
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-07-19 02:39:30.892574: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11735 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005947398021817207 73.27878112792969 0.10762003138661384 11.939643573760986 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027791043184697627 14.759220790863036 0.5222876369953156 10.725026607513428 225770 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0308387728407979 11.209696960449218 0.592307323217392 10.52871389389038 438008 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022791226021945478 15.779249668121338 0.46327102184295654 10.691746616363526 648849 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021972936950623988 20.208444023132323 0.45575774908065797 10.789290046691894 860825 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01306481333449483 19.073081684112548 0.1882977530360222 11.179178714752197 1072093 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013145808409899474 14.262359428405762 0.3872028261423111 10.31653127670288 1279768 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01468769907951355 21.11657123565674 0.2146753340959549 10.768656635284424 1491150 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018025094643235207 15.694319820404052 0.3588420212268829 10.618452548980713 1698738 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011842235783115029 13.808126163482665 0.3687650114297867 10.37024154663086 1905873 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010676944022998213 19.460185623168947 0.3677689224481583 10.445750904083251 2113036 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010036565968766809 12.48229284286499 0.45755758285522463 10.199785137176514 2319990 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01012847749516368 19.486987590789795 0.3483074545860291 10.724600028991699 2528589 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007215171353891492 17.999986839294433 0.4477671474218369 10.288935089111328 2736507 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009629820007830858 15.868187141418456 0.4572155624628067 10.236795902252197 2943605 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005800907197408378 20.279476165771484 0.3674433708190918 10.586182975769043 3151353 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0033017532259691507 11.15941333770752 0.17271521985530852 10.796547031402588 3359506 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022402839473215862 12.194218254089355 0.19279849231243135 10.856916809082032 3565741 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002191354733076878 22.54882926940918 0.1536201000213623 10.98937702178955 3776092 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008950830320827663 22.295842170715332 0.16866766512393952 11.081807804107665 3984953 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013211343495640904 13.965992259979249 0.1955847755074501 11.133743858337402 4193819 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00015202669601421803 21.51190814971924 0.1293213114142418 11.21981725692749 4399801 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001980755057593342 16.35719003677368 0.17082261592149733 10.852692699432373 4606820 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00024191890843212606 19.06574230194092 0.12404097020626068 11.764108371734618 4816921 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008792001637630165 30.428738594055176 0.10638960525393486 11.638512420654298 5022718 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006834655068814754 17.814704132080077 0.1148674800992012 11.778405284881591 5231134 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002775750734144822 16.295712566375734 0.09825228676199912 12.241598892211915 5438370 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016660559860611102 14.93348388671875 0.14696081206202508 11.863028526306152 5646888 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004887606744887307 32.028322792053224 0.1012358158826828 12.252661895751952 5855938 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000737725879298523 12.763569927215576 0.09688139855861663 12.142524719238281 6064167 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008535754401236773 15.798669338226318 0.08126832470297814 12.230798625946045 6271199 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004906456193566555 23.679498291015626 0.07024951912462711 12.544368934631347 6479136 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00045312597994779936 18.3673038482666 0.08210711479187012 12.345417881011963 6687874 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000694772041606484 17.46897964477539 0.06459351181983948 12.369601821899414 6895295 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003234453877666965 9.70535650253296 0.07688062340021133 12.1650954246521 7103238 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006659179958660389 12.787365627288818 0.05140649676322937 12.321273136138917 7311499 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017459028815210332 16.977664947509766 0.06775119602680206 12.369885730743409 7517826 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016669038406689652 13.52418909072876 0.05900440849363804 12.354444694519042 7725037 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00022417982763727194 12.162975692749024 0.05096690468490124 12.415461158752441 7933290 0
Recovering previous policy with expected return of 68.8407960199005. Long term value was 51.755 and short term was 51.17.


Pure best response payoff estimated to be 71.225 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 91.15 seconds to finish estimate with resulting utilities: [137.14    3.755]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 165.32 seconds to finish estimate with resulting utilities: [114.46   49.805]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 164.27 seconds to finish estimate with resulting utilities: [70.1  48.15]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 162.98 seconds to finish estimate with resulting utilities: [94.715 54.05 ]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 164.37 seconds to finish estimate with resulting utilities: [59.095 57.66 ]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 164.84 seconds to finish estimate with resulting utilities: [60.885 60.86 ]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 163.76 seconds to finish estimate with resulting utilities: [61.885 62.365]
Computing meta_strategies
Exited RRD with total regret 7.851526546595963 that was less than regret lambda 7.916666666666669 after 76 iterations 
REGRET STEPS:  25
NEW LAMBDA 7.500000000000002
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    48.64           4.46           2.27           2.23           4.25           4.41           3.75      
    1    190.35          95.38          53.29          55.08          48.70          48.72          49.80      
    2    170.54          123.33          42.63          84.50          48.56          49.37          48.15      
    3    145.38          114.15          75.76          73.73          53.66          54.29          54.05      
    4    144.13          114.66          68.70          96.53          64.23          62.09          57.66      
    5    141.59          114.23          73.33          97.28          61.12          61.50          60.86      
    6    137.14          114.46          70.10          94.72          59.09          60.88          62.12      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    48.64          190.35          170.54          145.38          144.13          141.59          137.14      
    1     4.46          95.38          123.33          114.15          114.66          114.23          114.46      
    2     2.27          53.29          42.63          75.76          68.70          73.33          70.10      
    3     2.23          55.08          84.50          73.73          96.53          97.28          94.72      
    4     4.25          48.70          48.56          53.66          64.23          61.12          59.09      
    5     4.41          48.72          49.37          54.29          62.09          61.50          60.88      
    6     3.75          49.80          48.15          54.05          57.66          60.86          62.12      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0    97.28          194.81          172.81          147.61          148.38          146.00          140.89      
    1    194.81          190.76          176.62          169.23          163.36          162.95          164.26      
    2    172.81          176.62          85.26          160.26          117.27          122.70          118.25      
    3    147.61          169.23          160.26          147.47          150.19          151.56          148.76      
    4    148.38          163.36          117.27          150.19          128.47          123.20          116.75      
    5    146.00          162.95          122.70          151.56          123.20          123.00          121.75      
    6    140.89          164.26          118.25          148.76          116.75          121.75          124.25      

 

Metagame probabilities: 
Player #0: 0.0008  0.0666  0.1006  0.1381  0.2324  0.2412  0.2202  
Player #1: 0.0008  0.0666  0.1006  0.1381  0.2324  0.2412  0.2202  
Iteration : 6
Time so far: 58303.51264643669
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-07-19 05:37:39.664833: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13713 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0076939878985285756 60.036608123779295 0.12086719423532485 11.869844532012939 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011580741219222545 20.990006256103516 0.2133607432246208 12.484334087371826 228464 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015257459972053767 13.711911296844482 0.31589427292346955 11.913782596588135 444933 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019255691953003407 12.648180198669433 0.3993078500032425 11.104251956939697 656627 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013508348632603884 11.109368228912354 0.31367915868759155 11.514679908752441 868943 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013831811677664518 20.794612312316893 0.3482461333274841 11.024462413787841 1079781 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010943389171734453 13.698657608032226 0.3094392865896225 11.412403106689453 1289722 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007823353447020055 14.10596170425415 0.24280352592468263 11.842072010040283 1499836 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01145266555249691 14.998534774780273 0.31228038370609285 11.280108642578124 1711246 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010548601765185595 14.173313236236572 0.36035346388816836 11.284285831451417 1921956 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009334116522222758 15.47071647644043 0.32823445200920104 11.407653713226319 2133007 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0072434624191373585 19.236401557922363 0.24406141340732573 11.305649948120116 2342714 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005780821712687611 12.943193340301514 0.21688736528158187 11.381062984466553 2550946 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0059249238576740025 17.794363498687744 0.24678786098957062 11.577841758728027 2757456 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005336554278619588 13.760002517700196 0.40185037553310393 11.60064344406128 2963735 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005518670426681638 14.56902379989624 0.2735549986362457 11.301062297821044 3170561 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005896851606667042 9.830538177490235 0.3538610190153122 10.942863845825196 3378547 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0029341545596253127 12.870011711120606 0.27394068390131 11.938683700561523 3583320 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0036927020526491107 22.898986053466796 0.30020221769809724 11.51633939743042 3788220 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010844690808880842 15.605178165435792 0.3172958821058273 11.460077857971191 3997455 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004295987309888005 8.433947277069091 0.30797522366046903 11.862109851837157 4204822 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002655218326253816 13.956465435028075 0.33095515668392184 11.488048267364501 4410117 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0024246662971563636 13.243816566467284 0.29897602200508117 11.58638048171997 4616085 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008436484218691476 12.580091762542725 0.1429094597697258 11.850555801391602 4821569 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000890529906610027 16.22831087112427 0.11193305104970933 12.137380981445313 5027192 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002160928162629716 18.6059832572937 0.1645260438323021 11.969137382507324 5232842 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0053216367028653625 14.772283554077148 0.31240934431552886 11.80499334335327 5439039 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005662189211579971 31.426908874511717 0.22064129263162613 12.12407464981079 5644063 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00327766869449988 11.773198318481445 0.3191248714923859 11.875097465515136 5848123 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006097615091130137 22.46019878387451 0.21339777261018752 12.214943218231202 6052705 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0046854684218487815 22.349562072753905 0.2355696067214012 12.245907878875732 6257012 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013369557833357248 16.305048942565918 0.2530420109629631 11.841189861297607 6461924 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007181879831478 10.166121482849121 0.295916223526001 12.1551589012146 6666536 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002661910954338964 12.604837703704835 0.2605971932411194 11.985440063476563 6871193 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00017675287163001485 16.737049865722657 0.2578878775238991 11.689754581451416 7077369 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0033865669756778516 9.948921966552735 0.20266362875699998 11.991241359710694 7281655 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005550554129877127 10.92850856781006 0.18473103791475295 11.997172164916993 7490488 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0033842002623714505 15.514195728302003 0.12143552005290985 12.619941711425781 7696290 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007316819348488935 12.324193668365478 0.11067851185798645 12.565528964996338 7902271 0
Recovering previous policy with expected return of 65.81592039800995. Long term value was 28.545 and short term was 26.855.


Pure best response payoff estimated to be 71.09 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 94.54 seconds to finish estimate with resulting utilities: [139.66    4.015]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 160.94 seconds to finish estimate with resulting utilities: [111.195  46.66 ]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 165.56 seconds to finish estimate with resulting utilities: [70.705 46.38 ]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 163.9 seconds to finish estimate with resulting utilities: [96.44  51.425]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 165.79 seconds to finish estimate with resulting utilities: [61.565 61.595]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 166.26 seconds to finish estimate with resulting utilities: [62.59  62.335]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 166.75 seconds to finish estimate with resulting utilities: [59.825 63.535]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 163.98 seconds to finish estimate with resulting utilities: [59.535 64.585]
Computing meta_strategies
Exited RRD with total regret 7.478085069929946 that was less than regret lambda 7.500000000000002 after 62 iterations 
REGRET STEPS:  25
NEW LAMBDA 7.083333333333335
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    48.64           4.46           2.27           2.23           4.25           4.41           3.75           4.01      
    1    190.35          95.38          53.29          55.08          48.70          48.72          49.80          46.66      
    2    170.54          123.33          42.63          84.50          48.56          49.37          48.15          46.38      
    3    145.38          114.15          75.76          73.73          53.66          54.29          54.05          51.42      
    4    144.13          114.66          68.70          96.53          64.23          62.09          57.66          61.59      
    5    141.59          114.23          73.33          97.28          61.12          61.50          60.86          62.34      
    6    137.14          114.46          70.10          94.72          59.09          60.88          62.12          63.53      
    7    139.66          111.19          70.70          96.44          61.56          62.59          59.83          62.06      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    48.64          190.35          170.54          145.38          144.13          141.59          137.14          139.66      
    1     4.46          95.38          123.33          114.15          114.66          114.23          114.46          111.19      
    2     2.27          53.29          42.63          75.76          68.70          73.33          70.10          70.70      
    3     2.23          55.08          84.50          73.73          96.53          97.28          94.72          96.44      
    4     4.25          48.70          48.56          53.66          64.23          61.12          59.09          61.56      
    5     4.41          48.72          49.37          54.29          62.09          61.50          60.88          62.59      
    6     3.75          49.80          48.15          54.05          57.66          60.86          62.12          59.83      
    7     4.01          46.66          46.38          51.42          61.59          62.34          63.53          62.06      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0    97.28          194.81          172.81          147.61          148.38          146.00          140.89          143.67      
    1    194.81          190.76          176.62          169.23          163.36          162.95          164.26          157.85      
    2    172.81          176.62          85.26          160.26          117.27          122.70          118.25          117.09      
    3    147.61          169.23          160.26          147.47          150.19          151.56          148.76          147.87      
    4    148.38          163.36          117.27          150.19          128.47          123.20          116.75          123.16      
    5    146.00          162.95          122.70          151.56          123.20          123.00          121.75          124.93      
    6    140.89          164.26          118.25          148.76          116.75          121.75          124.25          123.36      
    7    143.67          157.85          117.09          147.87          123.16          124.93          123.36          124.12      

 

Metagame probabilities: 
Player #0: 0.002  0.0675  0.0897  0.1156  0.1812  0.1873  0.1777  0.179  
Player #1: 0.002  0.0675  0.0897  0.1156  0.1812  0.1873  0.1777  0.179  
Iteration : 7
Time so far: 69099.67691397667
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-07-19 08:37:35.989491: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15691 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007999462983570993 56.294525146484375 0.12371475771069526 11.80596170425415 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01677023749798536 15.154341888427734 0.35231301486492156 11.706250858306884 228681 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020296327583491803 20.133234596252443 0.33318790793418884 11.399764442443848 437630 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014399843569844962 17.870887660980223 0.2739559769630432 11.59695053100586 646287 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019009329471737148 15.064695358276367 0.335537052154541 11.040827083587647 854208 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016312239971011878 19.944396018981934 0.32522058486938477 11.637497043609619 1060517 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006978323473595083 21.337738609313966 0.12887629866600037 13.080414390563964 1270345 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013809610065072775 21.613681411743165 0.31482760310173036 12.294264888763427 1476752 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009220927068963648 12.536876392364501 0.2810444638133049 12.050737285614014 1685689 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008379301242530345 10.88783826828003 0.3068364769220352 11.819736576080322 1890921 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011649994179606438 15.232247734069825 0.3215338408946991 11.782244682312012 2096331 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007490722625516355 7.501327896118164 0.2379721835255623 12.913036823272705 2303509 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0035864354111254217 19.746545028686523 0.14969531670212746 12.276573848724365 2511433 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004623410385102033 18.31164903640747 0.29316782057285307 11.621052074432374 2716635 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005761151923798025 9.401586723327636 0.3324441075325012 11.343883991241455 2924770 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008910796139389277 13.405412483215333 0.3047420233488083 11.6149188041687 3131575 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00436027969699353 12.046076774597168 0.27065220326185224 11.587664127349854 3339421 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004570838180370629 10.734446334838868 0.30443947315216063 11.071466827392578 3544838 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016912519873585551 16.926734256744385 0.2369585692882538 11.825152778625489 3751598 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0024686552307684904 13.85293960571289 0.2881032764911652 11.78825330734253 3957809 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0033381522283889354 9.271309947967529 0.2847060441970825 12.166480827331544 4163439 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004837339173536748 18.816175079345705 0.16526333540678023 12.457845497131348 4367997 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016687261071638205 23.078388214111328 0.09846020191907882 11.917586135864259 4574502 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014664889184132334 9.489292812347411 0.13539882600307465 11.930766010284424 4779713 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012233406945597381 14.177617835998536 0.1305151641368866 12.017138862609864 4987687 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003942802955862135 10.61626968383789 0.2608705148100853 11.72770709991455 5192778 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0040205579367466274 15.102869606018066 0.29398508071899415 12.00799856185913 5399409 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0020035485154949128 15.491955661773682 0.21170369386672974 11.686957836151123 5603351 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0051253831945359705 13.743582057952882 0.18070242553949356 12.453368282318115 5807351 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0026918940886389462 9.697709274291991 0.17005375772714615 12.175214099884034 6012291 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005297502828761935 21.002934455871582 0.1162107452750206 13.309546184539794 6216238 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000782184697163757 10.443228435516357 0.13549402505159377 12.655856609344482 6422341 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003338926445576362 14.014873695373534 0.10234686955809594 12.395513534545898 6627662 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00385005775897298 31.582769775390624 0.05276759415864944 12.880322360992432 6832217 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0043965132208541036 19.41822280883789 0.09332416579127312 12.434660339355469 7036535 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021960116690024735 20.346681594848633 0.08451098129153252 12.568634223937988 7239843 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00684984956169501 21.23884677886963 0.08576437011361122 12.821497058868408 7443122 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0028261541243409737 17.566209697723387 0.06958684474229812 13.245086765289306 7648245 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008494375390000642 7.855992412567138 0.07528040558099747 13.572284317016601 7853339 0
Recovering previous policy with expected return of 69.70149253731343. Long term value was 21.542 and short term was 21.03.


Pure best response payoff estimated to be 69.73 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (8, 0)
Current player 0 and current strategies (8, 0) took 98.02 seconds to finish estimate with resulting utilities: [139.885   4.025]
Estimating current strategies:  (8, 1)
Current player 0 and current strategies (8, 1) took 171.75 seconds to finish estimate with resulting utilities: [112.46  47.77]
Estimating current strategies:  (8, 2)
Current player 0 and current strategies (8, 2) took 173.1 seconds to finish estimate with resulting utilities: [73.03  44.985]
Estimating current strategies:  (8, 3)
Current player 0 and current strategies (8, 3) took 175.21 seconds to finish estimate with resulting utilities: [98.175 51.92 ]
Estimating current strategies:  (8, 4)
Current player 0 and current strategies (8, 4) took 171.1 seconds to finish estimate with resulting utilities: [62.065 59.41 ]
Estimating current strategies:  (8, 5)
Current player 0 and current strategies (8, 5) took 166.52 seconds to finish estimate with resulting utilities: [64.225 61.56 ]
Estimating current strategies:  (8, 6)
Current player 0 and current strategies (8, 6) took 164.9 seconds to finish estimate with resulting utilities: [62.01  60.075]
Estimating current strategies:  (8, 7)
Current player 0 and current strategies (8, 7) took 164.79 seconds to finish estimate with resulting utilities: [65.075 61.425]
Estimating current strategies:  (8, 8)
Current player 0 and current strategies (8, 8) took 162.37 seconds to finish estimate with resulting utilities: [62.95 60.22]
Computing meta_strategies
Exited RRD with total regret 7.080774212428167 that was less than regret lambda 7.083333333333335 after 82 iterations 
REGRET STEPS:  25
NEW LAMBDA 6.666666666666668
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    48.64           4.46           2.27           2.23           4.25           4.41           3.75           4.01           4.03      
    1    190.35          95.38          53.29          55.08          48.70          48.72          49.80          46.66          47.77      
    2    170.54          123.33          42.63          84.50          48.56          49.37          48.15          46.38          44.98      
    3    145.38          114.15          75.76          73.73          53.66          54.29          54.05          51.42          51.92      
    4    144.13          114.66          68.70          96.53          64.23          62.09          57.66          61.59          59.41      
    5    141.59          114.23          73.33          97.28          61.12          61.50          60.86          62.34          61.56      
    6    137.14          114.46          70.10          94.72          59.09          60.88          62.12          63.53          60.08      
    7    139.66          111.19          70.70          96.44          61.56          62.59          59.83          62.06          61.42      
    8    139.88          112.46          73.03          98.17          62.06          64.22          62.01          65.08          61.59      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    48.64          190.35          170.54          145.38          144.13          141.59          137.14          139.66          139.88      
    1     4.46          95.38          123.33          114.15          114.66          114.23          114.46          111.19          112.46      
    2     2.27          53.29          42.63          75.76          68.70          73.33          70.10          70.70          73.03      
    3     2.23          55.08          84.50          73.73          96.53          97.28          94.72          96.44          98.17      
    4     4.25          48.70          48.56          53.66          64.23          61.12          59.09          61.56          62.06      
    5     4.41          48.72          49.37          54.29          62.09          61.50          60.88          62.59          64.22      
    6     3.75          49.80          48.15          54.05          57.66          60.86          62.12          59.83          62.01      
    7     4.01          46.66          46.38          51.42          61.59          62.34          63.53          62.06          65.08      
    8     4.03          47.77          44.98          51.92          59.41          61.56          60.08          61.42          61.59      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8      
    0    97.28          194.81          172.81          147.61          148.38          146.00          140.89          143.67          143.91      
    1    194.81          190.76          176.62          169.23          163.36          162.95          164.26          157.85          160.23      
    2    172.81          176.62          85.26          160.26          117.27          122.70          118.25          117.09          118.02      
    3    147.61          169.23          160.26          147.47          150.19          151.56          148.76          147.87          150.09      
    4    148.38          163.36          117.27          150.19          128.47          123.20          116.75          123.16          121.47      
    5    146.00          162.95          122.70          151.56          123.20          123.00          121.75          124.93          125.78      
    6    140.89          164.26          118.25          148.76          116.75          121.75          124.25          123.36          122.09      
    7    143.67          157.85          117.09          147.87          123.16          124.93          123.36          124.12          126.50      
    8    143.91          160.23          118.02          150.09          121.47          125.78          122.09          126.50          123.17      

 

Metagame probabilities: 
Player #0: 0.0005  0.044  0.0579  0.0846  0.1558  0.1657  0.1541  0.1579  0.1793  
Player #1: 0.0005  0.044  0.0579  0.0846  0.1558  0.1657  0.1541  0.1579  0.1793  
Iteration : 8
Time so far: 80372.3944401741
Approximating Best Response
Training best response:  True 0.06
rtg values check:  (?,) (?, 1)
2023-07-19 11:45:28.828153: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_62/weights_2/Adam_1/Assign' id:17669 op device:{requested: '', assigned: ''} def:{{{node mlp_62/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_62/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_62/weights_2/Adam_1, mlp_62/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006872908072546124 57.235162353515626 0.12725031226873398 11.3541690826416 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013188826944679021 15.01758337020874 0.27629464864730835 11.663021659851074 226055 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01690870989114046 17.398369789123535 0.3183287411928177 11.329540252685547 441519 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01386049371212721 11.361703014373779 0.29989972710609436 11.377149105072021 655614 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009404362365603446 17.38414192199707 0.21080434918403626 11.727760124206544 868790 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017064674850553276 12.979567050933838 0.3740706741809845 11.233906459808349 1081308 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017036785650998355 10.530009269714355 0.4471718192100525 10.262457180023194 1292624 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013581911660730838 13.277355861663818 0.35035226345062254 10.698064708709717 1503593 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007350223045796156 11.247830009460449 0.25803406834602355 10.944332313537597 1713253 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011816768161952495 8.958166313171386 0.3354217678308487 10.726260948181153 1921792 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0160388452000916 13.20607099533081 0.40086970925331117 10.812744903564454 2131536 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010708490246906876 12.62590732574463 0.5165109604597091 10.166994953155518 2340078 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007771685486659408 11.258999824523926 0.3654878467321396 10.894026184082032 2548944 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0077054962981492284 19.520584297180175 0.4054991781711578 10.307350063323975 2755663 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008323048194870352 25.07600383758545 0.33861058950424194 10.226331233978271 2963640 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005741767585277557 14.463373470306397 0.40726407468318937 11.1498628616333 3172642 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005527870473451912 11.253887939453126 0.44795469641685487 11.064210033416748 3381126 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021930532180704176 21.439457511901857 0.4305298745632172 10.33120698928833 3590559 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0028373862733133137 11.719794750213623 0.4423033565282822 10.091444110870361 3799324 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001285331286408109 13.127677536010742 0.16682401299476624 10.787260055541992 4010161 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004640562283748295 12.708753490447998 0.2685298308730125 10.638193035125733 4219458 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0028035575815010816 15.352554321289062 0.2949471116065979 10.608342838287353 4429305 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022663475374429254 15.094892311096192 0.18303890228271485 10.923311042785645 4636734 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015304971200748696 17.206614208221435 0.17516001909971238 10.99572811126709 4844303 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003398945551634824 13.014516162872315 0.23191650509834288 10.729964351654052 5052593 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005058151856064797 14.22734375 0.25286976993083954 11.34410400390625 5261874 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000553033364849398 12.813920211791991 0.1393519327044487 11.012987422943116 5471267 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015154499269556254 12.82471342086792 0.23942035138607026 11.770098304748535 5678397 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012232779452460818 11.328478717803955 0.2769804120063782 10.80375280380249 5886098 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006790461076889188 32.394456672668454 0.10395757481455803 11.502202796936036 6094056 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00041804776701610536 11.091056060791015 0.10894763842225075 11.625981330871582 6303568 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004770576325245202 12.991348838806152 0.19668170511722566 11.402161693572998 6511811 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010738762557593873 21.46926803588867 0.20981473475694656 11.462053394317627 6720021 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007835683034500107 15.042276668548585 0.11030754595994949 11.330213356018067 6927074 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016176036442629993 14.03560152053833 0.10800524801015854 11.30596046447754 7133979 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002493214386049658 19.287994956970216 0.08915598839521408 12.116812419891357 7342320 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002999869226187002 21.62720355987549 0.07023787833750247 11.839192867279053 7551973 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003196343849413097 12.485631656646728 0.16938071697950363 11.562873363494873 7761255 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012467488544643857 13.600048542022705 0.16241569072008133 11.371457767486572 7969064 0
Recovering previous policy with expected return of 64.59203980099502. Long term value was 53.616 and short term was 51.475.


Pure best response payoff estimated to be 67.98 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (9, 0)
Current player 0 and current strategies (9, 0) took 93.58 seconds to finish estimate with resulting utilities: [138.6     3.955]
Estimating current strategies:  (9, 1)
Current player 0 and current strategies (9, 1) took 164.73 seconds to finish estimate with resulting utilities: [113.835  49.595]
Estimating current strategies:  (9, 2)
Current player 0 and current strategies (9, 2) took 165.95 seconds to finish estimate with resulting utilities: [71.795 47.73 ]
Estimating current strategies:  (9, 3)
Current player 0 and current strategies (9, 3) took 165.14 seconds to finish estimate with resulting utilities: [95.545 51.845]
Estimating current strategies:  (9, 4)
Current player 0 and current strategies (9, 4) took 165.29 seconds to finish estimate with resulting utilities: [63.605 62.785]
Estimating current strategies:  (9, 5)
Current player 0 and current strategies (9, 5) took 165.5 seconds to finish estimate with resulting utilities: [61.815 60.92 ]
Estimating current strategies:  (9, 6)
Current player 0 and current strategies (9, 6) took 165.39 seconds to finish estimate with resulting utilities: [57.44 64.07]
Estimating current strategies:  (9, 7)
Current player 0 and current strategies (9, 7) took 166.27 seconds to finish estimate with resulting utilities: [65.015 62.205]
Estimating current strategies:  (9, 8)
Current player 0 and current strategies (9, 8) took 167.58 seconds to finish estimate with resulting utilities: [62.475 61.95 ]
Estimating current strategies:  (9, 9)
Current player 0 and current strategies (9, 9) took 165.24 seconds to finish estimate with resulting utilities: [63.63 62.38]
Computing meta_strategies
Exited RRD with total regret 6.611601240849865 that was less than regret lambda 6.666666666666668 after 67 iterations 
REGRET STEPS:  25
NEW LAMBDA 6.250000000000001
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    48.64           4.46           2.27           2.23           4.25           4.41           3.75           4.01           4.03           3.96      
    1    190.35          95.38          53.29          55.08          48.70          48.72          49.80          46.66          47.77          49.59      
    2    170.54          123.33          42.63          84.50          48.56          49.37          48.15          46.38          44.98          47.73      
    3    145.38          114.15          75.76          73.73          53.66          54.29          54.05          51.42          51.92          51.84      
    4    144.13          114.66          68.70          96.53          64.23          62.09          57.66          61.59          59.41          62.78      
    5    141.59          114.23          73.33          97.28          61.12          61.50          60.86          62.34          61.56          60.92      
    6    137.14          114.46          70.10          94.72          59.09          60.88          62.12          63.53          60.08          64.07      
    7    139.66          111.19          70.70          96.44          61.56          62.59          59.83          62.06          61.42          62.20      
    8    139.88          112.46          73.03          98.17          62.06          64.22          62.01          65.08          61.59          61.95      
    9    138.60          113.83          71.80          95.55          63.60          61.81          57.44          65.02          62.48          63.01      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    48.64          190.35          170.54          145.38          144.13          141.59          137.14          139.66          139.88          138.60      
    1     4.46          95.38          123.33          114.15          114.66          114.23          114.46          111.19          112.46          113.83      
    2     2.27          53.29          42.63          75.76          68.70          73.33          70.10          70.70          73.03          71.80      
    3     2.23          55.08          84.50          73.73          96.53          97.28          94.72          96.44          98.17          95.55      
    4     4.25          48.70          48.56          53.66          64.23          61.12          59.09          61.56          62.06          63.60      
    5     4.41          48.72          49.37          54.29          62.09          61.50          60.88          62.59          64.22          61.81      
    6     3.75          49.80          48.15          54.05          57.66          60.86          62.12          59.83          62.01          57.44      
    7     4.01          46.66          46.38          51.42          61.59          62.34          63.53          62.06          65.08          65.02      
    8     4.03          47.77          44.98          51.92          59.41          61.56          60.08          61.42          61.59          62.48      
    9     3.96          49.59          47.73          51.84          62.78          60.92          64.07          62.20          61.95          63.01      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    97.28          194.81          172.81          147.61          148.38          146.00          140.89          143.67          143.91          142.56      
    1    194.81          190.76          176.62          169.23          163.36          162.95          164.26          157.85          160.23          163.43      
    2    172.81          176.62          85.26          160.26          117.27          122.70          118.25          117.09          118.02          119.53      
    3    147.61          169.23          160.26          147.47          150.19          151.56          148.76          147.87          150.09          147.39      
    4    148.38          163.36          117.27          150.19          128.47          123.20          116.75          123.16          121.47          126.39      
    5    146.00          162.95          122.70          151.56          123.20          123.00          121.75          124.93          125.78          122.73      
    6    140.89          164.26          118.25          148.76          116.75          121.75          124.25          123.36          122.09          121.51      
    7    143.67          157.85          117.09          147.87          123.16          124.93          123.36          124.12          126.50          127.22      
    8    143.91          160.23          118.02          150.09          121.47          125.78          122.09          126.50          123.17          124.43      
    9    142.56          163.43          119.53          147.39          126.39          122.73          121.51          127.22          124.43          126.01      

 

Metagame probabilities: 
Player #0: 0.0013  0.0481  0.0583  0.0783  0.1319  0.1357  0.132  0.1321  0.1445  0.1379  
Player #1: 0.0013  0.0481  0.0583  0.0783  0.1319  0.1357  0.132  0.1321  0.1445  0.1379  
Iteration : 9
Time so far: 91728.73405575752
Approximating Best Response
Training best response:  True 0.05500000000000001
rtg values check:  (?,) (?, 1)
2023-07-19 14:54:45.295918: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_69/weights_2/Adam_1/Assign' id:19647 op device:{requested: '', assigned: ''} def:{{{node mlp_69/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_69/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_69/weights_2/Adam_1, mlp_69/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005887344758957624 33.90213451385498 0.08513744845986367 12.71957712173462 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020837483555078508 15.424331378936767 0.38080201745033265 11.813294410705566 223789 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015886853728443385 14.079366111755371 0.32256737649440764 12.042636489868164 433295 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011965468153357505 17.76949806213379 0.2881010204553604 12.108273220062255 642308 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01258352492004633 11.969603633880615 0.26547762006521225 12.28953504562378 849054 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011958676716312766 12.501833820343018 0.2985375314950943 12.275798320770264 1055395 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011950044613331556 14.314862728118896 0.3092967301607132 12.04644947052002 1262224 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011732276342809201 14.916543006896973 0.32076792418956757 11.627720832824707 1472431 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013019001204520464 24.887978935241698 0.30153884291648864 11.944725704193115 1681405 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013714795932173729 20.504643630981445 0.3482776373624802 11.637426471710205 1891034 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00977703258395195 12.179204082489013 0.3226861894130707 12.14572877883911 2097266 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009850717103108763 21.338420867919922 0.2535692468285561 11.970817184448242 2305310 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0036950453650206327 17.04801368713379 0.16599339246749878 12.03790111541748 2514325 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009159029554575681 27.16272888183594 0.2860653728246689 11.52835693359375 2721251 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008692114474251866 25.558344650268555 0.3243545740842819 11.645526504516601 2929837 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005909941182471812 11.746883487701416 0.4191944569349289 11.630494499206543 3137709 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0030681953998282553 12.25745086669922 0.3741522878408432 11.75942325592041 3345612 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00430042363004759 13.781348419189452 0.315382319688797 12.023336124420165 3551964 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004198862228076905 13.792417049407959 0.31423417031764983 11.93494529724121 3756809 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002669765881728381 14.046115684509278 0.29786127209663393 11.350457096099854 3963699 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0025077122008951847 17.78544416427612 0.1763557717204094 12.435389804840089 4169776 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002549693750916049 16.554042434692384 0.2148517608642578 12.247434997558594 4377277 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006224445716725314 15.993392372131348 0.15239739865064622 12.795637130737305 4584045 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00014130457584542454 14.363996887207032 0.11595824211835862 12.49654712677002 4787982 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016789887275081127 18.619149494171143 0.17481424957513808 12.382101917266846 4992787 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004199274443089962 7.864245319366455 0.14414083138108252 12.76939640045166 5197102 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0006791607418563216 10.110211753845215 0.12583692297339438 12.578338432312012 5402944 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00016460298138554208 15.775335216522217 0.1016142226755619 12.618248653411865 5607431 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003349417649587849 13.242799186706543 0.10023317784070969 12.425573062896728 5812248 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004721109766978771 6.691783618927002 0.13256504759192467 12.03470344543457 6016138 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016504736777278595 18.989892768859864 0.1114547535777092 12.73198823928833 6221700 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001964777649845928 10.14517936706543 0.09925624281167984 12.521893215179443 6426013 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003650428494438529 11.662623596191406 0.10096659585833549 12.464538097381592 6629054 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003449656371958554 11.388396644592286 0.07794298604130745 12.724098587036133 6832984 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022058066409954336 8.159831714630126 0.11610418930649757 12.365288162231446 7039422 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004662286392704118 24.861986923217774 0.09955846667289733 12.514692974090575 7243650 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000947537391766673 10.708238172531129 0.10872523486614227 12.784311103820801 7447847 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005297936347778887 9.04394235610962 0.09385250583291053 13.12851972579956 7653070 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011444462405052036 16.241011714935304 0.07475832551717758 13.145362377166748 7856904 0
Recovering previous policy with expected return of 62.66169154228856. Long term value was 20.858 and short term was 21.55.


Pure best response payoff estimated to be 64.685 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (10, 0)
Current player 0 and current strategies (10, 0) took 90.14 seconds to finish estimate with resulting utilities: [135.515   3.725]
Estimating current strategies:  (10, 1)
Current player 0 and current strategies (10, 1) took 164.09 seconds to finish estimate with resulting utilities: [115.45   49.775]
Estimating current strategies:  (10, 2)
Current player 0 and current strategies (10, 2) took 166.23 seconds to finish estimate with resulting utilities: [73.38  50.015]
Estimating current strategies:  (10, 3)
Current player 0 and current strategies (10, 3) took 164.0 seconds to finish estimate with resulting utilities: [99.995 51.1  ]
Estimating current strategies:  (10, 4)
Current player 0 and current strategies (10, 4) took 166.23 seconds to finish estimate with resulting utilities: [61.165 62.49 ]
Estimating current strategies:  (10, 5)
Current player 0 and current strategies (10, 5) took 163.35 seconds to finish estimate with resulting utilities: [59.085 59.265]
Estimating current strategies:  (10, 6)
Current player 0 and current strategies (10, 6) took 165.83 seconds to finish estimate with resulting utilities: [63.125 62.945]
Estimating current strategies:  (10, 7)
Current player 0 and current strategies (10, 7) took 165.9 seconds to finish estimate with resulting utilities: [62.835 65.515]
Estimating current strategies:  (10, 8)
Current player 0 and current strategies (10, 8) took 166.39 seconds to finish estimate with resulting utilities: [60.405 62.34 ]
Estimating current strategies:  (10, 9)
Current player 0 and current strategies (10, 9) took 165.35 seconds to finish estimate with resulting utilities: [60.89 61.21]
Estimating current strategies:  (10, 10)
Current player 0 and current strategies (10, 10) took 164.33 seconds to finish estimate with resulting utilities: [60.65  64.525]
Computing meta_strategies
Exited RRD with total regret 6.1922973743739504 that was less than regret lambda 6.250000000000001 after 60 iterations 
REGRET STEPS:  25
NEW LAMBDA 5.833333333333334
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0    48.64           4.46           2.27           2.23           4.25           4.41           3.75           4.01           4.03           3.96           3.73      
    1    190.35          95.38          53.29          55.08          48.70          48.72          49.80          46.66          47.77          49.59          49.77      
    2    170.54          123.33          42.63          84.50          48.56          49.37          48.15          46.38          44.98          47.73          50.02      
    3    145.38          114.15          75.76          73.73          53.66          54.29          54.05          51.42          51.92          51.84          51.10      
    4    144.13          114.66          68.70          96.53          64.23          62.09          57.66          61.59          59.41          62.78          62.49      
    5    141.59          114.23          73.33          97.28          61.12          61.50          60.86          62.34          61.56          60.92          59.27      
    6    137.14          114.46          70.10          94.72          59.09          60.88          62.12          63.53          60.08          64.07          62.95      
    7    139.66          111.19          70.70          96.44          61.56          62.59          59.83          62.06          61.42          62.20          65.52      
    8    139.88          112.46          73.03          98.17          62.06          64.22          62.01          65.08          61.59          61.95          62.34      
    9    138.60          113.83          71.80          95.55          63.60          61.81          57.44          65.02          62.48          63.01          61.21      
   10    135.51          115.45          73.38          100.00          61.16          59.09          63.12          62.84          60.41          60.89          62.59      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0    48.64          190.35          170.54          145.38          144.13          141.59          137.14          139.66          139.88          138.60          135.51      
    1     4.46          95.38          123.33          114.15          114.66          114.23          114.46          111.19          112.46          113.83          115.45      
    2     2.27          53.29          42.63          75.76          68.70          73.33          70.10          70.70          73.03          71.80          73.38      
    3     2.23          55.08          84.50          73.73          96.53          97.28          94.72          96.44          98.17          95.55          100.00      
    4     4.25          48.70          48.56          53.66          64.23          61.12          59.09          61.56          62.06          63.60          61.16      
    5     4.41          48.72          49.37          54.29          62.09          61.50          60.88          62.59          64.22          61.81          59.09      
    6     3.75          49.80          48.15          54.05          57.66          60.86          62.12          59.83          62.01          57.44          63.12      
    7     4.01          46.66          46.38          51.42          61.59          62.34          63.53          62.06          65.08          65.02          62.84      
    8     4.03          47.77          44.98          51.92          59.41          61.56          60.08          61.42          61.59          62.48          60.41      
    9     3.96          49.59          47.73          51.84          62.78          60.92          64.07          62.20          61.95          63.01          60.89      
   10     3.73          49.77          50.02          51.10          62.49          59.27          62.95          65.52          62.34          61.21          62.59      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0    97.28          194.81          172.81          147.61          148.38          146.00          140.89          143.67          143.91          142.56          139.24      
    1    194.81          190.76          176.62          169.23          163.36          162.95          164.26          157.85          160.23          163.43          165.22      
    2    172.81          176.62          85.26          160.26          117.27          122.70          118.25          117.09          118.02          119.53          123.39      
    3    147.61          169.23          160.26          147.47          150.19          151.56          148.76          147.87          150.09          147.39          151.09      
    4    148.38          163.36          117.27          150.19          128.47          123.20          116.75          123.16          121.47          126.39          123.66      
    5    146.00          162.95          122.70          151.56          123.20          123.00          121.75          124.93          125.78          122.73          118.35      
    6    140.89          164.26          118.25          148.76          116.75          121.75          124.25          123.36          122.09          121.51          126.07      
    7    143.67          157.85          117.09          147.87          123.16          124.93          123.36          124.12          126.50          127.22          128.35      
    8    143.91          160.23          118.02          150.09          121.47          125.78          122.09          126.50          123.17          124.43          122.75      
    9    142.56          163.43          119.53          147.39          126.39          122.73          121.51          127.22          124.43          126.01          122.10      
   10    139.24          165.22          123.39          151.09          123.66          118.35          126.07          128.35          122.75          122.10          125.18      

 

Metagame probabilities: 
Player #0: 0.0018  0.0476  0.0557  0.0708  0.1152  0.1155  0.1155  0.1175  0.1237  0.1182  0.1186  
Player #1: 0.0018  0.0476  0.0557  0.0708  0.1152  0.1155  0.1155  0.1175  0.1237  0.1182  0.1186  
Iteration : 10
Time so far: 103112.84633421898
Approximating Best Response
Training best response:  True 0.05
rtg values check:  (?,) (?, 1)
2023-07-19 18:04:29.554016: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_76/weights_2/Adam_1/Assign' id:21625 op device:{requested: '', assigned: ''} def:{{{node mlp_76/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_76/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_76/weights_2/Adam_1, mlp_76/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007638208474963904 57.443853759765624 0.10549870356917382 12.058310985565186 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015275792405009269 12.197581481933593 0.2898991763591766 11.676400947570801 227407 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01472926689311862 10.786154079437257 0.31966218650341033 11.140697002410889 443322 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016021532751619817 13.112576580047607 0.27842229455709455 11.682124710083007 658657 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01500585414469242 14.046903991699219 0.3074079155921936 11.115044116973877 873226 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019932369608432053 15.26639699935913 0.4443275988101959 10.483271408081055 1086959 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01718548433855176 13.104515838623048 0.4189000576734543 10.981058025360108 1296987 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01492974180728197 14.792727088928222 0.43701411485672 10.720363807678222 1509172 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010129239736124873 10.917934322357178 0.3515858381986618 10.942728519439697 1717730 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008768048649653793 12.414907073974609 0.22683936059474946 11.176658153533936 1926053 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009076186222955585 11.303786945343017 0.3496589720249176 11.482682800292968 2133577 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01299770325422287 11.654890441894532 0.476929122209549 10.840531158447266 2343451 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010855938121676446 12.245483875274658 0.5001128435134887 10.487621784210205 2551473 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00850278625730425 12.405123424530029 0.46573088467121126 11.076444339752197 2760369 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0076355447992682455 16.49299669265747 0.37896055579185484 11.035999393463134 2968326 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0034642890328541397 15.060087585449219 0.3015180677175522 11.209001064300537 3175105 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001869269530288875 18.779339218139647 0.1489380829036236 11.91970739364624 3384097 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00808094539679587 24.559292793273926 0.3087454855442047 11.112608528137207 3590861 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002693486958742142 16.196625423431396 0.16828245520591736 10.984222316741944 3798531 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009158920001937076 15.820079231262207 0.12567452415823938 11.471135139465332 4006667 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003895259532146156 14.499047470092773 0.27780517637729646 11.831714534759522 4214627 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006603186384381843 9.452824878692628 0.30520165264606475 11.622272491455078 4422020 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00020784328662557528 14.629164505004884 0.09049445912241935 11.87750883102417 4630030 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006779154180549085 10.932904243469238 0.4194036692380905 11.67874460220337 4837123 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002553271234501153 10.708950233459472 0.2181531935930252 11.532801151275635 5043548 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011951001543366146 13.513139247894287 0.3478291451931 11.724890995025635 5252249 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003481461969204247 10.691995906829835 0.37828377485275266 11.645755195617676 5460304 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0004122348211240023 13.48734483718872 0.3542956322431564 11.559311676025391 5668708 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007129974226700142 11.080481243133544 0.3448392629623413 11.638857650756837 5876422 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018424853959004394 18.43961181640625 0.36885386407375337 12.327843284606933 6082384 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004290845210198313 14.55806131362915 0.317206072807312 11.608705997467041 6291016 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005262611521175131 13.718362712860108 0.33658632934093474 11.974070167541504 6496758 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008229876606492325 18.184370231628417 0.2613818600773811 12.110240268707276 6703748 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0036852221390290653 10.360048866271972 0.29741195440292356 12.18911533355713 6910047 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00154960606014356 13.943223667144775 0.23199381083250045 12.37280797958374 7118657 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0026682948227971792 13.910296821594239 0.23190859109163284 12.5575740814209 7323809 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0008644749621453229 10.40107307434082 0.2720513254404068 12.928873062133789 7529906 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0024521792380255646 14.217695713043213 0.26463631838560103 12.692736721038818 7736012 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018173307544202544 23.271357154846193 0.23394200205802917 12.64006748199463 7943299 0
Recovering previous policy with expected return of 63.3681592039801. Long term value was 40.412 and short term was 41.155.


Pure best response payoff estimated to be 70.49 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (11, 0)
Current player 0 and current strategies (11, 0) took 116.35 seconds to finish estimate with resulting utilities: [139.88   3.96]
Estimating current strategies:  (11, 1)
Current player 0 and current strategies (11, 1) took 204.29 seconds to finish estimate with resulting utilities: [113.095  47.92 ]
Estimating current strategies:  (11, 2)
Current player 0 and current strategies (11, 2) took 207.76 seconds to finish estimate with resulting utilities: [69.495 50.45 ]
Estimating current strategies:  (11, 3)
Current player 0 and current strategies (11, 3) took 205.08 seconds to finish estimate with resulting utilities: [98.005 49.285]
Estimating current strategies:  (11, 4)
Current player 0 and current strategies (11, 4) took 208.55 seconds to finish estimate with resulting utilities: [62.245 62.21 ]
Estimating current strategies:  (11, 5)
Current player 0 and current strategies (11, 5) took 210.45 seconds to finish estimate with resulting utilities: [61.795 64.61 ]
Estimating current strategies:  (11, 6)
Current player 0 and current strategies (11, 6) took 208.67 seconds to finish estimate with resulting utilities: [58.12 60.07]
Estimating current strategies:  (11, 7)
Current player 0 and current strategies (11, 7) took 208.77 seconds to finish estimate with resulting utilities: [61.475 62.34 ]
Estimating current strategies:  (11, 8)
Current player 0 and current strategies (11, 8) took 210.33 seconds to finish estimate with resulting utilities: [58.94  59.685]
Estimating current strategies:  (11, 9)
Current player 0 and current strategies (11, 9) took 211.56 seconds to finish estimate with resulting utilities: [62.75 64.34]
Estimating current strategies:  (11, 10)
Current player 0 and current strategies (11, 10) took 209.65 seconds to finish estimate with resulting utilities: [63.17 60.86]
Estimating current strategies:  (11, 11)
Current player 0 and current strategies (11, 11) took 203.28 seconds to finish estimate with resulting utilities: [64.34 64.19]
Computing meta_strategies
Exited RRD with total regret 5.812317777855355 that was less than regret lambda 5.833333333333334 after 49 iterations 
REGRET STEPS:  25
NEW LAMBDA 5.416666666666667
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11      
    0    48.64           4.46           2.27           2.23           4.25           4.41           3.75           4.01           4.03           3.96           3.73           3.96      
    1    190.35          95.38          53.29          55.08          48.70          48.72          49.80          46.66          47.77          49.59          49.77          47.92      
    2    170.54          123.33          42.63          84.50          48.56          49.37          48.15          46.38          44.98          47.73          50.02          50.45      
    3    145.38          114.15          75.76          73.73          53.66          54.29          54.05          51.42          51.92          51.84          51.10          49.28      
    4    144.13          114.66          68.70          96.53          64.23          62.09          57.66          61.59          59.41          62.78          62.49          62.21      
    5    141.59          114.23          73.33          97.28          61.12          61.50          60.86          62.34          61.56          60.92          59.27          64.61      
    6    137.14          114.46          70.10          94.72          59.09          60.88          62.12          63.53          60.08          64.07          62.95          60.07      
    7    139.66          111.19          70.70          96.44          61.56          62.59          59.83          62.06          61.42          62.20          65.52          62.34      
    8    139.88          112.46          73.03          98.17          62.06          64.22          62.01          65.08          61.59          61.95          62.34          59.69      
    9    138.60          113.83          71.80          95.55          63.60          61.81          57.44          65.02          62.48          63.01          61.21          64.34      
   10    135.51          115.45          73.38          100.00          61.16          59.09          63.12          62.84          60.41          60.89          62.59          60.86      
   11    139.88          113.09          69.50          98.00          62.24          61.80          58.12          61.48          58.94          62.75          63.17          64.27      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11      
    0    48.64          190.35          170.54          145.38          144.13          141.59          137.14          139.66          139.88          138.60          135.51          139.88      
    1     4.46          95.38          123.33          114.15          114.66          114.23          114.46          111.19          112.46          113.83          115.45          113.09      
    2     2.27          53.29          42.63          75.76          68.70          73.33          70.10          70.70          73.03          71.80          73.38          69.50      
    3     2.23          55.08          84.50          73.73          96.53          97.28          94.72          96.44          98.17          95.55          100.00          98.00      
    4     4.25          48.70          48.56          53.66          64.23          61.12          59.09          61.56          62.06          63.60          61.16          62.24      
    5     4.41          48.72          49.37          54.29          62.09          61.50          60.88          62.59          64.22          61.81          59.09          61.80      
    6     3.75          49.80          48.15          54.05          57.66          60.86          62.12          59.83          62.01          57.44          63.12          58.12      
    7     4.01          46.66          46.38          51.42          61.59          62.34          63.53          62.06          65.08          65.02          62.84          61.48      
    8     4.03          47.77          44.98          51.92          59.41          61.56          60.08          61.42          61.59          62.48          60.41          58.94      
    9     3.96          49.59          47.73          51.84          62.78          60.92          64.07          62.20          61.95          63.01          60.89          62.75      
   10     3.73          49.77          50.02          51.10          62.49          59.27          62.95          65.52          62.34          61.21          62.59          63.17      
   11     3.96          47.92          50.45          49.28          62.21          64.61          60.07          62.34          59.69          64.34          60.86          64.27      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11      
    0    97.28          194.81          172.81          147.61          148.38          146.00          140.89          143.67          143.91          142.56          139.24          143.84      
    1    194.81          190.76          176.62          169.23          163.36          162.95          164.26          157.85          160.23          163.43          165.22          161.01      
    2    172.81          176.62          85.26          160.26          117.27          122.70          118.25          117.09          118.02          119.53          123.39          119.95      
    3    147.61          169.23          160.26          147.47          150.19          151.56          148.76          147.87          150.09          147.39          151.09          147.29      
    4    148.38          163.36          117.27          150.19          128.47          123.20          116.75          123.16          121.47          126.39          123.66          124.45      
    5    146.00          162.95          122.70          151.56          123.20          123.00          121.75          124.93          125.78          122.73          118.35          126.41      
    6    140.89          164.26          118.25          148.76          116.75          121.75          124.25          123.36          122.09          121.51          126.07          118.19      
    7    143.67          157.85          117.09          147.87          123.16          124.93          123.36          124.12          126.50          127.22          128.35          123.81      
    8    143.91          160.23          118.02          150.09          121.47          125.78          122.09          126.50          123.17          124.43          122.75          118.62      
    9    142.56          163.43          119.53          147.39          126.39          122.73          121.51          127.22          124.43          126.01          122.10          127.09      
   10    139.24          165.22          123.39          151.09          123.66          118.35          126.07          128.35          122.75          122.10          125.18          124.03      
   11    143.84          161.01          119.95          147.29          124.45          126.41          118.19          123.81          118.62          127.09          124.03          128.53      

 

Metagame probabilities: 
Player #0: 0.0035  0.0498  0.0567  0.067  0.1016  0.103  0.1006  0.103  0.1057  0.1045  0.1031  0.1015  
Player #1: 0.0035  0.0498  0.0567  0.067  0.1016  0.103  0.1006  0.103  0.1057  0.1045  0.1031  0.1015  
Iteration : 11
Time so far: 115994.35349440575
Approximating Best Response
Training best response:  True 0.045
rtg values check:  (?,) (?, 1)
2023-07-19 21:39:11.504510: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_83/weights_2/Adam_1/Assign' id:23603 op device:{requested: '', assigned: ''} def:{{{node mlp_83/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_83/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_83/weights_2/Adam_1, mlp_83/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007189058256335557 48.555935287475585 0.0992469035089016 12.446012210845947 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014810107368975877 10.06925344467163 0.30613021850585936 11.62714786529541 226492 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01634454345330596 14.03420705795288 0.2925520032644272 11.754351139068604 435451 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014513337053358555 13.830461311340333 0.33906832039356233 11.412091255187988 643991 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014769674185663462 11.779475688934326 0.3607727110385895 11.424699974060058 851738 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012129282299429178 13.756348800659179 0.28799463212490084 11.630297756195068 1059410 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018266451358795167 10.61002140045166 0.3068308800458908 11.607120418548584 1267525 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013054403197020292 10.191151905059815 0.28699041306972506 11.539316749572754 1475609 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010907985921949147 13.795679759979247 0.27915998697280886 11.665417766571045 1683268 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012320221308618784 12.347314929962158 0.27481509894132616 11.188971900939942 1892634 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009377507981844247 13.413048267364502 0.2776518940925598 10.984314918518066 2100338 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008841123059391975 15.011804008483887 0.2280544713139534 11.73830795288086 2308823 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007231746055185795 12.277345943450928 0.2271311402320862 11.59012279510498 2518929 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006717464048415422 12.653588199615479 0.2512248769402504 11.490848636627197 2727648 0
slurmstepd: error: *** JOB 56042731 ON gl3401 CANCELLED AT 2023-07-19T22:50:25 ***
