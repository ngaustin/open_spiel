Job Id listed below:
55931177

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

2023-07-16 19:45:42.556037: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-16 19:45:43.638311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0716 19:45:45.370773 22751167347584 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x14b0e2002d10>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x14b0e2002d10>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-07-16 19:45:45.708000: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-07-16 19:45:46.020503: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.79 seconds to finish estimate with resulting utilities: [48.31  47.395]
Exited RRD with total regret 0.0 that was less than regret lambda 2.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    47.85      

 

Player 1 Payoff matrix: 

           0      
    0    47.85      

 

Social Welfare Sum Matrix: 

           0      
    0    95.71      

 

Iteration : 0
Time so far: 0.00020599365234375
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-07-16 19:46:05.874474: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.11960039213299752 23.27602062225342 2.054012084007263 0.0008314390346640721 10472 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09523188099265098 15.065438747406006 1.8771568655967712 0.17887272983789443 215332 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09602410495281219 16.430826282501222 1.8662182688713074 0.2298605740070343 417282 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08294852823019028 20.938980293273925 1.8151420474052429 0.32557309567928316 620060 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07606766074895858 15.46519432067871 1.7780661940574647 0.39589382112026217 822864 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07480662539601327 17.364328956604005 1.775148832798004 0.44884254932403567 1026262 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06757407002151013 16.09404935836792 1.7695684552192688 0.5002549111843109 1228958 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06960564255714416 15.020450115203857 1.7656631350517273 0.5273572593927384 1431857 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05810367465019226 13.953621864318848 1.729935896396637 0.6553962349891662 1634896 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.046869324147701265 21.401441764831542 1.5949859976768495 0.8523251354694367 1838793 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04005284383893013 19.94773426055908 1.5801872730255127 0.9179687023162841 2044190 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.033562888577580455 21.34999370574951 1.5002602100372315 1.1338797688484192 2252773 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03131313379853964 22.914504051208496 1.457109808921814 1.2859204053878783 2461030 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02554463054984808 22.844482803344725 1.4174786686897278 1.34853492975235 2670558 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021962103247642518 23.776126861572266 1.3673464894294738 1.5153642892837524 2880634 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01604686276987195 19.5018949508667 1.2748559832572937 1.7385752081871033 3089966 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012202909588813782 28.31294765472412 1.2348063349723817 1.8674645900726319 3300196 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007586445193737745 19.686231803894042 1.156624710559845 2.096757686138153 3511509 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005916929384693503 25.93417911529541 1.0934733152389526 2.368328905105591 3725023 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0026473242149222644 22.954378128051758 1.0534700870513916 2.5460703134536744 3937710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006491437758086249 25.527391052246095 0.961518532037735 2.8720515966415405 4151157 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022175093356054274 23.708265686035155 0.8795913100242615 3.2382526636123656 4367471 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00154528776765801 24.333056449890137 0.8696273267269135 3.425288200378418 4581502 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007567317810753593 29.029218673706055 0.7466197848320008 3.8414215326309202 4795879 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017592621967196464 21.627396488189696 0.711149948835373 4.111770963668823 5009601 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00028122771764174106 28.07979850769043 0.6941856682300568 4.256037998199463 5226066 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00043861565936822446 26.94938259124756 0.6423420906066895 4.635968494415283 5445088 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001989094878081232 25.289983177185057 0.5853299379348755 4.975666141510009 5661537 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009167914569843561 21.242906188964845 0.5513288021087647 5.1424400806427 5880147 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005623218596156221 18.656605052948 0.5115682423114777 5.331607627868652 6098864 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001563588483259082 28.026080322265624 0.4959707498550415 5.5499755382537845 6316990 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013753937489127566 29.536074829101562 0.4372912377119064 6.032901954650879 6535655 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00035649794808705336 20.66682529449463 0.45186434090137484 6.108572149276734 6753792 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007432967599015683 22.645119285583498 0.45412151515483856 6.279284381866455 6971082 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  7.40208852221258e-06 21.97101535797119 0.44729589819908144 6.5295545101165775 7190675 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001319385327224154 24.813507080078125 0.45914103388786315 6.379008197784424 7408886 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005361535449992516 19.46995277404785 0.456124347448349 6.605588674545288 7625367 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016041387265431696 24.522615242004395 0.4318453520536423 6.470028591156006 7844233 0


Pure best response payoff estimated to be 194.3 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 85.98 seconds to finish estimate with resulting utilities: [189.555   3.83 ]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 141.14 seconds to finish estimate with resulting utilities: [96.72 96.19]
Computing meta_strategies
Exited RRD with total regret 1.9456066114126997 that was less than regret lambda 2.0 after 43 iterations 
REGRET STEPS:  25
NEW LAMBDA 1.9166666666666667
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    47.85           3.83      
    1    189.56          96.45      

 

Player 1 Payoff matrix: 

           0              1      
    0    47.85          189.56      
    1     3.83          96.45      

 

Social Welfare Sum Matrix: 

           0              1      
    0    95.71          193.39      
    1    193.39          192.91      

 

Metagame probabilities: 
Player #0: 0.0104  0.9896  
Player #1: 0.0104  0.9896  
Iteration : 1
Time so far: 7357.521688222885
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-07-16 21:48:43.554975: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02923425752669573 87.70132980346679 0.5744329899549484 8.858610248565673 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029786696843802928 16.601955127716064 0.5980731785297394 8.298160362243653 230768 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03242120463401079 16.872368240356444 0.7038324058055878 7.433540153503418 446999 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029230052046477793 20.528240203857422 0.6737015962600708 6.904740810394287 666104 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.033370078541338444 16.166740131378173 0.8163752913475036 6.60965404510498 885531 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03179961573332548 17.601758766174317 0.8460666418075562 6.38025426864624 1104118 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030932420492172243 21.853422927856446 0.8759490489959717 5.995591878890991 1320982 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02687795925885439 25.306624984741212 0.8538856565952301 6.475532674789429 1536205 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026218427531421184 21.276599884033203 0.8667441964149475 5.960849905014038 1753042 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021650876104831695 17.175162029266357 0.7828688323497772 6.9858824729919435 1967315 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021981513313949107 18.96512928009033 0.8935239493846894 6.593523550033569 2182322 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016870795469731092 21.454693412780763 0.7987545192241668 6.72110013961792 2396940 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014929230138659478 22.533051872253417 0.7879948258399964 6.597476291656494 2612135 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013218340091407299 18.207687664031983 0.7944267690181732 6.616506671905517 2827422 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009547387529164553 17.648290729522706 0.7823190271854401 6.697622919082642 3042392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007928091753274202 24.774453735351564 0.7656169772148133 6.765246152877808 3254668 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006022978201508522 19.51251630783081 0.7825496137142182 6.663279867172241 3470307 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003813969320617616 19.82314987182617 0.6977147221565246 7.108619594573975 3682380 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002156107878545299 20.70771541595459 0.7860342383384704 6.997990703582763 3898220 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00014876864224788733 20.790581512451173 0.5983847737312317 7.604339170455932 4116075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000538252794649452 22.53025894165039 0.6020386576652527 7.436122751235962 4331810 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001772999801323749 17.950215911865236 0.5503450572490692 7.754988384246826 4547035 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002447449800092727 20.978990173339845 0.48338755667209626 7.788125610351562 4764578 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  8.490926120430231e-06 19.44163246154785 0.4714718610048294 8.15717887878418 4980986 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006083568645408377 16.35092067718506 0.4607373237609863 7.805271148681641 5198457 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011888389901287156 23.399934387207033 0.49541043043136596 8.300671052932739 5414079 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008632512865005993 20.112372207641602 0.5113826274871827 8.20871057510376 5632619 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  8.492612541886047e-05 20.33077220916748 0.4757013440132141 8.504611587524414 5848727 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007122773444280028 12.795612621307374 0.4729289174079895 8.885644435882568 6065038 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012184446008177475 20.23794231414795 0.5014749675989151 9.124267959594727 6281442 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011183916474692523 23.82568073272705 0.4137299358844757 8.809187126159667 6497610 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009297695680288598 19.758306884765624 0.4245604038238525 9.124677371978759 6717610 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00014676860882900655 22.93873462677002 0.3898904174566269 9.451661014556885 6936600 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00027002887727576306 18.464121437072755 0.3043591111898422 9.267510318756104 7152807 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -5.0556001951918006e-05 18.008640098571778 0.33075317442417146 9.282756805419922 7371140 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009516448859358207 25.518552589416505 0.3531149834394455 9.426499080657958 7590153 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00114271825295873 21.3281156539917 0.41674394309520724 9.854964447021484 7807730 0


Pure best response payoff estimated to be 127.53 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 80.4 seconds to finish estimate with resulting utilities: [165.81    2.255]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 141.44 seconds to finish estimate with resulting utilities: [125.72  53.25]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 138.78 seconds to finish estimate with resulting utilities: [45.445 48.13 ]
Computing meta_strategies
Exited RRD with total regret 1.9052239910005255 that was less than regret lambda 1.9166666666666667 after 162 iterations 
REGRET STEPS:  25
NEW LAMBDA 1.8333333333333335
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    47.85           3.83           2.25      
    1    189.56          96.45          53.25      
    2    165.81          125.72          46.79      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    47.85          189.56          165.81      
    1     3.83          96.45          125.72      
    2     2.25          53.25          46.79      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    95.71          193.39          168.06      
    1    193.39          192.91          178.97      
    2    168.06          178.97          93.58      

 

Metagame probabilities: 
Player #0: 0.0001  0.2766  0.7233  
Player #1: 0.0001  0.2766  0.7233  
Iteration : 2
Time so far: 17108.816730737686
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-07-17 00:31:15.035286: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011492609046399593 39.43402614593506 0.24609774053096772 11.5319242477417 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031631738506257535 23.23967742919922 0.6405780017375946 8.076677465438843 228329 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02879490554332733 14.96648817062378 0.6281451046466827 8.451858615875244 441517 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026389451511204243 17.22636651992798 0.6043961763381958 8.415585136413574 653953 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024919948354363442 19.40900573730469 0.6066419661045075 8.472580432891846 864586 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026545333862304687 18.06874074935913 0.695955753326416 7.939181709289551 1078795 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022973363660275935 18.588178443908692 0.6447690784931183 8.270244312286376 1287717 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02223964184522629 12.720695114135742 0.6919564008712769 8.176098489761353 1497660 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022158006951212884 20.517616271972656 0.7194490253925323 7.780678796768188 1709522 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019180075637996195 15.002024459838868 0.6815934300422668 7.811260986328125 1920667 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015632645413279533 15.44754753112793 0.6192083179950714 8.508011627197266 2128746 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013239796925336122 18.312374305725097 0.6368081569671631 8.184484148025513 2340485 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013453642651438714 15.52253532409668 0.6817404389381408 7.948683404922486 2554900 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010236497037112713 25.697896575927736 0.5659601390361786 8.185602140426635 2769155 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009741157852113247 19.466406440734865 0.6946770370006561 7.653589534759521 2981041 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007683498552069068 20.612078285217287 0.5803765654563904 8.409357404708862 3194959 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00480398831423372 21.36394805908203 0.5464635819196701 8.859542083740234 3406946 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0031181023106910287 20.35533046722412 0.5619056522846222 8.359918689727783 3620457 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0028479299740865826 19.611720275878906 0.6453195095062256 8.414558553695679 3833921 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00047980381059460343 23.699292755126955 0.5498285889625549 8.827233123779298 4045774 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008852107013808564 17.064469718933104 0.5708524465560914 8.116253137588501 4256276 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -6.488724320661277e-05 16.818987083435058 0.4059797614812851 8.911626625061036 4468140 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005814165302581387 18.8499116897583 0.41817966997623446 8.82149887084961 4679863 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00031179755787889005 13.564528846740723 0.42932442724704745 8.998262500762939 4891273 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016667013695041532 16.38348779678345 0.35304987132549287 8.993251132965089 5103244 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008561446971725673 23.271946907043457 0.3442059248685837 9.421193313598632 5316722 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012563818963826634 13.71452808380127 0.41502062380313876 9.096197128295898 5530070 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001730615767883137 17.736111640930176 0.37007731199264526 9.3897123336792 5742501 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003397935970497201 20.61973457336426 0.17498904913663865 10.003363037109375 5958828 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005338681079592788 13.451806259155273 0.29747873544692993 9.984007740020752 6175118 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011473411141196266 16.428175735473634 0.33835242986679076 9.668470764160157 6387167 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003971638790972065 18.477954387664795 0.22440022230148315 9.965820598602296 6599041 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010821028117788956 14.371306419372559 0.19683135449886321 10.074924278259278 6812073 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001807526841730578 17.85355739593506 0.20740982443094252 10.052687931060792 7024751 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010516337135413778 21.163171768188477 0.1524307131767273 10.662235164642334 7236504 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009559658108628355 17.91001319885254 0.12433216124773025 10.633238697052002 7451615 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004170251479081344 16.554838180541992 0.26936138719320296 10.408903503417969 7663696 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008091387397143989 14.56689157485962 0.1753750666975975 10.568739891052246 7874683 0


Pure best response payoff estimated to be 89.925 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 74.77 seconds to finish estimate with resulting utilities: [148.61   2.36]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 133.61 seconds to finish estimate with resulting utilities: [112.095  55.58 ]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 133.16 seconds to finish estimate with resulting utilities: [82.58 70.25]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 136.0 seconds to finish estimate with resulting utilities: [31.95 31.95]
Computing meta_strategies
Exited RRD with total regret 1.8292715008289093 that was less than regret lambda 1.8333333333333335 after 215 iterations 
REGRET STEPS:  25
NEW LAMBDA 1.7500000000000002
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0    47.85           3.83           2.25           2.36      
    1    189.56          96.45          53.25          55.58      
    2    165.81          125.72          46.79          70.25      
    3    148.61          112.09          82.58          31.95      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0    47.85          189.56          165.81          148.61      
    1     3.83          96.45          125.72          112.09      
    2     2.25          53.25          46.79          82.58      
    3     2.36          55.58          70.25          31.95      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0    95.71          193.39          168.06          150.97      
    1    193.39          192.91          178.97          167.68      
    2    168.06          178.97          93.58          152.83      
    3    150.97          167.68          152.83          63.90      

 

Metagame probabilities: 
Player #0: 0.0001  0.0961  0.4933  0.4105  
Player #1: 0.0001  0.0961  0.4933  0.4105  
Iteration : 3
Time so far: 26912.52823972702
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-07-17 03:14:38.686576: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7779 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01620468609035015 47.27140769958496 0.3130356103181839 12.003254508972168 10360 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01844752412289381 15.429312801361084 0.38746289908885956 10.873884868621825 222163 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021210756711661817 11.926380443572999 0.46070344746112823 9.923595714569093 432444 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020063584111630915 16.38177909851074 0.4564264118671417 10.070890522003173 646263 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02114606574177742 19.218484306335448 0.5152582198381424 8.937208938598634 858546 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023746885173022748 17.175857639312746 0.6299217522144318 8.024866485595703 1070193 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022861633449792862 18.067387676239015 0.648473608493805 7.697255706787109 1280252 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01779481964185834 19.014270210266112 0.53055619597435 9.105032539367675 1490922 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013317264057695866 10.818517589569092 0.4245874762535095 10.71772222518921 1702699 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014517437387257814 24.661895561218262 0.5468453377485275 8.747038459777832 1914946 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012325832154601812 14.468814754486084 0.5219593614339828 9.11782169342041 2123946 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011259685829281807 16.649878597259523 0.5253796607255936 8.866680145263672 2332937 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01057479428127408 14.113730525970459 0.5528019160032273 9.113684940338135 2546155 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00846360232681036 15.563705921173096 0.5682560861110687 8.461788940429688 2759706 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009312429744750262 16.55219488143921 0.6032435655593872 8.057798290252686 2972145 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006572424853220582 17.01413516998291 0.5519446164369584 8.36742343902588 3185532 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005553153925575316 20.71434326171875 0.5920577883720398 8.115962743759155 3397417 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0030535450903698804 20.552365207672118 0.53206966817379 8.86245346069336 3608210 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0027471204288303853 15.466683387756348 0.5271297246217728 9.235788154602051 3820003 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010939917498035356 17.234261417388915 0.4045225530862808 9.802017116546631 4032644 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004471874330192804 22.242434883117674 0.338887220621109 9.85039234161377 4246304 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009822245046962053 14.352716541290283 0.3095256119966507 11.396461296081544 4456073 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009895728144329041 27.081651878356933 0.35887999534606935 10.193079090118408 4669356 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -3.015347621158071e-05 17.572742176055907 0.2798891022801399 10.674560070037842 4880750 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00024073181339190342 19.732065963745118 0.35304777026176454 11.015093135833741 5093193 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00015853311488172038 17.845935344696045 0.40644398927688596 9.522994709014892 5303771 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007641879565198906 14.27358274459839 0.3574870377779007 10.327899265289307 5517050 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005273532522551249 20.160360431671144 0.36128194332122804 9.547518062591553 5729540 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00044895407045260073 17.554189491271973 0.37964228391647337 9.679137706756592 5942375 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007453858001099434 17.85693531036377 0.3948549211025238 9.88386287689209 6155288 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0002488502732376219 20.876347732543945 0.2385916829109192 10.498444747924804 6368278 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022273313283221797 17.92043046951294 0.30921200215816497 10.150512504577637 6584663 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010051494609797373 17.400196647644044 0.17333661764860153 10.834206199645996 6798383 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -9.762904810486361e-05 18.07331266403198 0.21627907007932662 10.53552713394165 7010516 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00013478660766850226 16.697436332702637 0.28305978178977964 10.4391019821167 7222626 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011947132239583879 19.623793029785155 0.2285910502076149 11.783715724945068 7432308 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00020288568775868044 19.3226676940918 0.2226296231150627 10.815400409698487 7641014 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006274561899772379 23.958358383178712 0.1606755182147026 11.425567626953125 7853316 0


Pure best response payoff estimated to be 85.325 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 60.44 seconds to finish estimate with resulting utilities: [111.115   3.065]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 111.08 seconds to finish estimate with resulting utilities: [88.835 36.895]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 129.75 seconds to finish estimate with resulting utilities: [77.765 54.66 ]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 125.68 seconds to finish estimate with resulting utilities: [66.11 39.71]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 118.22 seconds to finish estimate with resulting utilities: [51.18 51.1 ]
Computing meta_strategies
Exited RRD with total regret 1.7423590644421552 that was less than regret lambda 1.7500000000000002 after 421 iterations 
REGRET STEPS:  25
NEW LAMBDA 1.666666666666667
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0    47.85           3.83           2.25           2.36           3.06      
    1    189.56          96.45          53.25          55.58          36.90      
    2    165.81          125.72          46.79          70.25          54.66      
    3    148.61          112.09          82.58          31.95          39.71      
    4    111.11          88.83          77.77          66.11          51.14      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0    47.85          189.56          165.81          148.61          111.11      
    1     3.83          96.45          125.72          112.09          88.83      
    2     2.25          53.25          46.79          82.58          77.77      
    3     2.36          55.58          70.25          31.95          66.11      
    4     3.06          36.90          54.66          39.71          51.14      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0    95.71          193.39          168.06          150.97          114.18      
    1    193.39          192.91          178.97          167.68          125.73      
    2    168.06          178.97          93.58          152.83          132.43      
    3    150.97          167.68          152.83          63.90          105.82      
    4    114.18          125.73          132.43          105.82          102.28      

 

Metagame probabilities: 
Player #0: 0.0001  0.0017  0.1868  0.0338  0.7776  
Player #1: 0.0001  0.0017  0.1868  0.0338  0.7776  
Iteration : 4
Time so far: 37102.02097034454
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-07-17 06:04:28.488997: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9757 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013948222063481808 40.7536865234375 0.2716371163725853 12.590321254730224 10326 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02393822241574526 20.439928436279295 0.5075056701898575 10.71022834777832 219889 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028582152165472507 16.49224739074707 0.6308798313140869 9.573317718505859 429776 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022794184274971485 22.517700004577637 0.5008077621459961 10.435866260528565 639114 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018115697149187326 19.885729217529295 0.45052762031555177 10.997871780395508 849603 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020254487358033656 15.333935737609863 0.5407440572977066 10.515018749237061 1061435 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022247561253607273 15.277300453186035 0.6537565767765046 9.276995372772216 1269078 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019002770818769932 16.205740451812744 0.5787367641925811 9.628508567810059 1475561 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020195604395121337 17.808386421203615 0.6361287057399749 8.722523212432861 1687451 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016420487966388463 11.417306518554687 0.5313314259052276 9.953853988647461 1897121 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01365370349958539 21.387829208374022 0.5399464577436447 9.830931472778321 2107857 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013638702034950257 14.701582622528075 0.6210416257381439 8.904057884216309 2318507 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00993738705292344 27.277843856811522 0.4758914649486542 10.191945457458496 2526978 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008506731176748872 20.73573589324951 0.47571141123771665 10.30210313796997 2738200 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007961239945143462 21.904599952697755 0.5583884477615356 9.588896942138671 2949511 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006092066434212029 19.916514778137206 0.43873838186264036 10.927404975891113 3159546 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004757091915234923 22.926178550720216 0.5877107143402099 9.435627937316895 3372635 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0036530942190438507 21.04135055541992 0.543517529964447 10.20825128555298 3582148 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015653684036806225 14.883578014373779 0.32417684197425845 12.391660499572755 3789637 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007902017961896491 17.064367961883544 0.677548211812973 9.013842391967774 3998906 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022491178817290345 28.000966835021973 0.3999056577682495 10.658622932434081 4207711 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  3.817535689449869e-05 18.69350175857544 0.48487805426120756 9.15884428024292 4415498 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007868568951380439 28.739927291870117 0.41440693438053133 10.817073822021484 4626972 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003080257258261554 30.733266258239745 0.2596706748008728 11.51442222595215 4833739 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004355373886937741 16.79298906326294 0.18487898260354996 10.204237461090088 5043943 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006450831504480447 16.988935470581055 0.12427722588181496 11.941368770599365 5252973 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006031702010659501 17.697852230072023 0.083003830909729 12.186971282958984 5461951 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00040435520495520907 24.09223155975342 0.14001716151833535 11.207053279876709 5672594 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -3.568842039385345e-05 14.863891315460204 0.056322123110294345 12.180174827575684 5885523 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008288057586469221 17.208571338653563 0.06338991932570934 11.146108150482178 6095021 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00014955410952097737 19.707240486145018 0.043735018000006676 12.788979244232177 6304525 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004910419454972725 18.014105319976807 0.04890507273375988 11.349106884002685 6516780 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00023340666521107778 22.61306381225586 0.05341426469385624 11.635603141784667 6724937 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007267176688401378 13.997801876068115 0.04853184260427952 11.888433647155761 6937134 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001134052527777385 31.35651626586914 0.04421489126980305 12.088498210906982 7147336 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0028001654231047724 19.530979347229003 0.044346507638692856 13.092785263061524 7360417 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003353414809680544 13.736939525604248 0.03447188977152109 12.388718795776366 7568903 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006495252338936553 27.84946937561035 0.04135197196155786 12.939920234680176 7780319 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000562363911740249 17.23370580673218 0.05033930279314518 12.122849273681641 7991564 0


Pure best response payoff estimated to be 71.735 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 68.93 seconds to finish estimate with resulting utilities: [127.11    3.695]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 126.33 seconds to finish estimate with resulting utilities: [101.96   48.035]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 139.67 seconds to finish estimate with resulting utilities: [66.865 45.4  ]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 133.85 seconds to finish estimate with resulting utilities: [70.095 40.72 ]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 119.95 seconds to finish estimate with resulting utilities: [64.81 51.88]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 129.24 seconds to finish estimate with resulting utilities: [50.675 53.88 ]
Computing meta_strategies
Exited RRD with total regret 1.665032731319755 that was less than regret lambda 1.666666666666667 after 468 iterations 
REGRET STEPS:  25
NEW LAMBDA 1.5833333333333337
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0    47.85           3.83           2.25           2.36           3.06           3.69      
    1    189.56          96.45          53.25          55.58          36.90          48.03      
    2    165.81          125.72          46.79          70.25          54.66          45.40      
    3    148.61          112.09          82.58          31.95          39.71          40.72      
    4    111.11          88.83          77.77          66.11          51.14          51.88      
    5    127.11          101.96          66.86          70.09          64.81          52.28      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0    47.85          189.56          165.81          148.61          111.11          127.11      
    1     3.83          96.45          125.72          112.09          88.83          101.96      
    2     2.25          53.25          46.79          82.58          77.77          66.86      
    3     2.36          55.58          70.25          31.95          66.11          70.09      
    4     3.06          36.90          54.66          39.71          51.14          64.81      
    5     3.69          48.03          45.40          40.72          51.88          52.28      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0    95.71          193.39          168.06          150.97          114.18          130.81      
    1    193.39          192.91          178.97          167.68          125.73          150.00      
    2    168.06          178.97          93.58          152.83          132.43          112.26      
    3    150.97          167.68          152.83          63.90          105.82          110.81      
    4    114.18          125.73          132.43          105.82          102.28          116.69      
    5    130.81          150.00          112.26          110.81          116.69          104.56      

 

Metagame probabilities: 
Player #0: 0.0001  0.0037  0.0299  0.0035  0.1925  0.7703  
Player #1: 0.0001  0.0037  0.0299  0.0035  0.1925  0.7703  
Iteration : 5
Time so far: 47347.06698656082
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-07-17 08:55:14.010520: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11735 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016341771406587213 45.25341606140137 0.30860217958688735 11.807162666320801 10783 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02068066820502281 12.555226039886474 0.42891235649585724 11.13943338394165 222305 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026843529008328915 13.289043426513672 0.5896495878696442 10.465808582305907 429678 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027104740031063557 14.14222583770752 0.6338364362716675 10.234359073638917 638644 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024138935655355454 12.87670078277588 0.59283567070961 10.502890682220459 844048 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0239190649241209 13.18963565826416 0.6402297973632812 10.433258724212646 1052436 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022233624942600727 11.857292366027831 0.6174494206905365 10.18101749420166 1262727 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016353309247642757 15.40123109817505 0.49199304580688474 10.866297245025635 1470334 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01879190783947706 15.01515531539917 0.6142473876476288 10.06560525894165 1679067 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013860483095049858 20.7272798538208 0.508277851343155 10.661981868743897 1885836 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015949549339711665 12.101054286956787 0.6468943119049072 10.035225868225098 2095415 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011881683114916086 18.769395637512208 0.51319979429245 10.873045825958252 2304515 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00947581217624247 22.59587860107422 0.43155576288700104 11.719551849365235 2513309 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009622691804543138 16.668001461029053 0.5549962818622589 10.834737396240234 2721868 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008343718154355884 14.409756851196288 0.6149511754512786 10.470561599731445 2931121 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006418203469365835 14.822366333007812 0.5239209055900573 10.922329425811768 3140986 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0039732707315124575 22.211914443969725 0.4562769323587418 11.475134372711182 3351251 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0029035074403509498 17.307684898376465 0.5137381434440613 10.469449234008788 3559164 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002410006005084142 14.95259895324707 0.5445756077766418 10.499140167236328 3768536 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007960813934914767 18.39214210510254 0.48856773376464846 10.882912349700927 3977350 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019826708536129444 11.7554536819458 0.42805146276950834 10.26680393218994 4185719 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013733438943745568 14.366483592987061 0.3882790178060532 11.328691673278808 4395942 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002156735531752929 14.335198974609375 0.35585974156856537 10.472375583648681 4603303 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00012552624830277636 20.48219566345215 0.34637959599494933 11.265288925170898 4810826 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022102100969732417 25.47657718658447 0.15721601992845535 11.76517858505249 5021113 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005286392959533259 14.090630340576173 0.16837359517812728 11.843770408630371 5232458 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001133718976052478 12.910486888885497 0.13159219175577164 11.65888671875 5440497 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007585711806314066 16.03865146636963 0.15789946019649506 11.523394012451172 5651243 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00053850048861932 20.094424629211424 0.14588455706834794 11.196674823760986 5859659 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004925195840769448 23.149452781677248 0.12473934665322303 11.570918655395507 6070763 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004521895985817537 13.500601863861084 0.10290655568242073 11.242886447906494 6279817 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004099527548532933 21.88798408508301 0.08234774768352508 12.023477458953858 6489900 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001564151397360547 18.677235507965086 0.07728143408894539 11.885473823547363 6698115 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007961685943882913 16.74622745513916 0.05534383691847324 12.381077766418457 6905980 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005617058148345677 13.721470737457276 0.07791992016136647 12.048370552062988 7114743 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00036298003324191085 14.774662113189697 0.066729386895895 12.251683616638184 7323812 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00029291711107362064 17.20897274017334 0.08110000491142273 11.893521499633788 7533088 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000438870875586872 21.77239055633545 0.09377586022019387 12.79514274597168 7743840 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007759618893032894 10.86434326171875 0.22160061299800873 12.0597243309021 7954262 0


Pure best response payoff estimated to be 68.845 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 43.48 seconds to finish estimate with resulting utilities: [84.885  1.935]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 100.05 seconds to finish estimate with resulting utilities: [77.33  43.385]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 125.11 seconds to finish estimate with resulting utilities: [48.225 41.155]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 114.93 seconds to finish estimate with resulting utilities: [63.53 48.45]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 101.93 seconds to finish estimate with resulting utilities: [53.135 42.175]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 108.06 seconds to finish estimate with resulting utilities: [63.505 54.425]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 95.31 seconds to finish estimate with resulting utilities: [41.59 42.86]
Computing meta_strategies
Exited RRD with total regret 1.5769568055386287 that was less than regret lambda 1.5833333333333337 after 830 iterations 
REGRET STEPS:  25
NEW LAMBDA 1.5000000000000004
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    47.85           3.83           2.25           2.36           3.06           3.69           1.94      
    1    189.56          96.45          53.25          55.58          36.90          48.03          43.38      
    2    165.81          125.72          46.79          70.25          54.66          45.40          41.16      
    3    148.61          112.09          82.58          31.95          39.71          40.72          48.45      
    4    111.11          88.83          77.77          66.11          51.14          51.88          42.17      
    5    127.11          101.96          66.86          70.09          64.81          52.28          54.42      
    6    84.89          77.33          48.23          63.53          53.13          63.51          42.23      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    47.85          189.56          165.81          148.61          111.11          127.11          84.89      
    1     3.83          96.45          125.72          112.09          88.83          101.96          77.33      
    2     2.25          53.25          46.79          82.58          77.77          66.86          48.23      
    3     2.36          55.58          70.25          31.95          66.11          70.09          63.53      
    4     3.06          36.90          54.66          39.71          51.14          64.81          53.13      
    5     3.69          48.03          45.40          40.72          51.88          52.28          63.51      
    6     1.94          43.38          41.16          48.45          42.17          54.42          42.23      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0    95.71          193.39          168.06          150.97          114.18          130.81          86.82      
    1    193.39          192.91          178.97          167.68          125.73          150.00          120.72      
    2    168.06          178.97          93.58          152.83          132.43          112.26          89.38      
    3    150.97          167.68          152.83          63.90          105.82          110.81          111.98      
    4    114.18          125.73          132.43          105.82          102.28          116.69          95.31      
    5    130.81          150.00          112.26          110.81          116.69          104.56          117.93      
    6    86.82          120.72          89.38          111.98          95.31          117.93          84.45      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0005  0.0001  0.0157  0.5709  0.4126  
Player #1: 0.0001  0.0001  0.0005  0.0001  0.0157  0.5709  0.4126  
Iteration : 6
Time so far: 57604.65165185928
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-07-17 11:46:11.374499: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13713 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018239014875143768 56.80443496704102 0.3555131509900093 11.461638641357421 10012 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0326192470267415 16.95237741470337 0.6472552716732025 9.698288726806641 220389 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03264286052435637 15.613132858276368 0.7229140222072601 9.055134773254395 427775 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0321284269914031 16.63207902908325 0.7502779066562653 9.20486764907837 636857 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022655306942760946 30.82896480560303 0.6509720325469971 9.942422008514404 846111 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03041738774627447 13.340274143218995 0.8155728101730346 8.822993946075439 1057002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0255282498896122 12.781581783294678 0.7074367105960846 9.168077278137208 1266186 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020998924039304256 15.373548316955567 0.651922756433487 9.152642154693604 1476508 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02075545508414507 18.637518215179444 0.6868671000003814 9.500020694732665 1683987 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020280623622238637 15.890763092041016 0.6687370777130127 9.14211015701294 1894290 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017538661416620017 13.005705547332763 0.697964483499527 9.016893863677979 2101865 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015776197798550128 14.602290058135987 0.7431156396865845 8.629799461364746 2309292 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011682412214577198 16.897511959075928 0.6990646004676819 9.213614654541015 2517674 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010552938375622035 12.686700916290283 0.577227771282196 10.422758102416992 2725141 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009262141212821007 18.9674373626709 0.6646310985088348 9.686621475219727 2934822 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0075372091960161924 14.649395275115968 0.6725679993629455 9.532393550872802 3143084 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005393840232864022 12.1484845161438 0.7023258984088898 9.506655502319337 3351692 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0025694256066344677 21.382663536071778 0.59970982670784 9.979530811309814 3559456 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022210695315152407 12.966962814331055 0.6305589258670807 10.188822841644287 3769530 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007116688328096643 16.44880142211914 0.6639924347400665 9.48413143157959 3977376 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0004197919122816529 18.80552272796631 0.5009042412042618 9.415002822875977 4184654 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005039929601480253 13.912913131713868 0.4125778526067734 10.765731048583984 4391825 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005944106771494262 19.9055534362793 0.5193225592374802 9.908902835845947 4600323 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007818882615538314 15.675578594207764 0.5248769819736481 9.176217079162598 4809463 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001415611289849039 15.023528861999512 0.3539381712675095 10.235886192321777 5016746 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018525522551499306 14.74842348098755 0.28294545114040376 9.595497035980225 5222651 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015648431406589225 15.281677150726319 0.33566159904003146 10.417010974884032 5431068 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006937377773283515 16.306697845458984 0.3281123280525208 10.468547153472901 5640322 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009327083207608666 16.769946002960204 0.16999734193086624 11.031693935394287 5849146 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00039928704645717514 14.459310054779053 0.10095532536506653 11.217756748199463 6055084 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010427080411318456 16.4799955368042 0.19162181317806243 11.51785192489624 6263232 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00026944009732687846 14.82059907913208 0.5120248824357987 10.090786552429199 6474149 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00040401902770099697 14.644955158233643 0.3346045821905136 10.226283359527589 6681830 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004171438471530564 11.746146965026856 0.17355305403470994 10.346501922607422 6890123 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -2.8856676362920552e-05 19.387303829193115 0.31411952078342437 10.615832138061524 7097501 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001164613288710825 14.674140739440919 0.370584163069725 10.278673458099366 7305503 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009499252410023473 14.121300601959229 0.347006818652153 10.813535976409913 7512162 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  2.8466132062021642e-05 12.361929035186767 0.20444050431251526 10.471676921844482 7720023 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005143994290847332 20.67129611968994 0.18695442229509354 11.081387042999268 7927146 0


Pure best response payoff estimated to be 65.005 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 36.32 seconds to finish estimate with resulting utilities: [74.765  1.705]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 95.02 seconds to finish estimate with resulting utilities: [75.18  43.635]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 123.1 seconds to finish estimate with resulting utilities: [44.22  45.695]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 110.84 seconds to finish estimate with resulting utilities: [57.505 46.595]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 106.43 seconds to finish estimate with resulting utilities: [54.24 45.38]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 95.78 seconds to finish estimate with resulting utilities: [57.11  46.275]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 80.58 seconds to finish estimate with resulting utilities: [48.525 44.185]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 77.29 seconds to finish estimate with resulting utilities: [48.375 48.665]
Computing meta_strategies
Exited RRD with total regret 1.494283792997635 that was less than regret lambda 1.5000000000000004 after 884 iterations 
REGRET STEPS:  25
NEW LAMBDA 1.4166666666666672
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    47.85           3.83           2.25           2.36           3.06           3.69           1.94           1.71      
    1    189.56          96.45          53.25          55.58          36.90          48.03          43.38          43.63      
    2    165.81          125.72          46.79          70.25          54.66          45.40          41.16          45.70      
    3    148.61          112.09          82.58          31.95          39.71          40.72          48.45          46.59      
    4    111.11          88.83          77.77          66.11          51.14          51.88          42.17          45.38      
    5    127.11          101.96          66.86          70.09          64.81          52.28          54.42          46.27      
    6    84.89          77.33          48.23          63.53          53.13          63.51          42.23          44.19      
    7    74.77          75.18          44.22          57.51          54.24          57.11          48.52          48.52      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    47.85          189.56          165.81          148.61          111.11          127.11          84.89          74.77      
    1     3.83          96.45          125.72          112.09          88.83          101.96          77.33          75.18      
    2     2.25          53.25          46.79          82.58          77.77          66.86          48.23          44.22      
    3     2.36          55.58          70.25          31.95          66.11          70.09          63.53          57.51      
    4     3.06          36.90          54.66          39.71          51.14          64.81          53.13          54.24      
    5     3.69          48.03          45.40          40.72          51.88          52.28          63.51          57.11      
    6     1.94          43.38          41.16          48.45          42.17          54.42          42.23          48.52      
    7     1.71          43.63          45.70          46.59          45.38          46.27          44.19          48.52      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0    95.71          193.39          168.06          150.97          114.18          130.81          86.82          76.47      
    1    193.39          192.91          178.97          167.68          125.73          150.00          120.72          118.81      
    2    168.06          178.97          93.58          152.83          132.43          112.26          89.38          89.91      
    3    150.97          167.68          152.83          63.90          105.82          110.81          111.98          104.10      
    4    114.18          125.73          132.43          105.82          102.28          116.69          95.31          99.62      
    5    130.81          150.00          112.26          110.81          116.69          104.56          117.93          103.38      
    6    86.82          120.72          89.38          111.98          95.31          117.93          84.45          92.71      
    7    76.47          118.81          89.91          104.10          99.62          103.38          92.71          97.04      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0005  0.0001  0.0147  0.5421  0.3831  0.0593  
Player #1: 0.0001  0.0001  0.0005  0.0001  0.0147  0.5421  0.3831  0.0593  
Iteration : 7
Time so far: 67935.19880700111
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-07-17 14:38:22.035959: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15691 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02784243393689394 62.07560157775879 0.5354824781417846 10.548425769805908 10207 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.036156723648309706 14.51490831375122 0.7340743362903595 9.86171236038208 219045 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03257914558053017 12.931533241271973 0.7028447151184082 10.01083517074585 429624 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025294078327715398 13.596891593933105 0.5830273568630219 10.57955722808838 637016 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026834809221327305 23.15596122741699 0.6578158020973206 10.05410385131836 843283 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029291134886443616 15.480126190185548 0.7595977425575257 9.552521514892579 1051924 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02561220433562994 14.841185569763184 0.7104772925376892 9.503564548492431 1260751 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022903338819742203 14.336390209197997 0.7088993787765503 9.710100078582764 1469676 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017075141426175833 29.138066291809082 0.5617611885070801 11.065069580078125 1674866 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016387653816491366 17.020869541168214 0.6140807151794434 10.312792682647705 1882641 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01742502236738801 12.564937973022461 0.6974697530269622 9.393783664703369 2090192 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01441881451755762 16.904209899902344 0.6379462420940399 10.426767444610595 2299011 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014261564426124095 16.98463487625122 0.7331778347492218 9.83007287979126 2510515 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0112909653224051 18.457852935791017 0.6546109139919281 10.14834976196289 2717878 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009538388438522815 16.638112831115723 0.571818083524704 10.885243606567382 2929317 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0070327061228454115 15.9426775932312 0.6502014935016632 10.08219108581543 3135042 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006289631221443414 18.637873268127443 0.6002233624458313 10.323090553283691 3342006 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004364512232132256 13.960373878479004 0.715050208568573 9.8231689453125 3549427 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003267857455648482 14.547844409942627 0.6780921697616578 9.944372081756592 3757494 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017947121275938116 13.314191818237305 0.7817513108253479 9.362539768218994 3965128 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003784208960496471 11.121799659729003 0.48257799744606017 10.051996231079102 4174107 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0031808515544980765 14.083269309997558 0.4940432280302048 10.384301662445068 4382391 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007000086916377768 14.8987606048584 0.47945148348808286 10.036857795715331 4588597 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006900397827848792 9.850640869140625 0.37975296974182127 10.175070762634277 4795000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001203676898876438 15.220524215698243 0.13309451565146446 10.966005611419678 5003709 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00019883694185409694 14.790043926239013 0.3543743282556534 10.557104015350342 5212284 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008750182518269866 16.56681728363037 0.18766146153211594 10.906721496582032 5420160 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014327290147775785 13.727372169494629 0.4436704218387604 10.904683589935303 5629792 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0012201128189190057 17.727810287475585 0.3623144567012787 11.045152282714843 5838391 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003497142737614922 17.612567710876466 0.3809356242418289 10.94419240951538 6046846 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00011186237015863299 18.115194034576415 0.21428109407424928 11.131465244293214 6255307 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006575052761036204 15.60537223815918 0.21250657737255096 11.528942584991455 6463537 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001121779001550749 17.615287208557127 0.12901589199900626 11.486027812957763 6671502 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003268619650043547 14.360219287872315 0.33735412657260894 11.051994323730469 6880063 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005193948396481574 18.306523323059082 0.21386451423168182 11.788142776489257 7088730 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004156070572207682 14.671508407592773 0.2672307312488556 11.399817085266113 7296109 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0001651661026699003 17.016861152648925 0.09353461787104607 11.723541641235352 7503565 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006443360878620296 13.2413724899292 0.11612788066267968 11.592822647094726 7709750 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010068035044241697 15.770143413543702 0.24104175418615342 11.380772495269776 7917603 0


Pure best response payoff estimated to be 63.39 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (8, 0)
Current player 0 and current strategies (8, 0) took 43.95 seconds to finish estimate with resulting utilities: [83.25  1.73]
Estimating current strategies:  (8, 1)
Current player 0 and current strategies (8, 1) took 102.28 seconds to finish estimate with resulting utilities: [79.05  46.785]
Estimating current strategies:  (8, 2)
Current player 0 and current strategies (8, 2) took 132.75 seconds to finish estimate with resulting utilities: [41.93  46.695]
Estimating current strategies:  (8, 3)
Current player 0 and current strategies (8, 3) took 119.94 seconds to finish estimate with resulting utilities: [55.655 49.24 ]
Estimating current strategies:  (8, 4)
Current player 0 and current strategies (8, 4) took 109.25 seconds to finish estimate with resulting utilities: [54.01  42.055]
Estimating current strategies:  (8, 5)
Current player 0 and current strategies (8, 5) took 109.83 seconds to finish estimate with resulting utilities: [66.535 52.7  ]
Estimating current strategies:  (8, 6)
Current player 0 and current strategies (8, 6) took 79.83 seconds to finish estimate with resulting utilities: [46.475 41.55 ]
Estimating current strategies:  (8, 7)
Current player 0 and current strategies (8, 7) took 80.27 seconds to finish estimate with resulting utilities: [46.97 43.13]
Estimating current strategies:  (8, 8)
Current player 0 and current strategies (8, 8) took 91.14 seconds to finish estimate with resulting utilities: [47.195 47.86 ]
Computing meta_strategies
Exited RRD with total regret 1.4140305322095656 that was less than regret lambda 1.4166666666666672 after 993 iterations 
REGRET STEPS:  25
NEW LAMBDA 1.333333333333334
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    47.85           3.83           2.25           2.36           3.06           3.69           1.94           1.71           1.73      
    1    189.56          96.45          53.25          55.58          36.90          48.03          43.38          43.63          46.78      
    2    165.81          125.72          46.79          70.25          54.66          45.40          41.16          45.70          46.70      
    3    148.61          112.09          82.58          31.95          39.71          40.72          48.45          46.59          49.24      
    4    111.11          88.83          77.77          66.11          51.14          51.88          42.17          45.38          42.05      
    5    127.11          101.96          66.86          70.09          64.81          52.28          54.42          46.27          52.70      
    6    84.89          77.33          48.23          63.53          53.13          63.51          42.23          44.19          41.55      
    7    74.77          75.18          44.22          57.51          54.24          57.11          48.52          48.52          43.13      
    8    83.25          79.05          41.93          55.66          54.01          66.53          46.48          46.97          47.53      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    47.85          189.56          165.81          148.61          111.11          127.11          84.89          74.77          83.25      
    1     3.83          96.45          125.72          112.09          88.83          101.96          77.33          75.18          79.05      
    2     2.25          53.25          46.79          82.58          77.77          66.86          48.23          44.22          41.93      
    3     2.36          55.58          70.25          31.95          66.11          70.09          63.53          57.51          55.66      
    4     3.06          36.90          54.66          39.71          51.14          64.81          53.13          54.24          54.01      
    5     3.69          48.03          45.40          40.72          51.88          52.28          63.51          57.11          66.53      
    6     1.94          43.38          41.16          48.45          42.17          54.42          42.23          48.52          46.48      
    7     1.71          43.63          45.70          46.59          45.38          46.27          44.19          48.52          46.97      
    8     1.73          46.78          46.70          49.24          42.05          52.70          41.55          43.13          47.53      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8      
    0    95.71          193.39          168.06          150.97          114.18          130.81          86.82          76.47          84.98      
    1    193.39          192.91          178.97          167.68          125.73          150.00          120.72          118.81          125.83      
    2    168.06          178.97          93.58          152.83          132.43          112.26          89.38          89.91          88.62      
    3    150.97          167.68          152.83          63.90          105.82          110.81          111.98          104.10          104.90      
    4    114.18          125.73          132.43          105.82          102.28          116.69          95.31          99.62          96.06      
    5    130.81          150.00          112.26          110.81          116.69          104.56          117.93          103.38          119.23      
    6    86.82          120.72          89.38          111.98          95.31          117.93          84.45          92.71          88.03      
    7    76.47          118.81          89.91          104.10          99.62          103.38          92.71          97.04          90.10      
    8    84.98          125.83          88.62          104.90          96.06          119.23          88.03          90.10          95.06      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0003  0.0001  0.0019  0.3439  0.0335  0.005  0.6151  
Player #1: 0.0001  0.0001  0.0003  0.0001  0.0019  0.3439  0.0335  0.005  0.6151  
Iteration : 8
Time so far: 78249.21724700928
Approximating Best Response
Training best response:  True 0.06
rtg values check:  (?,) (?, 1)
2023-07-17 17:30:16.123981: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_62/weights_2/Adam_1/Assign' id:17669 op device:{requested: '', assigned: ''} def:{{{node mlp_62/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_62/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_62/weights_2/Adam_1, mlp_62/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022175282845273613 61.69020538330078 0.44052489548921586 11.527363491058349 10735 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030935913696885108 19.241029930114745 0.6369187414646149 10.035022449493407 220388 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02605278044939041 12.478063297271728 0.5593752145767212 10.848328495025635 427762 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028315576538443566 11.5983491897583 0.6628274917602539 10.307010555267334 638093 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02649961579591036 15.507214736938476 0.6503012418746948 10.354644870758056 845843 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024373880960047244 12.958753967285157 0.6527797222137451 10.619908809661865 1054341 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02393525466322899 10.298588275909424 0.6642177939414978 10.315610027313232 1262989 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019787344429641963 14.13651351928711 0.641452431678772 10.591134929656983 1473343 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01957022035494447 19.32781352996826 0.5673059552907944 10.83275899887085 1681635 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016575321089476347 15.304376697540283 0.5931765496730804 10.493410778045654 1889603 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01436604829505086 17.844161796569825 0.6384162783622742 10.590791606903077 2098201 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01218232037499547 15.735431575775147 0.5547439634799958 10.768003463745117 2307154 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012070077750831843 13.064037132263184 0.5862043082714081 10.893354606628417 2513945 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009538814378902315 13.281497573852539 0.5717423111200333 10.86858901977539 2722614 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007142701139673591 27.86599807739258 0.49890920519828796 11.2185302734375 2930800 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007546251546591521 18.22098226547241 0.5753262996673584 10.702284908294677 3139952 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005751176388002932 17.26924076080322 0.6109367609024048 10.68418092727661 3349211 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0041281939949840306 15.02416181564331 0.6609764218330383 10.454101848602296 3557863 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0020769564318470655 16.360384368896483 0.6303617417812347 10.579413890838623 3766418 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016918194654863329 13.91823606491089 0.6319570124149323 10.456536388397216 3975642 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00048088918265420943 14.29136619567871 0.43758864104747774 10.774371433258057 4184408 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00020261957834009082 15.135729789733887 0.12251191958785057 11.478542518615722 4392643 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -2.022148692049086e-06 17.24036626815796 0.12789070904254912 11.922181987762452 4600346 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008391978641157039 15.43648157119751 0.2626294240355492 11.5143253326416 4808489 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00023969034664332866 18.672478199005127 0.48317387104034426 11.040200805664062 5018208 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  1.4598941197618842e-05 20.361876678466796 0.3651672095060349 11.710350513458252 5228405 0
/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/imitation_fine_tune.py:379: RuntimeWarning: invalid value encountered in divide
  legal_probs = legal_probs / np.sum(legal_probs)
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0016754686046624556 15.328220558166503 0.4468351751565933 10.899354076385498 5436102 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006194274894369301 19.758395767211915 0.15729505866765975 11.256475067138672 5643941 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015905744090559891 11.752050971984863 0.18306232690811158 10.943990802764892 5853538 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007324990234337747 27.138892936706544 0.0498172365128994 12.573930358886718 6062676 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010762630961835384 17.512792015075682 0.24435435384511947 11.787667083740235 6271186 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004018575142254122 12.197712421417236 0.06840695068240166 11.404356575012207 6480998 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017001810992951505 15.33735694885254 0.17936700582504272 11.207849216461181 6690059 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00025129648856818675 15.86193780899048 0.1271841250360012 11.784917831420898 6900856 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00043550746086111757 16.190840625762938 0.18333060294389725 11.455525779724121 7108839 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000197371632384602 26.831861114501955 0.23939538151025772 12.012109088897706 7317908 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011154422129038722 28.825410652160645 0.050937995314598083 13.414365768432617 7524699 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005972664803266525 14.321006298065186 0.10966887846589088 12.282053470611572 7732230 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009387287776917219 20.843247413635254 0.16280843913555146 12.073563575744629 7940493 0
Recovering previous policy with expected return of 50.54228855721393. Long term value was 50.151 and short term was 53.495.


Pure best response payoff estimated to be 59.905 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (9, 0)
Current player 0 and current strategies (9, 0) took 42.28 seconds to finish estimate with resulting utilities: [78.965  1.69 ]
Estimating current strategies:  (9, 1)
Current player 0 and current strategies (9, 1) took 100.97 seconds to finish estimate with resulting utilities: [74.67  45.035]
Estimating current strategies:  (9, 2)
Current player 0 and current strategies (9, 2) took 126.34 seconds to finish estimate with resulting utilities: [37.2  38.86]
Estimating current strategies:  (9, 3)
Current player 0 and current strategies (9, 3) took 121.64 seconds to finish estimate with resulting utilities: [53.535 47.125]
Estimating current strategies:  (9, 4)
Current player 0 and current strategies (9, 4) took 104.81 seconds to finish estimate with resulting utilities: [51.43  39.805]
Estimating current strategies:  (9, 5)
Current player 0 and current strategies (9, 5) took 106.82 seconds to finish estimate with resulting utilities: [64.88  50.445]
Estimating current strategies:  (9, 6)
Current player 0 and current strategies (9, 6) took 84.76 seconds to finish estimate with resulting utilities: [46.655 44.79 ]
Estimating current strategies:  (9, 7)
Current player 0 and current strategies (9, 7) took 89.3 seconds to finish estimate with resulting utilities: [51.355 45.275]
Estimating current strategies:  (9, 8)
Current player 0 and current strategies (9, 8) took 90.17 seconds to finish estimate with resulting utilities: [49.06 45.88]
Estimating current strategies:  (9, 9)
Current player 0 and current strategies (9, 9) took 89.2 seconds to finish estimate with resulting utilities: [46.025 49.115]
Computing meta_strategies
Exited RRD with total regret 1.333034096012078 that was less than regret lambda 1.333333333333334 after 1188 iterations 
REGRET STEPS:  25
NEW LAMBDA 1.2500000000000004
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    47.85           3.83           2.25           2.36           3.06           3.69           1.94           1.71           1.73           1.69      
    1    189.56          96.45          53.25          55.58          36.90          48.03          43.38          43.63          46.78          45.03      
    2    165.81          125.72          46.79          70.25          54.66          45.40          41.16          45.70          46.70          38.86      
    3    148.61          112.09          82.58          31.95          39.71          40.72          48.45          46.59          49.24          47.12      
    4    111.11          88.83          77.77          66.11          51.14          51.88          42.17          45.38          42.05          39.80      
    5    127.11          101.96          66.86          70.09          64.81          52.28          54.42          46.27          52.70          50.45      
    6    84.89          77.33          48.23          63.53          53.13          63.51          42.23          44.19          41.55          44.79      
    7    74.77          75.18          44.22          57.51          54.24          57.11          48.52          48.52          43.13          45.27      
    8    83.25          79.05          41.93          55.66          54.01          66.53          46.48          46.97          47.53          45.88      
    9    78.97          74.67          37.20          53.53          51.43          64.88          46.66          51.35          49.06          47.57      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    47.85          189.56          165.81          148.61          111.11          127.11          84.89          74.77          83.25          78.97      
    1     3.83          96.45          125.72          112.09          88.83          101.96          77.33          75.18          79.05          74.67      
    2     2.25          53.25          46.79          82.58          77.77          66.86          48.23          44.22          41.93          37.20      
    3     2.36          55.58          70.25          31.95          66.11          70.09          63.53          57.51          55.66          53.53      
    4     3.06          36.90          54.66          39.71          51.14          64.81          53.13          54.24          54.01          51.43      
    5     3.69          48.03          45.40          40.72          51.88          52.28          63.51          57.11          66.53          64.88      
    6     1.94          43.38          41.16          48.45          42.17          54.42          42.23          48.52          46.48          46.66      
    7     1.71          43.63          45.70          46.59          45.38          46.27          44.19          48.52          46.97          51.35      
    8     1.73          46.78          46.70          49.24          42.05          52.70          41.55          43.13          47.53          49.06      
    9     1.69          45.03          38.86          47.12          39.80          50.45          44.79          45.27          45.88          47.57      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    95.71          193.39          168.06          150.97          114.18          130.81          86.82          76.47          84.98          80.66      
    1    193.39          192.91          178.97          167.68          125.73          150.00          120.72          118.81          125.83          119.70      
    2    168.06          178.97          93.58          152.83          132.43          112.26          89.38          89.91          88.62          76.06      
    3    150.97          167.68          152.83          63.90          105.82          110.81          111.98          104.10          104.90          100.66      
    4    114.18          125.73          132.43          105.82          102.28          116.69          95.31          99.62          96.06          91.23      
    5    130.81          150.00          112.26          110.81          116.69          104.56          117.93          103.38          119.23          115.32      
    6    86.82          120.72          89.38          111.98          95.31          117.93          84.45          92.71          88.03          91.44      
    7    76.47          118.81          89.91          104.10          99.62          103.38          92.71          97.04          90.10          96.63      
    8    84.98          125.83          88.62          104.90          96.06          119.23          88.03          90.10          95.06          94.94      
    9    80.66          119.70          76.06          100.66          91.23          115.32          91.44          96.63          94.94          95.14      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0001  0.0001  0.0001  0.282  0.0154  0.002  0.4051  0.295  
Player #1: 0.0001  0.0001  0.0001  0.0001  0.0001  0.282  0.0154  0.002  0.4051  0.295  
Iteration : 9
Time so far: 88850.07000279427
Approximating Best Response
Training best response:  True 0.05500000000000001
rtg values check:  (?,) (?, 1)
2023-07-17 20:26:57.300291: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_69/weights_2/Adam_1/Assign' id:19647 op device:{requested: '', assigned: ''} def:{{{node mlp_69/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_69/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_69/weights_2/Adam_1, mlp_69/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02704295888543129 68.59930038452148 0.538723748922348 10.520670890808105 10692 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03275224678218365 16.340005970001222 0.691042560338974 9.533648109436035 219173 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027441604994237423 18.096042823791503 0.5966325044631958 10.691053009033203 428687 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028764447197318077 13.081814575195313 0.6557466328144074 9.50514726638794 636879 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021368362195789813 18.190331268310548 0.5216411054134369 10.738226699829102 847019 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02439248524606228 13.001405429840087 0.6341058850288391 9.679777145385742 1055005 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02189688812941313 18.4573112487793 0.6162070512771607 10.139409923553467 1262663 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022228421457111836 11.398124504089356 0.6899189114570617 9.210139274597168 1471901 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02136853989213705 14.210276985168457 0.7207195341587067 8.964692783355712 1678622 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01571775684133172 18.176051521301268 0.5686389923095703 10.630850887298584 1888240 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016794615797698496 14.520406723022461 0.6623381435871124 9.73597002029419 2099402 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01564933778718114 14.308498668670655 0.6584996521472931 9.702691650390625 2307600 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012060065381228923 17.49962797164917 0.5978854715824127 9.733086013793946 2515862 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013008964620530606 14.474205017089844 0.6650769650936127 9.347407054901122 2724277 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008946914039552212 15.368191146850586 0.6570290386676788 9.36787223815918 2932196 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007911029551178218 16.82271280288696 0.649098789691925 9.344209671020508 3139511 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005887141870334745 13.830129528045655 0.6280933320522308 9.580952739715576 3345816 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0037141657783649864 20.011595153808592 0.553236934542656 10.188439559936523 3554826 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0023324240231886507 15.742478370666504 0.5164695888757705 10.392087841033936 3764298 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00035339042806299404 25.37427520751953 0.5122778624296188 10.47895622253418 3973062 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00030285830289358275 15.027824687957764 0.4414815068244934 10.371623039245605 4181026 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004716695504612289 15.044174575805664 0.49437940716743467 10.060262870788574 4390108 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005304163689288544 26.709368324279787 0.33577660620212557 11.65610466003418 4598036 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009316032956121489 18.019504356384278 0.4268507272005081 10.817037677764892 4805115 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008042364017455839 12.869630432128906 0.37029457092285156 10.014289093017577 5012741 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005232127998965552 12.024974727630616 0.32862223088741305 9.960317993164063 5221156 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00047319123623310586 17.88603630065918 0.13739388212561607 10.989070224761964 5428576 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000899454893078655 15.706428527832031 0.4899197220802307 10.12353229522705 5637992 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006060707572032697 20.741011810302734 0.19209900498390198 11.002067756652831 5846217 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000879638653168513 18.23250617980957 0.20706575065851213 11.9948655128479 6055036 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010034763767180267 18.41478157043457 0.18219834864139556 11.025972270965577 6266247 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00013549841241911054 16.048724365234374 0.08835985362529755 11.201891326904297 6475232 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011302710132440551 17.48470821380615 0.14754679650068284 11.58275909423828 6682847 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008874260354787111 14.19311876296997 0.4046276867389679 10.984981060028076 6891192 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  5.208379716350464e-05 24.442130661010744 0.27142573446035384 11.747059631347657 7098877 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004107827775442274 32.898726081848146 0.22500942051410674 12.085841369628906 7308351 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004923928354401141 14.321466732025147 0.2603702455759048 11.14822702407837 7518958 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00010717706754803658 16.970031356811525 0.16880255192518234 11.296225357055665 7726128 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009506004942522849 10.496083545684815 0.1369408681988716 11.208052730560302 7934692 0


Pure best response payoff estimated to be 62.955 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (10, 0)
Current player 0 and current strategies (10, 0) took 44.45 seconds to finish estimate with resulting utilities: [87.82   2.645]
Estimating current strategies:  (10, 1)
Current player 0 and current strategies (10, 1) took 93.04 seconds to finish estimate with resulting utilities: [78.54 39.45]
Estimating current strategies:  (10, 2)
Current player 0 and current strategies (10, 2) took 124.59 seconds to finish estimate with resulting utilities: [38.035 40.28 ]
Estimating current strategies:  (10, 3)
Current player 0 and current strategies (10, 3) took 112.97 seconds to finish estimate with resulting utilities: [56.56  45.835]
Estimating current strategies:  (10, 4)
Current player 0 and current strategies (10, 4) took 99.1 seconds to finish estimate with resulting utilities: [48.59  39.945]
Estimating current strategies:  (10, 5)
Current player 0 and current strategies (10, 5) took 98.02 seconds to finish estimate with resulting utilities: [64.7   48.025]
Estimating current strategies:  (10, 6)
Current player 0 and current strategies (10, 6) took 85.13 seconds to finish estimate with resulting utilities: [53.075 41.715]
Estimating current strategies:  (10, 7)
Current player 0 and current strategies (10, 7) took 75.49 seconds to finish estimate with resulting utilities: [52.94  37.835]
Estimating current strategies:  (10, 8)
Current player 0 and current strategies (10, 8) took 82.41 seconds to finish estimate with resulting utilities: [52.185 46.17 ]
Estimating current strategies:  (10, 9)
Current player 0 and current strategies (10, 9) took 75.96 seconds to finish estimate with resulting utilities: [52.125 40.61 ]
Estimating current strategies:  (10, 10)
Current player 0 and current strategies (10, 10) took 88.21 seconds to finish estimate with resulting utilities: [52.56 53.89]
Computing meta_strategies
Exited RRD with total regret 1.2458501182063202 that was less than regret lambda 1.2500000000000004 after 1022 iterations 
REGRET STEPS:  25
NEW LAMBDA 1.1666666666666672
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0    47.85           3.83           2.25           2.36           3.06           3.69           1.94           1.71           1.73           1.69           2.65      
    1    189.56          96.45          53.25          55.58          36.90          48.03          43.38          43.63          46.78          45.03          39.45      
    2    165.81          125.72          46.79          70.25          54.66          45.40          41.16          45.70          46.70          38.86          40.28      
    3    148.61          112.09          82.58          31.95          39.71          40.72          48.45          46.59          49.24          47.12          45.84      
    4    111.11          88.83          77.77          66.11          51.14          51.88          42.17          45.38          42.05          39.80          39.95      
    5    127.11          101.96          66.86          70.09          64.81          52.28          54.42          46.27          52.70          50.45          48.02      
    6    84.89          77.33          48.23          63.53          53.13          63.51          42.23          44.19          41.55          44.79          41.72      
    7    74.77          75.18          44.22          57.51          54.24          57.11          48.52          48.52          43.13          45.27          37.84      
    8    83.25          79.05          41.93          55.66          54.01          66.53          46.48          46.97          47.53          45.88          46.17      
    9    78.97          74.67          37.20          53.53          51.43          64.88          46.66          51.35          49.06          47.57          40.61      
   10    87.82          78.54          38.03          56.56          48.59          64.70          53.08          52.94          52.19          52.12          53.23      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0    47.85          189.56          165.81          148.61          111.11          127.11          84.89          74.77          83.25          78.97          87.82      
    1     3.83          96.45          125.72          112.09          88.83          101.96          77.33          75.18          79.05          74.67          78.54      
    2     2.25          53.25          46.79          82.58          77.77          66.86          48.23          44.22          41.93          37.20          38.03      
    3     2.36          55.58          70.25          31.95          66.11          70.09          63.53          57.51          55.66          53.53          56.56      
    4     3.06          36.90          54.66          39.71          51.14          64.81          53.13          54.24          54.01          51.43          48.59      
    5     3.69          48.03          45.40          40.72          51.88          52.28          63.51          57.11          66.53          64.88          64.70      
    6     1.94          43.38          41.16          48.45          42.17          54.42          42.23          48.52          46.48          46.66          53.08      
    7     1.71          43.63          45.70          46.59          45.38          46.27          44.19          48.52          46.97          51.35          52.94      
    8     1.73          46.78          46.70          49.24          42.05          52.70          41.55          43.13          47.53          49.06          52.19      
    9     1.69          45.03          38.86          47.12          39.80          50.45          44.79          45.27          45.88          47.57          52.12      
   10     2.65          39.45          40.28          45.84          39.95          48.02          41.72          37.84          46.17          40.61          53.23      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0    95.71          193.39          168.06          150.97          114.18          130.81          86.82          76.47          84.98          80.66          90.46      
    1    193.39          192.91          178.97          167.68          125.73          150.00          120.72          118.81          125.83          119.70          117.99      
    2    168.06          178.97          93.58          152.83          132.43          112.26          89.38          89.91          88.62          76.06          78.31      
    3    150.97          167.68          152.83          63.90          105.82          110.81          111.98          104.10          104.90          100.66          102.40      
    4    114.18          125.73          132.43          105.82          102.28          116.69          95.31          99.62          96.06          91.23          88.53      
    5    130.81          150.00          112.26          110.81          116.69          104.56          117.93          103.38          119.23          115.32          112.72      
    6    86.82          120.72          89.38          111.98          95.31          117.93          84.45          92.71          88.03          91.44          94.79      
    7    76.47          118.81          89.91          104.10          99.62          103.38          92.71          97.04          90.10          96.63          90.78      
    8    84.98          125.83          88.62          104.90          96.06          119.23          88.03          90.10          95.06          94.94          98.36      
    9    80.66          119.70          76.06          100.66          91.23          115.32          91.44          96.63          94.94          95.14          92.73      
   10    90.46          117.99          78.31          102.40          88.53          112.72          94.79          90.78          98.36          92.73          106.45      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0001  0.0001  0.0001  0.0637  0.0014  0.0001  0.0354  0.0021  0.8968  
Player #1: 0.0001  0.0001  0.0001  0.0001  0.0001  0.0637  0.0014  0.0001  0.0354  0.0021  0.8968  
Iteration : 10
Time so far: 99149.26686429977
Approximating Best Response
Training best response:  True 0.05
rtg values check:  (?,) (?, 1)
2023-07-17 23:18:36.745290: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_76/weights_2/Adam_1/Assign' id:21625 op device:{requested: '', assigned: ''} def:{{{node mlp_76/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_76/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_76/weights_2/Adam_1, mlp_76/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030408726073801517 58.033043670654294 0.5749061346054077 10.735302352905274 10274 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029551898688077928 18.321852684020996 0.6116439044475556 10.204744052886962 218897 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02683532889932394 17.739680671691893 0.5822895467281342 10.872934436798095 428677 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026641692221164703 16.0737455368042 0.6209911286830903 10.580451583862304 635937 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027460991591215133 15.752311897277831 0.6853913307189942 10.114077377319337 843766 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022371073067188264 25.131912994384766 0.6222015440464019 10.538578128814697 1050561 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023623789846897125 14.26800889968872 0.6649144649505615 10.537112522125245 1258455 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02210497744381428 13.589672946929932 0.6417851090431214 10.347256755828857 1464739 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020267228595912457 18.59467124938965 0.6388982355594635 10.521600532531739 1673961 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017521519865840673 12.834481048583985 0.6492707669734955 10.212353515625 1880710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014772333949804307 22.344193267822266 0.5762122988700866 10.727734375 2088530 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014100391324609518 16.69266195297241 0.6225719511508941 10.448580360412597 2295791 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011390647571533918 24.97624263763428 0.5596872448921204 10.888145542144775 2503457 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01153752626851201 13.454000473022461 0.6388803243637085 10.429531383514405 2709976 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007856720080599188 18.53672342300415 0.5519928574562073 10.716842746734619 2920486 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007288814056664705 12.636374568939209 0.6262794137001038 10.396185493469238 3128412 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006435398827306926 14.200055408477784 0.6345858633518219 10.525559520721435 3337811 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0036537084029987454 13.727048969268798 0.5729802429676056 10.70426549911499 3547155 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022337954374961554 34.51723022460938 0.4569300562143326 11.812629318237304 3754667 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009571963149937801 16.947967052459717 0.47824934124946594 11.425739860534668 3965351 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008913520578062162 25.848252487182616 0.4874508500099182 11.272967433929443 4171976 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006357892918458675 16.130885219573976 0.3778569847345352 11.133964824676514 4381022 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006106176559114828 21.677969551086427 0.3929588049650192 11.568539237976074 4589617 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00013246613088995217 17.033013820648193 0.3438182145357132 11.300476264953613 4798447 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012889838675619104 14.107040786743164 0.25790759474039077 11.538930702209473 5005756 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0007339970778957649 14.597911548614501 0.11045759916305542 11.60386257171631 5212095 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008391429233597592 16.362271785736084 0.053746183216571805 11.948473739624024 5419540 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016020673821913079 20.77062225341797 0.06844677142798901 12.006240081787109 5626975 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008548799043637701 14.51460599899292 0.08286882415413857 11.519061183929443 5834251 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00015401526034111157 21.663080406188964 0.3084615856409073 11.31476879119873 6043191 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008813947875751183 16.86447048187256 0.30299687683582305 11.436730670928956 6251478 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0005083723255211225 39.1886547088623 0.10984419882297516 12.388847923278808 6459056 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00024383853378822095 13.806063079833985 0.19025866240262984 11.670034694671632 6665559 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008045401889830827 18.41092128753662 0.09476679787039757 11.91781587600708 6874726 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00045791756419930605 27.23247699737549 0.07206100784242153 12.21532745361328 7080649 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011523921886691824 15.87307424545288 0.23122016936540604 12.102150440216064 7289375 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002005272766109556 17.835297966003417 0.2740217477083206 12.044158554077148 7496180 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006269816163694486 14.224876403808594 0.1710663452744484 12.178602123260498 7703244 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001174761581933126 18.336956024169922 0.2311484172940254 12.105742740631104 7911451 0


Pure best response payoff estimated to be 63.39 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (11, 0)
Current player 0 and current strategies (11, 0) took 41.64 seconds to finish estimate with resulting utilities: [85.885  1.76 ]
Estimating current strategies:  (11, 1)
Current player 0 and current strategies (11, 1) took 100.73 seconds to finish estimate with resulting utilities: [81.195 42.4  ]
Estimating current strategies:  (11, 2)
Current player 0 and current strategies (11, 2) took 125.59 seconds to finish estimate with resulting utilities: [42.395 33.975]
Estimating current strategies:  (11, 3)
Current player 0 and current strategies (11, 3) took 122.47 seconds to finish estimate with resulting utilities: [47.67 36.72]
Estimating current strategies:  (11, 4)
Current player 0 and current strategies (11, 4) took 110.64 seconds to finish estimate with resulting utilities: [48.83 37.73]
Estimating current strategies:  (11, 5)
Current player 0 and current strategies (11, 5) took 110.71 seconds to finish estimate with resulting utilities: [55.24 39.6 ]
Estimating current strategies:  (11, 6)
Current player 0 and current strategies (11, 6) took 78.22 seconds to finish estimate with resulting utilities: [51.555 34.68 ]
Estimating current strategies:  (11, 7)
Current player 0 and current strategies (11, 7) took 82.55 seconds to finish estimate with resulting utilities: [54.41  43.225]
Estimating current strategies:  (11, 8)
Current player 0 and current strategies (11, 8) took 86.4 seconds to finish estimate with resulting utilities: [52.75 41.13]
Estimating current strategies:  (11, 9)
Current player 0 and current strategies (11, 9) took 92.31 seconds to finish estimate with resulting utilities: [55.96  43.265]
Estimating current strategies:  (11, 10)
Current player 0 and current strategies (11, 10) took 81.25 seconds to finish estimate with resulting utilities: [56.525 41.27 ]
Estimating current strategies:  (11, 11)
Current player 0 and current strategies (11, 11) took 97.51 seconds to finish estimate with resulting utilities: [47.115 46.55 ]
Computing meta_strategies
Exited RRD with total regret 1.1666075501008066 that was less than regret lambda 1.1666666666666672 after 1499 iterations 
REGRET STEPS:  25
NEW LAMBDA 1.083333333333334
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11      
    0    47.85           3.83           2.25           2.36           3.06           3.69           1.94           1.71           1.73           1.69           2.65           1.76      
    1    189.56          96.45          53.25          55.58          36.90          48.03          43.38          43.63          46.78          45.03          39.45          42.40      
    2    165.81          125.72          46.79          70.25          54.66          45.40          41.16          45.70          46.70          38.86          40.28          33.98      
    3    148.61          112.09          82.58          31.95          39.71          40.72          48.45          46.59          49.24          47.12          45.84          36.72      
    4    111.11          88.83          77.77          66.11          51.14          51.88          42.17          45.38          42.05          39.80          39.95          37.73      
    5    127.11          101.96          66.86          70.09          64.81          52.28          54.42          46.27          52.70          50.45          48.02          39.60      
    6    84.89          77.33          48.23          63.53          53.13          63.51          42.23          44.19          41.55          44.79          41.72          34.68      
    7    74.77          75.18          44.22          57.51          54.24          57.11          48.52          48.52          43.13          45.27          37.84          43.23      
    8    83.25          79.05          41.93          55.66          54.01          66.53          46.48          46.97          47.53          45.88          46.17          41.13      
    9    78.97          74.67          37.20          53.53          51.43          64.88          46.66          51.35          49.06          47.57          40.61          43.27      
   10    87.82          78.54          38.03          56.56          48.59          64.70          53.08          52.94          52.19          52.12          53.23          41.27      
   11    85.89          81.19          42.40          47.67          48.83          55.24          51.55          54.41          52.75          55.96          56.52          46.83      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11      
    0    47.85          189.56          165.81          148.61          111.11          127.11          84.89          74.77          83.25          78.97          87.82          85.89      
    1     3.83          96.45          125.72          112.09          88.83          101.96          77.33          75.18          79.05          74.67          78.54          81.19      
    2     2.25          53.25          46.79          82.58          77.77          66.86          48.23          44.22          41.93          37.20          38.03          42.40      
    3     2.36          55.58          70.25          31.95          66.11          70.09          63.53          57.51          55.66          53.53          56.56          47.67      
    4     3.06          36.90          54.66          39.71          51.14          64.81          53.13          54.24          54.01          51.43          48.59          48.83      
    5     3.69          48.03          45.40          40.72          51.88          52.28          63.51          57.11          66.53          64.88          64.70          55.24      
    6     1.94          43.38          41.16          48.45          42.17          54.42          42.23          48.52          46.48          46.66          53.08          51.55      
    7     1.71          43.63          45.70          46.59          45.38          46.27          44.19          48.52          46.97          51.35          52.94          54.41      
    8     1.73          46.78          46.70          49.24          42.05          52.70          41.55          43.13          47.53          49.06          52.19          52.75      
    9     1.69          45.03          38.86          47.12          39.80          50.45          44.79          45.27          45.88          47.57          52.12          55.96      
   10     2.65          39.45          40.28          45.84          39.95          48.02          41.72          37.84          46.17          40.61          53.23          56.52      
   11     1.76          42.40          33.98          36.72          37.73          39.60          34.68          43.23          41.13          43.27          41.27          46.83      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11      
    0    95.71          193.39          168.06          150.97          114.18          130.81          86.82          76.47          84.98          80.66          90.46          87.65      
    1    193.39          192.91          178.97          167.68          125.73          150.00          120.72          118.81          125.83          119.70          117.99          123.59      
    2    168.06          178.97          93.58          152.83          132.43          112.26          89.38          89.91          88.62          76.06          78.31          76.37      
    3    150.97          167.68          152.83          63.90          105.82          110.81          111.98          104.10          104.90          100.66          102.40          84.39      
    4    114.18          125.73          132.43          105.82          102.28          116.69          95.31          99.62          96.06          91.23          88.53          86.56      
    5    130.81          150.00          112.26          110.81          116.69          104.56          117.93          103.38          119.23          115.32          112.72          94.84      
    6    86.82          120.72          89.38          111.98          95.31          117.93          84.45          92.71          88.03          91.44          94.79          86.23      
    7    76.47          118.81          89.91          104.10          99.62          103.38          92.71          97.04          90.10          96.63          90.78          97.63      
    8    84.98          125.83          88.62          104.90          96.06          119.23          88.03          90.10          95.06          94.94          98.36          93.88      
    9    80.66          119.70          76.06          100.66          91.23          115.32          91.44          96.63          94.94          95.14          92.73          99.22      
   10    90.46          117.99          78.31          102.40          88.53          112.72          94.79          90.78          98.36          92.73          106.45          97.80      
   11    87.65          123.59          76.37          84.39          86.56          94.84          86.23          97.63          93.88          99.22          97.80          93.66      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0001  0.0001  0.0001  0.0031  0.0001  0.0001  0.003  0.0003  0.1008  0.8922  
Player #1: 0.0001  0.0001  0.0001  0.0001  0.0001  0.0031  0.0001  0.0001  0.003  0.0003  0.1008  0.8922  
Iteration : 11
Time so far: 108969.2647986412
Approximating Best Response
Training best response:  True 0.045
rtg values check:  (?,) (?, 1)
2023-07-18 02:02:19.042246: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_83/weights_2/Adam_1/Assign' id:23603 op device:{requested: '', assigned: ''} def:{{{node mlp_83/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_83/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_83/weights_2/Adam_1, mlp_83/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019439558032900095 42.05571365356445 0.3944714516401291 12.94838981628418 10735 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031585502251982686 13.42479944229126 0.6532708168029785 11.86173448562622 219093 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027323305420577527 10.072265815734863 0.6083229660987854 12.049731826782226 427364 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026718372665345668 16.297476768493652 0.6054432451725006 11.783463191986083 637772 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022709768265485764 14.290833854675293 0.5816988706588745 11.580597972869873 845491 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02423803713172674 16.605802536010742 0.646019971370697 11.58289737701416 1055134 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021210641227662565 13.725372314453125 0.6102243781089782 11.88132381439209 1262400 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020632388070225716 17.79076271057129 0.5974200010299683 11.525489902496338 1472198 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021721498109400272 13.41689167022705 0.7384020805358886 11.142062854766845 1678404 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01713861506432295 12.886416912078857 0.6172696828842164 11.710590648651124 1887081 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016672156006097793 17.975087642669678 0.6824309647083282 11.504059886932373 2096176 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015911969263106583 12.815636157989502 0.7073531210422516 11.041021060943603 2304673 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0138505763374269 13.99392728805542 0.7081126987934112 11.554108715057373 2511100 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010926491394639016 13.176602840423584 0.6535254836082458 11.114442062377929 2718745 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009696338139474393 17.76918601989746 0.6255864143371582 11.655680084228516 2925287 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006974866334348917 25.355743980407716 0.5672458618879318 12.062762546539307 3131643 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006696296064183116 19.096281623840333 0.6866132140159606 11.306995296478272 3340857 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00398404966108501 14.645615482330323 0.694560068845749 11.344877338409423 3548315 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0035309397149831056 15.722780895233154 0.7675292313098907 11.135014724731445 3756454 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010834697706741281 14.867934608459473 0.6341537296772003 11.697077178955078 3966049 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004385159816592932 10.897290325164795 0.42254420220851896 11.587971782684326 4174482 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010724555409979074 12.977070617675782 0.24282377809286118 12.140259170532227 4382889 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00020241642487235368 14.70730152130127 0.5212943822145462 11.659465503692626 4589822 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00039757961058057846 11.111540985107421 0.5922644793987274 12.276125144958495 4798688 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0003250561272579944 16.35893850326538 0.44127023220062256 12.074583148956298 5006427 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009603294252883643 11.7599666595459 0.6052346467971802 11.755856132507324 5214887 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005194018316160509 10.323188495635986 0.5316058695316315 11.921184921264649 5420776 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  1.7124431957427076e-05 9.68750877380371 0.391362601518631 11.71135492324829 5630315 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0002358551219003857 14.679483032226562 0.47145028710365294 11.615060329437256 5837283 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00045000987011007966 10.030304908752441 0.2979579657316208 12.5401460647583 6042755 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002152913421014091 14.522386360168458 0.676502239704132 11.60306224822998 6250788 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00018178224854636937 14.916943836212159 0.5962767958641052 11.72837209701538 6459712 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000691543515495141 16.623896217346193 0.5504348814487457 11.744019412994385 6670241 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012535672023659572 17.380399227142334 0.13435420542955398 13.014299583435058 6878761 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005960174139545416 19.164755249023436 0.537053057551384 11.932571125030517 7083969 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008732000831514597 17.080445861816408 0.1575731486082077 12.444093894958495 7291285 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0008934253219194943 11.900848579406738 0.3484050899744034 12.068023490905762 7500115 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  7.603179437865037e-05 14.887606430053712 0.4977876037359238 11.9212890625 7709479 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -4.186108726571547e-05 12.62645902633667 0.43420386910438535 12.095291996002198 7915845 0


Pure best response payoff estimated to be 54.58 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (12, 0)
Current player 0 and current strategies (12, 0) took 39.74 seconds to finish estimate with resulting utilities: [75.945  2.225]
Estimating current strategies:  (12, 1)
Current player 0 and current strategies (12, 1) took 96.83 seconds to finish estimate with resulting utilities: [71.23 40.84]
Estimating current strategies:  (12, 2)
Current player 0 and current strategies (12, 2) took 127.55 seconds to finish estimate with resulting utilities: [40.67 37.07]
Estimating current strategies:  (12, 3)
Current player 0 and current strategies (12, 3) took 116.52 seconds to finish estimate with resulting utilities: [49.32  41.025]
Estimating current strategies:  (12, 4)
Current player 0 and current strategies (12, 4) took 120.73 seconds to finish estimate with resulting utilities: [45.25 38.93]
Estimating current strategies:  (12, 5)
Current player 0 and current strategies (12, 5) took 115.8 seconds to finish estimate with resulting utilities: [58.43 46.19]
Estimating current strategies:  (12, 6)
Current player 0 and current strategies (12, 6) took 84.0 seconds to finish estimate with resulting utilities: [46.345 37.48 ]
Estimating current strategies:  (12, 7)
Current player 0 and current strategies (12, 7) took 84.84 seconds to finish estimate with resulting utilities: [46.935 40.96 ]
Estimating current strategies:  (12, 8)
Current player 0 and current strategies (12, 8) took 81.27 seconds to finish estimate with resulting utilities: [47.64 42.29]
Estimating current strategies:  (12, 9)
Current player 0 and current strategies (12, 9) took 81.09 seconds to finish estimate with resulting utilities: [48.61 39.68]
Estimating current strategies:  (12, 10)
Current player 0 and current strategies (12, 10) took 86.98 seconds to finish estimate with resulting utilities: [54.07  47.945]
Estimating current strategies:  (12, 11)
Current player 0 and current strategies (12, 11) took 86.18 seconds to finish estimate with resulting utilities: [49.215 50.825]
Estimating current strategies:  (12, 12)
Current player 0 and current strategies (12, 12) took 87.43 seconds to finish estimate with resulting utilities: [46.185 46.38 ]
Computing meta_strategies
Exited RRD with total regret 1.0824416586325896 that was less than regret lambda 1.083333333333334 after 1624 iterations 
REGRET STEPS:  25
NEW LAMBDA 1.0000000000000004
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11             12      
    0    47.85           3.83           2.25           2.36           3.06           3.69           1.94           1.71           1.73           1.69           2.65           1.76           2.23      
    1    189.56          96.45          53.25          55.58          36.90          48.03          43.38          43.63          46.78          45.03          39.45          42.40          40.84      
    2    165.81          125.72          46.79          70.25          54.66          45.40          41.16          45.70          46.70          38.86          40.28          33.98          37.07      
    3    148.61          112.09          82.58          31.95          39.71          40.72          48.45          46.59          49.24          47.12          45.84          36.72          41.02      
    4    111.11          88.83          77.77          66.11          51.14          51.88          42.17          45.38          42.05          39.80          39.95          37.73          38.93      
    5    127.11          101.96          66.86          70.09          64.81          52.28          54.42          46.27          52.70          50.45          48.02          39.60          46.19      
    6    84.89          77.33          48.23          63.53          53.13          63.51          42.23          44.19          41.55          44.79          41.72          34.68          37.48      
    7    74.77          75.18          44.22          57.51          54.24          57.11          48.52          48.52          43.13          45.27          37.84          43.23          40.96      
    8    83.25          79.05          41.93          55.66          54.01          66.53          46.48          46.97          47.53          45.88          46.17          41.13          42.29      
    9    78.97          74.67          37.20          53.53          51.43          64.88          46.66          51.35          49.06          47.57          40.61          43.27          39.68      
   10    87.82          78.54          38.03          56.56          48.59          64.70          53.08          52.94          52.19          52.12          53.23          41.27          47.95      
   11    85.89          81.19          42.40          47.67          48.83          55.24          51.55          54.41          52.75          55.96          56.52          46.83          50.83      
   12    75.94          71.23          40.67          49.32          45.25          58.43          46.34          46.94          47.64          48.61          54.07          49.22          46.28      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11             12      
    0    47.85          189.56          165.81          148.61          111.11          127.11          84.89          74.77          83.25          78.97          87.82          85.89          75.94      
    1     3.83          96.45          125.72          112.09          88.83          101.96          77.33          75.18          79.05          74.67          78.54          81.19          71.23      
    2     2.25          53.25          46.79          82.58          77.77          66.86          48.23          44.22          41.93          37.20          38.03          42.40          40.67      
    3     2.36          55.58          70.25          31.95          66.11          70.09          63.53          57.51          55.66          53.53          56.56          47.67          49.32      
    4     3.06          36.90          54.66          39.71          51.14          64.81          53.13          54.24          54.01          51.43          48.59          48.83          45.25      
    5     3.69          48.03          45.40          40.72          51.88          52.28          63.51          57.11          66.53          64.88          64.70          55.24          58.43      
    6     1.94          43.38          41.16          48.45          42.17          54.42          42.23          48.52          46.48          46.66          53.08          51.55          46.34      
    7     1.71          43.63          45.70          46.59          45.38          46.27          44.19          48.52          46.97          51.35          52.94          54.41          46.94      
    8     1.73          46.78          46.70          49.24          42.05          52.70          41.55          43.13          47.53          49.06          52.19          52.75          47.64      
    9     1.69          45.03          38.86          47.12          39.80          50.45          44.79          45.27          45.88          47.57          52.12          55.96          48.61      
   10     2.65          39.45          40.28          45.84          39.95          48.02          41.72          37.84          46.17          40.61          53.23          56.52          54.07      
   11     1.76          42.40          33.98          36.72          37.73          39.60          34.68          43.23          41.13          43.27          41.27          46.83          49.22      
   12     2.23          40.84          37.07          41.02          38.93          46.19          37.48          40.96          42.29          39.68          47.95          50.83          46.28      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11             12      
    0    95.71          193.39          168.06          150.97          114.18          130.81          86.82          76.47          84.98          80.66          90.46          87.65          78.17      
    1    193.39          192.91          178.97          167.68          125.73          150.00          120.72          118.81          125.83          119.70          117.99          123.59          112.07      
    2    168.06          178.97          93.58          152.83          132.43          112.26          89.38          89.91          88.62          76.06          78.31          76.37          77.74      
    3    150.97          167.68          152.83          63.90          105.82          110.81          111.98          104.10          104.90          100.66          102.40          84.39          90.34      
    4    114.18          125.73          132.43          105.82          102.28          116.69          95.31          99.62          96.06          91.23          88.53          86.56          84.18      
    5    130.81          150.00          112.26          110.81          116.69          104.56          117.93          103.38          119.23          115.32          112.72          94.84          104.62      
    6    86.82          120.72          89.38          111.98          95.31          117.93          84.45          92.71          88.03          91.44          94.79          86.23          83.82      
    7    76.47          118.81          89.91          104.10          99.62          103.38          92.71          97.04          90.10          96.63          90.78          97.63          87.90      
    8    84.98          125.83          88.62          104.90          96.06          119.23          88.03          90.10          95.06          94.94          98.36          93.88          89.93      
    9    80.66          119.70          76.06          100.66          91.23          115.32          91.44          96.63          94.94          95.14          92.73          99.22          88.29      
   10    90.46          117.99          78.31          102.40          88.53          112.72          94.79          90.78          98.36          92.73          106.45          97.80          102.02      
   11    87.65          123.59          76.37          84.39          86.56          94.84          86.23          97.63          93.88          99.22          97.80          93.66          100.04      
   12    78.17          112.07          77.74          90.34          84.18          104.62          83.82          87.90          89.93          88.29          102.02          100.04          92.56      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0001  0.0001  0.0001  0.0007  0.0001  0.0001  0.0003  0.0001  0.0273  0.7224  0.2485  
Player #1: 0.0001  0.0001  0.0001  0.0001  0.0001  0.0007  0.0001  0.0001  0.0003  0.0001  0.0273  0.7224  0.2485  
Iteration : 12
Time so far: 120070.20357584953
Approximating Best Response
Training best response:  True 0.04000000000000001
rtg values check:  (?,) (?, 1)
2023-07-18 05:07:18.482567: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_90/weights_2/Adam_1/Assign' id:25581 op device:{requested: '', assigned: ''} def:{{{node mlp_90/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_90/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_90/weights_2/Adam_1, mlp_90/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0203876843675971 54.7276496887207 0.38942977488040925 12.057590770721436 10487 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03306173831224442 15.206135368347168 0.6803787112236023 10.253749561309814 217987 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029301744885742664 20.395293998718262 0.6349152207374573 10.326650714874267 423721 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02804840337485075 24.351588249206543 0.5735720217227935 10.918418407440186 633428 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028639395348727704 14.15221939086914 0.7134105384349823 9.867487812042237 843066 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025396423973143102 15.803365135192871 0.6676404595375061 10.101600742340088 1051845 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018738254718482495 14.018191146850587 0.7173151791095733 9.675889778137208 1259645 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022007143683731557 22.256576347351075 0.6694207906723022 10.421711730957032 1466797 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0194711497053504 16.65045032501221 0.6366858959197998 10.44175100326538 1675950 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017991979140788317 14.95511245727539 0.6590733587741852 10.094145202636719 1886384 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015108594484627248 18.67760944366455 0.6288909673690796 10.38989200592041 2094769 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015433928184211253 9.985103511810303 0.6840529918670655 10.034231662750244 2304558 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013997235707938671 10.595336627960204 0.7052602052688599 9.641081142425538 2512152 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011909214034676551 14.153866195678711 0.6631255090236664 9.944233226776124 2723381 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00861851186491549 12.263617134094238 0.6375648021697998 10.775145626068115 2932121 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008382995566353201 27.819080352783203 0.645968908071518 10.429213047027588 3140692 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005236663529649377 19.13085594177246 0.6086304873228073 10.60229263305664 3349085 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004718422424048185 15.34398832321167 0.6905569911003113 9.803350639343261 3559513 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002518714079633355 11.043271923065186 0.7395416378974915 9.778910064697266 3767450 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00064012350194389 15.538204574584961 0.7024929344654083 9.890161323547364 3977781 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -8.693868239788572e-05 15.585078144073487 0.1569841980934143 10.854141998291016 4186255 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001396091072820127 14.800376892089844 0.11813907250761986 10.951424598693848 4395139 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -4.8422347026644276e-05 14.31081600189209 0.2338590294122696 10.904183292388916 4604897 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005998114040266956 17.323197078704833 0.3555400252342224 10.726304817199708 4809630 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -4.7253817319869996e-05 13.094255065917968 0.2924098610877991 10.768329906463624 5017679 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008963325875811279 20.376169967651368 0.15873027145862578 11.363662147521973 5225155 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00012789115426130592 18.32687911987305 0.31503532230854037 10.872785568237305 5435818 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012630285207706037 13.32639513015747 0.13697222471237183 11.333431816101074 5645028 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -4.345218767412007e-05 12.638726615905762 0.17302553951740265 11.399109172821046 5852715 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -8.652222022647038e-05 14.528920364379882 0.19244083315134047 11.71374578475952 6061870 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021335968340281397 12.729828453063964 0.09105511978268624 11.9204026222229 6268992 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002706718034460209 13.493869209289551 0.058248625695705415 12.239606857299805 6476485 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005534097464988009 27.420993995666503 0.03446457833051682 13.078794384002686 6683908 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005770867748651654 13.692208671569825 0.03902527503669262 12.493073749542237 6892473 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009021698409924283 17.806089019775392 0.047564811818301675 12.729828071594238 7100487 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006314386591839138 15.577541923522949 0.07115768007934094 12.129435348510743 7308606 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004525032636593096 11.849821090698242 0.08898042067885399 12.689575576782227 7517262 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00016918950859690085 14.546016502380372 0.09599390178918839 12.636602210998536 7727578 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  7.869542605476454e-06 26.45865650177002 0.09892608150839806 13.048005199432373 7935644 0


Pure best response payoff estimated to be 59.275 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (13, 0)
Current player 0 and current strategies (13, 0) took 44.81 seconds to finish estimate with resulting utilities: [88.71  2.19]
Estimating current strategies:  (13, 1)
Current player 0 and current strategies (13, 1) took 98.35 seconds to finish estimate with resulting utilities: [77.6  45.97]
Estimating current strategies:  (13, 2)
Current player 0 and current strategies (13, 2) took 126.8 seconds to finish estimate with resulting utilities: [35.91  34.405]
Estimating current strategies:  (13, 3)
Current player 0 and current strategies (13, 3) took 118.23 seconds to finish estimate with resulting utilities: [51.97 43.66]
Estimating current strategies:  (13, 4)
Current player 0 and current strategies (13, 4) took 114.84 seconds to finish estimate with resulting utilities: [48.69 41.92]
Estimating current strategies:  (13, 5)
Current player 0 and current strategies (13, 5) took 110.89 seconds to finish estimate with resulting utilities: [60.51 46.68]
Estimating current strategies:  (13, 6)
Current player 0 and current strategies (13, 6) took 91.14 seconds to finish estimate with resulting utilities: [47.445 38.065]
Estimating current strategies:  (13, 7)
Current player 0 and current strategies (13, 7) took 83.64 seconds to finish estimate with resulting utilities: [51.225 42.5  ]
Estimating current strategies:  (13, 8)
Current player 0 and current strategies (13, 8) took 88.81 seconds to finish estimate with resulting utilities: [53.45 48.11]
Estimating current strategies:  (13, 9)
Current player 0 and current strategies (13, 9) took 89.89 seconds to finish estimate with resulting utilities: [53.07 47.91]
Estimating current strategies:  (13, 10)
Current player 0 and current strategies (13, 10) took 91.36 seconds to finish estimate with resulting utilities: [54.81  50.925]
Estimating current strategies:  (13, 11)
Current player 0 and current strategies (13, 11) took 91.93 seconds to finish estimate with resulting utilities: [54.965 52.025]
Estimating current strategies:  (13, 12)
Current player 0 and current strategies (13, 12) took 82.55 seconds to finish estimate with resulting utilities: [43.755 42.87 ]
Estimating current strategies:  (13, 13)
Current player 0 and current strategies (13, 13) took 94.88 seconds to finish estimate with resulting utilities: [49.87  49.715]
Computing meta_strategies
Exited RRD with total regret 0.9974920808226528 that was less than regret lambda 1.0000000000000004 after 1221 iterations 
REGRET STEPS:  25
NEW LAMBDA 0.9166666666666671
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11             12             13      
    0    47.85           3.83           2.25           2.36           3.06           3.69           1.94           1.71           1.73           1.69           2.65           1.76           2.23           2.19      
    1    189.56          96.45          53.25          55.58          36.90          48.03          43.38          43.63          46.78          45.03          39.45          42.40          40.84          45.97      
    2    165.81          125.72          46.79          70.25          54.66          45.40          41.16          45.70          46.70          38.86          40.28          33.98          37.07          34.41      
    3    148.61          112.09          82.58          31.95          39.71          40.72          48.45          46.59          49.24          47.12          45.84          36.72          41.02          43.66      
    4    111.11          88.83          77.77          66.11          51.14          51.88          42.17          45.38          42.05          39.80          39.95          37.73          38.93          41.92      
    5    127.11          101.96          66.86          70.09          64.81          52.28          54.42          46.27          52.70          50.45          48.02          39.60          46.19          46.68      
    6    84.89          77.33          48.23          63.53          53.13          63.51          42.23          44.19          41.55          44.79          41.72          34.68          37.48          38.06      
    7    74.77          75.18          44.22          57.51          54.24          57.11          48.52          48.52          43.13          45.27          37.84          43.23          40.96          42.50      
    8    83.25          79.05          41.93          55.66          54.01          66.53          46.48          46.97          47.53          45.88          46.17          41.13          42.29          48.11      
    9    78.97          74.67          37.20          53.53          51.43          64.88          46.66          51.35          49.06          47.57          40.61          43.27          39.68          47.91      
   10    87.82          78.54          38.03          56.56          48.59          64.70          53.08          52.94          52.19          52.12          53.23          41.27          47.95          50.92      
   11    85.89          81.19          42.40          47.67          48.83          55.24          51.55          54.41          52.75          55.96          56.52          46.83          50.83          52.02      
   12    75.94          71.23          40.67          49.32          45.25          58.43          46.34          46.94          47.64          48.61          54.07          49.22          46.28          42.87      
   13    88.71          77.60          35.91          51.97          48.69          60.51          47.45          51.23          53.45          53.07          54.81          54.97          43.76          49.79      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11             12             13      
    0    47.85          189.56          165.81          148.61          111.11          127.11          84.89          74.77          83.25          78.97          87.82          85.89          75.94          88.71      
    1     3.83          96.45          125.72          112.09          88.83          101.96          77.33          75.18          79.05          74.67          78.54          81.19          71.23          77.60      
    2     2.25          53.25          46.79          82.58          77.77          66.86          48.23          44.22          41.93          37.20          38.03          42.40          40.67          35.91      
    3     2.36          55.58          70.25          31.95          66.11          70.09          63.53          57.51          55.66          53.53          56.56          47.67          49.32          51.97      
    4     3.06          36.90          54.66          39.71          51.14          64.81          53.13          54.24          54.01          51.43          48.59          48.83          45.25          48.69      
    5     3.69          48.03          45.40          40.72          51.88          52.28          63.51          57.11          66.53          64.88          64.70          55.24          58.43          60.51      
    6     1.94          43.38          41.16          48.45          42.17          54.42          42.23          48.52          46.48          46.66          53.08          51.55          46.34          47.45      
    7     1.71          43.63          45.70          46.59          45.38          46.27          44.19          48.52          46.97          51.35          52.94          54.41          46.94          51.23      
    8     1.73          46.78          46.70          49.24          42.05          52.70          41.55          43.13          47.53          49.06          52.19          52.75          47.64          53.45      
    9     1.69          45.03          38.86          47.12          39.80          50.45          44.79          45.27          45.88          47.57          52.12          55.96          48.61          53.07      
   10     2.65          39.45          40.28          45.84          39.95          48.02          41.72          37.84          46.17          40.61          53.23          56.52          54.07          54.81      
   11     1.76          42.40          33.98          36.72          37.73          39.60          34.68          43.23          41.13          43.27          41.27          46.83          49.22          54.97      
   12     2.23          40.84          37.07          41.02          38.93          46.19          37.48          40.96          42.29          39.68          47.95          50.83          46.28          43.76      
   13     2.19          45.97          34.41          43.66          41.92          46.68          38.06          42.50          48.11          47.91          50.92          52.02          42.87          49.79      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11             12             13      
    0    95.71          193.39          168.06          150.97          114.18          130.81          86.82          76.47          84.98          80.66          90.46          87.65          78.17          90.90      
    1    193.39          192.91          178.97          167.68          125.73          150.00          120.72          118.81          125.83          119.70          117.99          123.59          112.07          123.57      
    2    168.06          178.97          93.58          152.83          132.43          112.26          89.38          89.91          88.62          76.06          78.31          76.37          77.74          70.31      
    3    150.97          167.68          152.83          63.90          105.82          110.81          111.98          104.10          104.90          100.66          102.40          84.39          90.34          95.63      
    4    114.18          125.73          132.43          105.82          102.28          116.69          95.31          99.62          96.06          91.23          88.53          86.56          84.18          90.61      
    5    130.81          150.00          112.26          110.81          116.69          104.56          117.93          103.38          119.23          115.32          112.72          94.84          104.62          107.19      
    6    86.82          120.72          89.38          111.98          95.31          117.93          84.45          92.71          88.03          91.44          94.79          86.23          83.82          85.51      
    7    76.47          118.81          89.91          104.10          99.62          103.38          92.71          97.04          90.10          96.63          90.78          97.63          87.90          93.72      
    8    84.98          125.83          88.62          104.90          96.06          119.23          88.03          90.10          95.06          94.94          98.36          93.88          89.93          101.56      
    9    80.66          119.70          76.06          100.66          91.23          115.32          91.44          96.63          94.94          95.14          92.73          99.22          88.29          100.98      
   10    90.46          117.99          78.31          102.40          88.53          112.72          94.79          90.78          98.36          92.73          106.45          97.80          102.02          105.73      
   11    87.65          123.59          76.37          84.39          86.56          94.84          86.23          97.63          93.88          99.22          97.80          93.66          100.04          106.99      
   12    78.17          112.07          77.74          90.34          84.18          104.62          83.82          87.90          89.93          88.29          102.02          100.04          92.56          86.62      
   13    90.90          123.57          70.31          95.63          90.61          107.19          85.51          93.72          101.56          100.98          105.73          106.99          86.62          99.59      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.0001  0.0001  0.0001  0.004  0.0001  0.0001  0.0036  0.0012  0.0723  0.2751  0.0033  0.6398  
Player #1: 0.0001  0.0001  0.0001  0.0001  0.0001  0.004  0.0001  0.0001  0.0036  0.0012  0.0723  0.2751  0.0033  0.6398  
Iteration : 13
Time so far: 131500.4900932312
Approximating Best Response
Training best response:  True 0.034999999999999996
rtg values check:  (?,) (?, 1)
2023-07-18 08:17:48.295144: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_97/weights_2/Adam_1/Assign' id:27559 op device:{requested: '', assigned: ''} def:{{{node mlp_97/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_97/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_97/weights_2/Adam_1, mlp_97/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02270926609635353 57.3045539855957 0.4377621352672577 12.151750087738037 10672 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03172497097402811 13.493404197692872 0.664606773853302 11.430647468566894 216551 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030254139751195907 15.242374324798584 0.6574432909488678 10.7348952293396 422739 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03008474111557007 13.501222324371337 0.6871495008468628 10.700891876220703 632832 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029056215658783913 14.216083908081055 0.721394520998001 10.357122993469238 840613 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024706085678189993 19.800009155273436 0.6624080777168274 10.88663330078125 1051063 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02391600180417299 17.528932762145995 0.6767526686191558 10.59385414123535 1256509 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022880869172513484 14.446669101715088 0.6831335544586181 10.604293060302734 1464952 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020335488952696323 16.34051513671875 0.671604460477829 10.412772369384765 1675613 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019756733998656274 13.882483673095702 0.7057329773902893 10.176947784423827 1883110 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017525828443467617 17.807460975646972 0.71204873919487 10.179531383514405 2093006 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014360486809164286 16.492626667022705 0.6561333417892456 10.46266736984253 2301865 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015104277711361646 16.22472162246704 0.7251303374767304 10.102409648895264 2513338 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011055353935807944 16.25912265777588 0.6639748394489289 10.288142871856689 2723769 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009183305641636252 18.099980449676515 0.6560837984085083 10.36789026260376 2931425 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008670963998883962 13.469515800476074 0.6319827973842621 10.64446792602539 3139863 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0063896860461682085 15.6705828666687 0.6877923309803009 10.400657463073731 3346633 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004005418042652309 15.232821559906006 0.6897875249385834 10.29647512435913 3554946 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002385701221646741 30.58612766265869 0.6366311848163605 10.96286973953247 3762506 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019083947059698402 12.926590824127198 0.6881219208240509 10.559768676757812 3971550 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007321946235606447 15.207068061828613 0.4050166517496109 10.82899980545044 4180083 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008443665341474116 18.107777690887453 0.3050251990556717 10.792914199829102 4389861 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00218225493299542 19.73568525314331 0.28328130543231966 11.334215259552002 4597658 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00011016229218512308 15.071888542175293 0.29317247718572614 11.056724452972412 4807785 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007421893104037735 21.067197608947755 0.24386416226625443 11.111572074890137 5017158 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00010210729597019963 21.889749336242676 0.3093429446220398 11.143917560577393 5226080 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00040039647137746215 17.283017253875734 0.4934265226125717 10.785366916656494 5434811 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000818186328979209 14.771308994293213 0.2524460285902023 10.824837112426758 5641200 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006118617937318049 20.3643648147583 0.08528770506381989 11.42992582321167 5850156 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001678130881555262 17.248544692993164 0.0997204564511776 11.51694040298462 6056867 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005353403437766246 13.451230430603028 0.3287125051021576 10.872836685180664 6264389 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00041416897875024006 17.068152236938477 0.14845327585935592 11.43205099105835 6471279 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0002095762265525991 17.22576770782471 0.3589332431554794 11.250560665130616 6680856 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006409565816284157 16.02256546020508 0.4132254242897034 10.986346530914307 6888143 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009784679721633438 13.766566944122314 0.32218735218048095 11.161641693115234 7098016 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006468550407589646 19.007677841186524 0.3814576804637909 11.886540794372559 7306717 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007200825668405742 14.07115888595581 0.325007888674736 11.21129035949707 7515952 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  4.069454735144973e-06 16.690039348602294 0.10094049572944641 11.643981170654296 7726122 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0011425158052588813 20.59340705871582 0.2674615040421486 11.59989423751831 7934311 0


Pure best response payoff estimated to be 69.35 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (14, 0)
Current player 0 and current strategies (14, 0) took 55.73 seconds to finish estimate with resulting utilities: [108.18   3.79]
Estimating current strategies:  (14, 1)
Current player 0 and current strategies (14, 1) took 114.17 seconds to finish estimate with resulting utilities: [90.43  49.975]
Estimating current strategies:  (14, 2)
Current player 0 and current strategies (14, 2) took 135.63 seconds to finish estimate with resulting utilities: [42.91  44.395]
Estimating current strategies:  (14, 3)
Current player 0 and current strategies (14, 3) took 124.68 seconds to finish estimate with resulting utilities: [54.605 43.3  ]
Estimating current strategies:  (14, 4)
Current player 0 and current strategies (14, 4) took 120.14 seconds to finish estimate with resulting utilities: [56.025 44.98 ]
Estimating current strategies:  (14, 5)
Current player 0 and current strategies (14, 5) took 117.95 seconds to finish estimate with resulting utilities: [66.795 49.925]
Estimating current strategies:  (14, 6)
Current player 0 and current strategies (14, 6) took 98.04 seconds to finish estimate with resulting utilities: [61.69 43.02]
Estimating current strategies:  (14, 7)
Current player 0 and current strategies (14, 7) took 90.54 seconds to finish estimate with resulting utilities: [67.08  47.555]
Estimating current strategies:  (14, 8)
Current player 0 and current strategies (14, 8) took 93.29 seconds to finish estimate with resulting utilities: [65.69  48.785]
Estimating current strategies:  (14, 9)
Current player 0 and current strategies (14, 9) took 93.14 seconds to finish estimate with resulting utilities: [64.505 47.805]
Estimating current strategies:  (14, 10)
Current player 0 and current strategies (14, 10) took 98.39 seconds to finish estimate with resulting utilities: [67.415 51.88 ]
Estimating current strategies:  (14, 11)
Current player 0 and current strategies (14, 11) took 93.26 seconds to finish estimate with resulting utilities: [61.12  52.995]
Estimating current strategies:  (14, 12)
Current player 0 and current strategies (14, 12) took 87.98 seconds to finish estimate with resulting utilities: [59.03 49.91]
Estimating current strategies:  (14, 13)
Current player 0 and current strategies (14, 13) took 92.5 seconds to finish estimate with resulting utilities: [65.21 52.48]
Estimating current strategies:  (14, 14)
Current player 0 and current strategies (14, 14) took 105.48 seconds to finish estimate with resulting utilities: [68.065 67.865]
Computing meta_strategies
Exited RRD with total regret 0.9022274022206318 that was less than regret lambda 0.9166666666666671 after 404 iterations 
REGRET STEPS:  25
NEW LAMBDA 0.8333333333333337
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11             12             13             14      
    0    47.85           3.83           2.25           2.36           3.06           3.69           1.94           1.71           1.73           1.69           2.65           1.76           2.23           2.19           3.79      
    1    189.56          96.45          53.25          55.58          36.90          48.03          43.38          43.63          46.78          45.03          39.45          42.40          40.84          45.97          49.98      
    2    165.81          125.72          46.79          70.25          54.66          45.40          41.16          45.70          46.70          38.86          40.28          33.98          37.07          34.41          44.40      
    3    148.61          112.09          82.58          31.95          39.71          40.72          48.45          46.59          49.24          47.12          45.84          36.72          41.02          43.66          43.30      
    4    111.11          88.83          77.77          66.11          51.14          51.88          42.17          45.38          42.05          39.80          39.95          37.73          38.93          41.92          44.98      
    5    127.11          101.96          66.86          70.09          64.81          52.28          54.42          46.27          52.70          50.45          48.02          39.60          46.19          46.68          49.92      
    6    84.89          77.33          48.23          63.53          53.13          63.51          42.23          44.19          41.55          44.79          41.72          34.68          37.48          38.06          43.02      
    7    74.77          75.18          44.22          57.51          54.24          57.11          48.52          48.52          43.13          45.27          37.84          43.23          40.96          42.50          47.55      
    8    83.25          79.05          41.93          55.66          54.01          66.53          46.48          46.97          47.53          45.88          46.17          41.13          42.29          48.11          48.78      
    9    78.97          74.67          37.20          53.53          51.43          64.88          46.66          51.35          49.06          47.57          40.61          43.27          39.68          47.91          47.80      
   10    87.82          78.54          38.03          56.56          48.59          64.70          53.08          52.94          52.19          52.12          53.23          41.27          47.95          50.92          51.88      
   11    85.89          81.19          42.40          47.67          48.83          55.24          51.55          54.41          52.75          55.96          56.52          46.83          50.83          52.02          52.99      
   12    75.94          71.23          40.67          49.32          45.25          58.43          46.34          46.94          47.64          48.61          54.07          49.22          46.28          42.87          49.91      
   13    88.71          77.60          35.91          51.97          48.69          60.51          47.45          51.23          53.45          53.07          54.81          54.97          43.76          49.79          52.48      
   14    108.18          90.43          42.91          54.60          56.02          66.80          61.69          67.08          65.69          64.50          67.42          61.12          59.03          65.21          67.97      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11             12             13             14      
    0    47.85          189.56          165.81          148.61          111.11          127.11          84.89          74.77          83.25          78.97          87.82          85.89          75.94          88.71          108.18      
    1     3.83          96.45          125.72          112.09          88.83          101.96          77.33          75.18          79.05          74.67          78.54          81.19          71.23          77.60          90.43      
    2     2.25          53.25          46.79          82.58          77.77          66.86          48.23          44.22          41.93          37.20          38.03          42.40          40.67          35.91          42.91      
    3     2.36          55.58          70.25          31.95          66.11          70.09          63.53          57.51          55.66          53.53          56.56          47.67          49.32          51.97          54.60      
    4     3.06          36.90          54.66          39.71          51.14          64.81          53.13          54.24          54.01          51.43          48.59          48.83          45.25          48.69          56.02      
    5     3.69          48.03          45.40          40.72          51.88          52.28          63.51          57.11          66.53          64.88          64.70          55.24          58.43          60.51          66.80      
    6     1.94          43.38          41.16          48.45          42.17          54.42          42.23          48.52          46.48          46.66          53.08          51.55          46.34          47.45          61.69      
    7     1.71          43.63          45.70          46.59          45.38          46.27          44.19          48.52          46.97          51.35          52.94          54.41          46.94          51.23          67.08      
    8     1.73          46.78          46.70          49.24          42.05          52.70          41.55          43.13          47.53          49.06          52.19          52.75          47.64          53.45          65.69      
    9     1.69          45.03          38.86          47.12          39.80          50.45          44.79          45.27          45.88          47.57          52.12          55.96          48.61          53.07          64.50      
   10     2.65          39.45          40.28          45.84          39.95          48.02          41.72          37.84          46.17          40.61          53.23          56.52          54.07          54.81          67.42      
   11     1.76          42.40          33.98          36.72          37.73          39.60          34.68          43.23          41.13          43.27          41.27          46.83          49.22          54.97          61.12      
   12     2.23          40.84          37.07          41.02          38.93          46.19          37.48          40.96          42.29          39.68          47.95          50.83          46.28          43.76          59.03      
   13     2.19          45.97          34.41          43.66          41.92          46.68          38.06          42.50          48.11          47.91          50.92          52.02          42.87          49.79          65.21      
   14     3.79          49.98          44.40          43.30          44.98          49.92          43.02          47.55          48.78          47.80          51.88          52.99          49.91          52.48          67.97      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11             12             13             14      
    0    95.71          193.39          168.06          150.97          114.18          130.81          86.82          76.47          84.98          80.66          90.46          87.65          78.17          90.90          111.97      
    1    193.39          192.91          178.97          167.68          125.73          150.00          120.72          118.81          125.83          119.70          117.99          123.59          112.07          123.57          140.41      
    2    168.06          178.97          93.58          152.83          132.43          112.26          89.38          89.91          88.62          76.06          78.31          76.37          77.74          70.31          87.31      
    3    150.97          167.68          152.83          63.90          105.82          110.81          111.98          104.10          104.90          100.66          102.40          84.39          90.34          95.63          97.91      
    4    114.18          125.73          132.43          105.82          102.28          116.69          95.31          99.62          96.06          91.23          88.53          86.56          84.18          90.61          101.00      
    5    130.81          150.00          112.26          110.81          116.69          104.56          117.93          103.38          119.23          115.32          112.72          94.84          104.62          107.19          116.72      
    6    86.82          120.72          89.38          111.98          95.31          117.93          84.45          92.71          88.03          91.44          94.79          86.23          83.82          85.51          104.71      
    7    76.47          118.81          89.91          104.10          99.62          103.38          92.71          97.04          90.10          96.63          90.78          97.63          87.90          93.72          114.63      
    8    84.98          125.83          88.62          104.90          96.06          119.23          88.03          90.10          95.06          94.94          98.36          93.88          89.93          101.56          114.47      
    9    80.66          119.70          76.06          100.66          91.23          115.32          91.44          96.63          94.94          95.14          92.73          99.22          88.29          100.98          112.31      
   10    90.46          117.99          78.31          102.40          88.53          112.72          94.79          90.78          98.36          92.73          106.45          97.80          102.02          105.73          119.30      
   11    87.65          123.59          76.37          84.39          86.56          94.84          86.23          97.63          93.88          99.22          97.80          93.66          100.04          106.99          114.11      
   12    78.17          112.07          77.74          90.34          84.18          104.62          83.82          87.90          89.93          88.29          102.02          100.04          92.56          86.62          108.94      
   13    90.90          123.57          70.31          95.63          90.61          107.19          85.51          93.72          101.56          100.98          105.73          106.99          86.62          99.59          117.69      
   14    111.97          140.41          87.31          97.91          101.00          116.72          104.71          114.63          114.47          112.31          119.30          114.11          108.94          117.69          135.93      

 

Metagame probabilities: 
Player #0: 0.0001  0.0013  0.0004  0.0003  0.0005  0.0044  0.0002  0.0007  0.0015  0.0009  0.0042  0.0055  0.0015  0.0047  0.9738  
Player #1: 0.0001  0.0013  0.0004  0.0003  0.0005  0.0044  0.0002  0.0007  0.0015  0.0009  0.0042  0.0055  0.0015  0.0047  0.9738  
Iteration : 14
Time so far: 142632.9994161129
Approximating Best Response
Training best response:  True 0.030000000000000006
rtg values check:  (?,) (?, 1)
2023-07-18 11:23:20.654733: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_104/weights_2/Adam_1/Assign' id:29537 op device:{requested: '', assigned: ''} def:{{{node mlp_104/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_104/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_104/weights_2/Adam_1, mlp_104/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03255260344594717 57.96427230834961 0.6391182780265808 10.252657318115235 10678 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031547810323536395 16.782709693908693 0.659525203704834 10.239977169036866 215688 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03490825034677982 14.42306022644043 0.7706120848655701 9.741373825073243 426053 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02882972117513418 18.980169105529786 0.6765379190444947 10.244058895111085 636962 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03042471017688513 24.25597515106201 0.7439250349998474 9.969131660461425 846511 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024899820424616338 16.869764423370363 0.6594557404518128 10.121442699432373 1055607 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020350473932921886 16.993814945220947 0.5717644333839417 10.945620346069337 1263265 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020372267812490463 18.234665298461913 0.6620137929916382 10.155153179168702 1471367 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02063205875456333 15.04962797164917 0.6256846189498901 10.801053619384765 1680624 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01852587340399623 13.970842361450195 0.6495387375354766 10.10639991760254 1891847 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018258638307452203 14.489120960235596 0.7053422033786774 10.128830242156983 2100846 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013641182333230972 18.11339340209961 0.6205663859844208 10.776442623138427 2311747 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013507356960326433 28.689306259155273 0.6815560698509217 10.267706394195557 2519562 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013034113217145205 15.187553024291992 0.7416610836982727 9.97686185836792 2728141 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008888432290405035 17.18319444656372 0.6612474620342255 10.180837821960449 2937544 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009074520785361528 20.487047004699708 0.6395875036716461 10.204312419891357 3145987 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007273249141871929 14.155755710601806 0.6692318975925445 10.333553981781005 3355326 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0034943542676046492 18.494274616241455 0.5868448734283447 10.470125484466553 3562603 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0032143431366421283 15.660069274902344 0.5135966300964355 11.355432033538818 3771694 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004266024916432798 15.133895301818848 0.6064005613327026 10.600857543945313 3979988 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0006937183119589463 16.36802473068237 0.5403767585754394 11.152982711791992 4187596 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00039759131977916694 17.80924825668335 0.40424416661262513 11.064863204956055 4399552 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005223603016929701 15.928976345062257 0.31285169422626496 11.930311298370361 4607124 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00041773889679461715 15.428926944732666 0.20333463251590728 11.54561185836792 4816638 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010035912622697652 14.502233123779297 0.270975349843502 11.08472032546997 5025348 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005498373622685904 20.68710403442383 0.17018491476774217 11.359613990783691 5234138 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0005300510427332483 15.930087184906006 0.4079350084066391 11.16432523727417 5444631 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  5.3007956012152135e-05 16.470226669311522 0.2249236062169075 11.833599376678468 5654347 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -2.9307135264389218e-06 20.14585933685303 0.36559211313724516 11.384355163574218 5861843 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00026021550002042204 22.305127334594726 0.13658182621002196 12.03709716796875 6071570 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007283797996933572 14.246588802337646 0.23534686714410782 12.483907318115234 6282006 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004471554813790135 15.476265144348144 0.17099228799343108 12.02466869354248 6493309 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00038804965588497 16.30471076965332 0.06898706331849098 13.089760875701904 6703241 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00040362250292673706 19.4501615524292 0.06268508210778237 12.401050281524657 6910682 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00037374085659394043 19.625447845458986 0.06499802768230438 12.134153461456298 7119097 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005545081687159837 15.312475681304932 0.19923395663499832 11.48408489227295 7327761 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014313162595499308 17.22525405883789 0.16912319809198378 11.52602310180664 7537846 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -1.7227019998244942e-05 14.43504066467285 0.15239330232143403 12.03507595062256 7747532 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00078228435741039 16.162848567962648 0.12422997429966927 12.074561882019044 7957850 0


Pure best response payoff estimated to be 86.26 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (15, 0)
Current player 0 and current strategies (15, 0) took 62.65 seconds to finish estimate with resulting utilities: [124.36   3.82]
Estimating current strategies:  (15, 1)
Fatal Python error: Segmentation fault

Current thread 0x000014b12b3f7b80 (most recent call first):
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 83 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 84 in sample_episode
  ...

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, google._upb._message, tensorflow.python.framework.fast_tensor_util, charset_normalizer.md, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, PIL._imaging, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label, cvxopt.base, cvxopt.blas, cvxopt.lapack (total: 117)
/var/spool/slurmd.spool/job55931177/slurm_script: line 34: 228494 Segmentation fault      (core dumped) python3 -u psro_v2_simultaneous.py --game_name=harvest --save_folder_path="../examples/data/harvest/vanilla_ppo_full/($trial)_$parameter_id" --meta_strategy_method=prd_collab --gpsro_iterations=30 --consensus_imitation=False --regret_lambda_init=$curr_lambda --regret_steps=$curr_steps --policy_constraint_steps=20 --joint_action=False --rewards_joint=False --fine_tune=True --clear_trajectories=False --consensus_hidden_layer_size=50 --consensus_n_hidden_layers=2 --min_buffer_size_fine_tune=10000 --epochs_ppo=10 --minibatches_ppo=50 --number_training_steps=8000000 --symmetric_game=True --sims_per_entry=200 --eps_clip=.1 --eps_clip_value=.5 --entropy_decay_duration=.5 --ppo_entropy=.05 --fine_tune_policy_lr=.00003 --fine_tune_value_lr=.0003 --transfer_policy_minimum_entropy=0 --recovery_window=200 --hidden_layer_size=50 --n_hidden_layers=2 --batch_size=64 --dqn_learning_rate=3e-4 --update_target_network_every=500 --learn_every=1 --max_buffer_size=100000 --epsilon_decay_duration=100000 --min_buffer_size_to_learn=20000 --regret_calculation_steps=1500000
