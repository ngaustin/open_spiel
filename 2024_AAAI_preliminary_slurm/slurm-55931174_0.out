Job Id listed below:
55931175

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

2023-07-16 19:45:42.399681: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-16 19:45:43.392267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0716 19:45:44.975865 22671330917248 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x149e4b60ed10>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x149e4b60ed10>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-07-16 19:45:45.289451: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-07-16 19:45:45.580676: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.95 seconds to finish estimate with resulting utilities: [49.4   48.465]
Exited RRD with total regret 0.0 that was less than regret lambda 2.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    48.93      

 

Player 1 Payoff matrix: 

           0      
    0    48.93      

 

Social Welfare Sum Matrix: 

           0      
    0    97.87      

 

Iteration : 0
Time so far: 0.0001895427703857422
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-07-16 19:46:05.506206: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.11993665620684624 23.29442596435547 2.052468013763428 0.001580696424935013 10856 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.10269200131297111 17.603320789337157 1.9081853866577148 0.20060485154390334 217786 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09092026725411415 15.755138969421386 1.867839777469635 0.29359530806541445 420606 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0861215315759182 17.927077102661134 1.847667646408081 0.3778697490692139 622088 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08097460344433785 16.46195020675659 1.7944138407707215 0.5017634451389312 825411 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07502763718366623 15.850963497161866 1.7719249367713927 0.5685669839382171 1028036 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06068526208400726 16.881559085845947 1.6683769345283508 0.7678188800811767 1232356 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.061101696267724034 18.94448928833008 1.6058683395385742 1.0332119166851044 1436888 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05058685354888439 21.142838287353516 1.5120540618896485 1.2266433000564576 1642310 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0468018289655447 19.65068244934082 1.4911550641059876 1.4016329169273376 1850098 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035073240473866465 20.18959445953369 1.3906767249107361 1.567413854598999 2059201 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03485971055924893 17.806210136413576 1.373849904537201 1.6462157130241395 2267189 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027413180284202098 22.41521453857422 1.303087317943573 1.7990132451057435 2476732 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023214395344257354 21.86871471405029 1.2776243567466736 1.8621301651000977 2685839 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01866206917911768 23.734793853759765 1.2096482276916505 2.1079312801361083 2895836 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015797424502670764 21.682330513000487 1.1813621640205383 2.1883252382278444 3105407 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011024411208927632 23.634136772155763 1.0686076641082765 2.60536150932312 3314922 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007030878588557244 25.125732421875 1.025681209564209 2.844446396827698 3528901 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0036197865149006246 21.61337013244629 0.9705117583274842 3.150407862663269 3739564 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002277609068551101 26.246883392333984 0.8867500007152558 3.457861089706421 3953353 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016411772085120901 19.858460235595704 0.7890929222106934 3.8487533807754515 4168407 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016554958536289632 26.44210605621338 0.7338944673538208 4.103758597373963 4383505 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001999596692621708 26.096593284606932 0.6709531605243683 4.516878604888916 4601608 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014746310887858271 25.417522048950197 0.6261313915252685 4.812358427047729 4816986 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017578476050402968 25.997373390197755 0.55952388048172 5.068045902252197 5031026 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001313451068563154 25.040449142456055 0.5722438812255859 5.272920894622803 5246565 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009906968800351024 26.680545234680174 0.5335276067256928 5.855460214614868 5465904 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001811828336212784 23.11638984680176 0.5174738973379135 6.333886957168579 5684654 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008568069155444391 23.766203117370605 0.470047265291214 7.010571718215942 5901677 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009269874277379131 27.68213176727295 0.45965675711631776 7.21844801902771 6119858 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001821798994205892 23.997806358337403 0.41447238326072694 7.693100261688232 6339006 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00046223170356824995 31.07132740020752 0.4001218259334564 7.778965854644776 6555116 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006859892903776199 24.191736030578614 0.4249086260795593 7.916302061080932 6772240 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00036661558842752127 24.93485794067383 0.4031495273113251 8.048786544799805 6991457 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006646048346738098 24.181488609313966 0.37427385747432707 8.16237177848816 7206275 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006687907458399423 26.6904203414917 0.3787750959396362 8.260902309417725 7423138 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00152380262297811 25.101677894592285 0.3384663760662079 8.29938440322876 7642113 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0002189680002629757 24.588056564331055 0.34408503472805024 8.240352344512939 7860327 0


Pure best response payoff estimated to be 195.645 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 84.76 seconds to finish estimate with resulting utilities: [194.735   4.11 ]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 137.81 seconds to finish estimate with resulting utilities: [96.53 98.  ]
Computing meta_strategies
Exited RRD with total regret 1.8705718708121424 that was less than regret lambda 2.0 after 43 iterations 
REGRET STEPS:  15
NEW LAMBDA 1.8571428571428572
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    48.93           4.11      
    1    194.74          97.27      

 

Player 1 Payoff matrix: 

           0              1      
    0    48.93          194.74      
    1     4.11          97.27      

 

Social Welfare Sum Matrix: 

           0              1      
    0    97.87          198.85      
    1    198.85          194.53      

 

Metagame probabilities: 
Player #0: 0.01  0.99  
Player #1: 0.01  0.99  
Iteration : 1
Time so far: 6771.055196762085
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-07-16 21:38:56.662730: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02633522879332304 82.68467788696289 0.5100754857063293 8.980623340606689 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035634835436940196 13.720566082000733 0.726479297876358 8.11246371269226 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03416141532361507 15.934397220611572 0.7296085715293884 6.251541614532471 449919 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03468380142003298 17.64747018814087 0.8252511024475098 5.929212665557861 668108 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035583659261465075 23.269774436950684 0.8936371624469757 5.119363641738891 886764 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035593504458665846 14.229888248443604 0.9798736095428466 5.082657814025879 1105508 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03380905874073505 15.946882343292236 0.9631333649158478 5.178305053710938 1323490 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02980035003274679 17.20832815170288 0.9447142660617829 5.111219167709351 1542179 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02788937706500292 20.82816333770752 0.9481840014457703 5.299189901351928 1759066 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027010303363204 19.848861312866212 0.9940048336982727 4.988735342025757 1978669 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023380581103265286 16.778563022613525 1.0329107880592345 4.960536050796509 2197023 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022327031940221786 17.323820400238038 1.0106654703617095 4.8833061218261715 2414101 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018644450046122075 20.208055686950683 0.9767911851406097 4.938525867462158 2632083 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014419103786349297 20.35786018371582 0.9048187851905822 5.275623416900634 2849893 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01267028683796525 16.435167503356933 0.9183187305927276 5.358687782287598 3067530 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009437060356140137 19.969463729858397 0.8449295163154602 5.5582959175109865 3285886 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006356631964445114 16.850453758239745 0.8258707463741303 5.500124120712281 3505465 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0037776768440380692 20.187853050231933 0.8248352348804474 5.743812227249146 3725344 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017561607935931533 18.9304557800293 0.7855269730091095 5.906945848464966 3944759 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005602731101589598 18.36596155166626 0.7882895767688751 6.153049898147583 4163841 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001679017235073843 17.00337381362915 0.6751005589962006 6.469854688644409 4383088 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006914437137311325 17.35071392059326 0.6737354815006256 6.622254753112793 4603088 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007142180460505187 16.714559745788574 0.6610743582248688 6.805237913131714 4822721 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008172755828127265 21.20053482055664 0.5893482506275177 7.262187719345093 5042297 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005223314910836052 18.58746862411499 0.5570046544075012 7.915244197845459 5261596 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -9.34396346565336e-05 21.16691036224365 0.45388283431529997 8.239883422851562 5481596 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005706520656531211 17.38227891921997 0.36821829378604887 8.677833938598633 5701596 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0009068185929208994 20.29084129333496 0.39210219383239747 8.509905529022216 5921596 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000616334277583519 19.742744827270506 0.28698807656764985 8.638347625732422 6141596 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011907929030712694 23.73705177307129 0.3054273158311844 8.972387409210205 6361538 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007721520742052235 18.835853576660156 0.4335500180721283 8.977446842193604 6581538 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011866036482388153 17.101282787322997 0.45328415632247926 8.735018348693847 6801538 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010921145265456289 21.095100593566894 0.4500144600868225 9.05084400177002 7020961 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005532316336029908 18.700588035583497 0.43414114117622377 8.84973087310791 7240961 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -1.495182077633217e-05 20.72360143661499 0.34169618785381317 9.563196659088135 7460961 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006759808391507249 19.548464012145995 0.26146766245365144 9.570835304260253 7680961 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00013313184317667037 23.008618545532226 0.33450732827186586 9.516677284240723 7900961 0


Pure best response payoff estimated to be 129.325 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 84.32 seconds to finish estimate with resulting utilities: [185.065   2.835]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 141.03 seconds to finish estimate with resulting utilities: [130.45   56.355]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 140.32 seconds to finish estimate with resulting utilities: [34.95  37.085]
Computing meta_strategies
Exited RRD with total regret 1.8369506367749153 that was less than regret lambda 1.8571428571428572 after 94 iterations 
REGRET STEPS:  15
NEW LAMBDA 1.7142857142857144
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    48.93           4.11           2.83      
    1    194.74          97.27          56.35      
    2    185.06          130.45          36.02      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    48.93          194.74          185.06      
    1     4.11          97.27          130.45      
    2     2.83          56.35          36.02      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    97.87          198.85          187.90      
    1    198.85          194.53          186.80      
    2    187.90          186.80          72.03      

 

Metagame probabilities: 
Player #0: 0.0003  0.42  0.5798  
Player #1: 0.0003  0.42  0.5798  
Iteration : 2
Time so far: 15821.42215871811
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-07-17 00:09:47.206397: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012805177737027407 22.19335231781006 0.25572696328163147 11.327355861663818 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03355054669082165 15.916335105895996 0.6968567252159119 8.005281782150268 230388 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03787666149437428 19.316555404663085 0.8197881698608398 6.138937950134277 450388 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.037712933495640755 20.489975357055663 0.8626104354858398 5.651755523681641 669824 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034226123802363874 18.010371112823485 0.8494519591331482 5.7392171859741214 888944 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035174408368766306 17.505910301208495 0.9287956118583679 5.117806625366211 1108068 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029416641779243947 24.30103588104248 0.8463129460811615 5.931310749053955 1326532 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027897300757467748 21.33043098449707 0.8736358284950256 5.605763339996338 1544705 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02455572746694088 17.4615177154541 0.8685114026069641 5.779144144058227 1763797 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02551220580935478 16.964293956756592 0.9183686256408692 4.851358318328858 1982260 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02066622246056795 17.896769332885743 0.868979400396347 5.412466669082642 2199022 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01602446585893631 19.369925117492677 0.810110479593277 5.84588851928711 2417238 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014904584549367429 21.786330223083496 0.8736936092376709 5.336396741867065 2633947 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014824505057185888 16.19220714569092 0.9407166302204132 5.174554491043091 2850810 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011187138687819242 16.849269008636476 0.8601422607898712 5.315768384933472 3066597 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008327840873971582 27.487998008728027 0.7791698813438416 5.6689647197723385 3283141 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006619936367496848 17.039003372192383 0.8589397013187409 5.463250255584716 3501244 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004793188930489123 20.243377113342284 0.764648699760437 5.689296722412109 3720064 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017745263816323132 20.719104194641112 0.7670493721961975 6.110108375549316 3938460 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008646463575132657 24.77202949523926 0.5777275204658509 6.656070852279663 4156826 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004301576322177425 24.985422897338868 0.5966440975666046 6.948569440841675 4376541 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0031487485801335426 17.410202980041504 0.430291947722435 7.960970497131347 4595695 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013486346859281184 16.878565406799318 0.5051328092813492 7.005673360824585 4815312 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005552742426516488 19.091871070861817 0.40465202927589417 7.654362630844116 5035149 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  3.914061089744791e-05 22.986286926269532 0.366827392578125 8.438250637054443 5254392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009573427872965112 17.25713539123535 0.3983430862426758 7.903465604782104 5474392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000938977231271565 16.335337448120118 0.3860399752855301 8.568452072143554 5694392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006233094813069328 16.216031074523926 0.3872425615787506 9.426344871520996 5914392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013929636385000776 20.400525283813476 0.2909999489784241 9.98744821548462 6134392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00017491534526925534 21.787184715270996 0.25681380182504654 10.192813873291016 6354139 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009532351130474126 18.97174596786499 0.22789940536022185 9.962713241577148 6574139 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003429062315262854 24.319014739990234 0.26437503695487974 10.320219993591309 6794139 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008295904077385785 19.521748733520507 0.3310567826032639 10.391813182830811 7014139 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00040445151062158403 20.60160427093506 0.3076458990573883 9.985774707794189 7234139 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008709093264769763 15.0034255027771 0.29143373370170594 10.633297729492188 7453394 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001021758023853181 22.24880599975586 0.22355295717716217 10.282833003997803 7673077 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007721979360212572 19.466400146484375 0.2365738034248352 10.245538806915283 7892125 0


Pure best response payoff estimated to be 96.79 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 85.1 seconds to finish estimate with resulting utilities: [156.435   2.315]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 142.23 seconds to finish estimate with resulting utilities: [115.575  55.54 ]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 142.79 seconds to finish estimate with resulting utilities: [78.485 83.35 ]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 141.66 seconds to finish estimate with resulting utilities: [50.14  51.615]
Computing meta_strategies
Exited RRD with total regret 1.703758953565682 that was less than regret lambda 1.7142857142857144 after 227 iterations 
REGRET STEPS:  15
NEW LAMBDA 1.5714285714285716
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0    48.93           4.11           2.83           2.31      
    1    194.74          97.27          56.35          55.54      
    2    185.06          130.45          36.02          83.35      
    3    156.44          115.58          78.48          50.88      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0    48.93          194.74          185.06          156.44      
    1     4.11          97.27          130.45          115.58      
    2     2.83          56.35          36.02          78.48      
    3     2.31          55.54          83.35          50.88      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0    97.87          198.85          187.90          158.75      
    1    198.85          194.53          186.80          171.12      
    2    187.90          186.80          72.03          161.83      
    3    158.75          171.12          161.83          101.75      

 

Metagame probabilities: 
Player #0: 0.0001  0.0603  0.43  0.5096  
Player #1: 0.0001  0.0603  0.43  0.5096  
Iteration : 3
Time so far: 25648.823938131332
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-07-17 02:53:34.779060: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7779 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017841158993542196 49.33314208984375 0.3320345014333725 11.803364753723145 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029718021303415297 12.988164043426513 0.6098800241947174 9.210823154449463 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030458560958504678 13.176440811157226 0.6877263188362122 7.523778772354126 450506 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031811986863613126 13.327824974060059 0.7433135390281678 6.972812843322754 670506 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029266272857785224 12.949126625061036 0.7549758791923523 6.896156883239746 889234 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025173436291515826 15.76290168762207 0.6797017395496369 6.869828605651856 1108046 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02364540323615074 18.242844009399413 0.683027309179306 7.027850484848022 1327065 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018911168817430733 23.581628227233885 0.5915960907936096 7.59465274810791 1546044 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025054685398936273 13.283802795410157 0.858843183517456 6.016936874389648 1764580 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02103363610804081 15.21666316986084 0.8314934730529785 6.239884853363037 1984284 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018565595895051957 11.578040504455567 0.8046233177185058 6.236991930007934 2203205 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01174964583478868 22.771037292480468 0.6165735781192779 7.719749164581299 2421029 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012191067449748516 30.17553596496582 0.6673495829105377 7.250373315811157 2639538 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013249720353633166 13.334718704223633 0.8276818752288818 5.972321939468384 2857876 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009529100451618432 16.201948738098146 0.7906485855579376 6.845062351226806 3076836 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008232740545645356 20.49485855102539 0.7605753660202026 6.861359548568726 3295415 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0031265737139619886 25.647129821777344 0.5164101123809814 8.402476978302001 3515415 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0031163895269855856 19.71744499206543 0.6025380253791809 7.787237167358398 3735415 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007861841761041433 18.076457023620605 0.5867428421974182 7.543848657608033 3954484 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -7.140936795622111e-05 20.71424217224121 0.5231961816549301 8.142962980270386 4174326 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007847545057302341 22.826635551452636 0.4246137052774429 8.517389488220214 4394142 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009562680354065379 24.098718643188477 0.28724569976329806 9.115359210968018 4614142 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013007857574848457 26.612766647338866 0.3232468217611313 9.294980716705322 4834142 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011977793925325385 13.83025074005127 0.41826232969760896 9.227772521972657 5054142 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00045324953243834897 23.4225793838501 0.2858144447207451 10.003829288482667 5274142 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005279662982502487 21.925594520568847 0.27345099300146103 10.406635189056397 5494051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010928279429208488 16.521579837799074 0.4431155800819397 9.843535041809082 5714051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -3.495131895761006e-05 20.60223560333252 0.48930673897266386 9.748093128204346 5934051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006775131667382083 15.107713985443116 0.41742388606071473 9.75989465713501 6154051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00044413222931325433 15.507971954345702 0.18410010635852814 10.152119350433349 6374051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019502101931720972 27.373565483093262 0.17618706077337265 10.602110767364502 6594051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004975282983650687 19.407214736938478 0.3624051928520203 10.234816360473634 6814051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001749410108095617 20.665912055969237 0.23525029122829438 10.765239143371582 7034051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012220343625813257 26.176500511169433 0.23343140482902527 10.824509620666504 7254051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0020649441052228213 13.096930122375488 0.21041689515113832 11.32281436920166 7474051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008893537931726314 20.414445686340333 0.24945174306631088 11.091720294952392 7694051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00015430572675541044 14.356569480895995 0.32946269810199735 10.950110530853271 7914051 0


Pure best response payoff estimated to be 82.12 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 83.55 seconds to finish estimate with resulting utilities: [151.505   2.095]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 141.82 seconds to finish estimate with resulting utilities: [117.405  52.39 ]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 143.11 seconds to finish estimate with resulting utilities: [72.57  81.205]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 141.8 seconds to finish estimate with resulting utilities: [84.955 73.49 ]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 141.77 seconds to finish estimate with resulting utilities: [59.13  58.085]
Computing meta_strategies
Exited RRD with total regret 1.5698265701632579 that was less than regret lambda 1.5714285714285716 after 231 iterations 
REGRET STEPS:  15
NEW LAMBDA 1.4285714285714288
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0    48.93           4.11           2.83           2.31           2.10      
    1    194.74          97.27          56.35          55.54          52.39      
    2    185.06          130.45          36.02          83.35          81.20      
    3    156.44          115.58          78.48          50.88          73.49      
    4    151.50          117.41          72.57          84.95          58.61      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0    48.93          194.74          185.06          156.44          151.50      
    1     4.11          97.27          130.45          115.58          117.41      
    2     2.83          56.35          36.02          78.48          72.57      
    3     2.31          55.54          83.35          50.88          84.95      
    4     2.10          52.39          81.20          73.49          58.61      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0    97.87          198.85          187.90          158.75          153.60      
    1    198.85          194.53          186.80          171.12          169.80      
    2    187.90          186.80          72.03          161.83          153.77      
    3    158.75          171.12          161.83          101.75          158.44      
    4    153.60          169.80          153.77          158.44          117.22      

 

Metagame probabilities: 
Player #0: 0.0001  0.0108  0.2906  0.2676  0.4308  
Player #1: 0.0001  0.0108  0.2906  0.2676  0.4308  
Iteration : 4
Time so far: 35661.79165124893
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-07-17 05:40:27.873011: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9757 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023460991866886615 59.29244842529297 0.42620195746421813 11.257393836975098 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03330270145088434 10.451794624328613 0.6924465239048004 9.518692874908448 230926 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023324772156774996 16.01060914993286 0.5163481414318085 10.138708877563477 448925 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01929848836734891 20.36999855041504 0.45417702198028564 10.950444793701172 667145 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0257286136969924 11.244458961486817 0.6366933941841125 9.19034652709961 883137 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024434192664921285 14.477415180206298 0.654824823141098 8.060174322128296 1101848 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02626753691583872 13.769497776031494 0.7400937080383301 7.412698459625244 1320813 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025495092384517193 14.243586349487305 0.8167166113853455 7.143662691116333 1538951 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021078337356448172 10.001434993743896 0.721775209903717 7.316159629821778 1758951 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01771719064563513 40.12580490112305 0.6436586856842041 8.314369010925294 1978270 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016999911051243544 11.82470417022705 0.7022625029087066 7.35395188331604 2198018 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013820342347025871 19.618205261230468 0.6604730010032653 8.528242778778075 2415803 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01296251742169261 20.907761573791504 0.6902859926223754 8.071461200714111 2634335 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00879754419438541 24.565817260742186 0.5436229884624482 8.992851066589356 2851169 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007526839431375265 23.692580032348634 0.6406296610832214 8.034592056274414 3065499 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006335258460603654 27.01884994506836 0.6238023936748505 8.576683712005615 3280121 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008072031557094305 23.51446361541748 0.4994407594203949 9.508570671081543 3498176 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002571458800230175 14.075536727905273 0.599454814195633 8.62456521987915 3715388 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010239784722216427 18.351410579681396 0.5915172457695007 9.213076305389404 3932594 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011767038377001882 14.503055667877197 0.5151522248983383 9.223828792572021 4146856 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004732648460048949 13.338117313385009 0.4027888298034668 9.593638896942139 4364049 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009302734507400601 15.483304214477538 0.33340432345867155 10.906996631622315 4581289 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00048481439152965323 26.487814903259277 0.37458007633686063 10.51735544204712 4796338 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00018607764286571182 21.12014045715332 0.3405733734369278 10.987982654571534 5008625 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006396941527782474 20.045174407958985 0.2702809736132622 10.901674747467041 5224616 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -9.98779956717044e-05 19.502483749389647 0.20973671674728395 10.900689888000489 5442792 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012680165978963486 16.595126724243165 0.13436696529388428 11.301025581359863 5659887 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00040352999931201337 14.52137794494629 0.27809885442256926 11.026813125610351 5877235 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001171337760752067 15.580476093292237 0.26091138571500777 11.080337619781494 6095269 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005205877541811788 18.304371070861816 0.16879787892103196 12.221645832061768 6313502 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010003742238041014 19.688258171081543 0.22254480570554733 11.597733211517333 6530350 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0019305213027109858 22.910447311401366 0.16225374937057496 12.029274559020996 6743106 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006108909088652581 24.16721382141113 0.055294375866651535 12.451377296447754 6955831 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004013519970840207 17.135205268859863 0.053516706079244615 12.179240036010743 7173830 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007778828243317548 16.641079139709472 0.06987846828997135 11.865596294403076 7392268 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009844773789154716 34.24561939239502 0.04084263071417808 13.381733798980713 7610024 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002171905031809729 26.136799621582032 0.05090179853141308 12.779138088226318 7821910 0
Recovering previous policy with expected return of 68.73134328358209. Long term value was 67.645 and short term was 62.53.


Pure best response payoff estimated to be 76.305 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 112.71 seconds to finish estimate with resulting utilities: [151.575   2.325]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 182.28 seconds to finish estimate with resulting utilities: [116.915  54.82 ]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 170.06 seconds to finish estimate with resulting utilities: [66.59  82.215]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 169.25 seconds to finish estimate with resulting utilities: [87.065 72.09 ]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 178.32 seconds to finish estimate with resulting utilities: [58.035 56.845]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 175.16 seconds to finish estimate with resulting utilities: [58.415 59.825]
Computing meta_strategies
Exited RRD with total regret 1.419123590336227 that was less than regret lambda 1.4285714285714288 after 162 iterations 
REGRET STEPS:  15
NEW LAMBDA 1.285714285714286
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0    48.93           4.11           2.83           2.31           2.10           2.33      
    1    194.74          97.27          56.35          55.54          52.39          54.82      
    2    185.06          130.45          36.02          83.35          81.20          82.22      
    3    156.44          115.58          78.48          50.88          73.49          72.09      
    4    151.50          117.41          72.57          84.95          58.61          56.84      
    5    151.57          116.92          66.59          87.06          58.03          59.12      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0    48.93          194.74          185.06          156.44          151.50          151.57      
    1     4.11          97.27          130.45          115.58          117.41          116.92      
    2     2.83          56.35          36.02          78.48          72.57          66.59      
    3     2.31          55.54          83.35          50.88          84.95          87.06      
    4     2.10          52.39          81.20          73.49          58.61          58.03      
    5     2.33          54.82          82.22          72.09          56.84          59.12      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0    97.87          198.85          187.90          158.75          153.60          153.90      
    1    198.85          194.53          186.80          171.12          169.80          171.74      
    2    187.90          186.80          72.03          161.83          153.77          148.81      
    3    158.75          171.12          161.83          101.75          158.44          159.16      
    4    153.60          169.80          153.77          158.44          117.22          114.88      
    5    153.90          171.74          148.81          159.16          114.88          118.24      

 

Metagame probabilities: 
Player #0: 0.0001  0.025  0.2964  0.2423  0.2329  0.2034  
Player #1: 0.0001  0.025  0.2964  0.2423  0.2329  0.2034  
Iteration : 5
Time so far: 46455.56919455528
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-07-17 08:40:22.180776: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11735 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022366921603679656 70.61634826660156 0.4207247108221054 11.785462760925293 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02458362132310867 13.491283988952636 0.5061813503503799 10.698188304901123 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028713671118021013 13.197644329071045 0.6381620526313782 9.889992713928223 449244 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02788180224597454 22.025238609313966 0.6425036430358887 8.910313320159911 669244 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030575108155608176 14.239380931854248 0.7607496380805969 8.157857131958007 888423 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02707924284040928 15.14766435623169 0.729899400472641 8.111351013183594 1106199 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0239450303837657 17.180677032470705 0.6833094477653503 8.065568351745606 1325710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021902571246027947 16.17647590637207 0.6775209486484528 8.396608638763428 1545710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02224656883627176 15.40788288116455 0.7084009945392609 8.344255352020264 1765710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01517861969769001 20.64886646270752 0.5715079307556152 9.073120498657227 1985710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015834135096520187 22.564258575439453 0.6547193586826324 8.622041034698487 2205710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01237163608893752 22.30023784637451 0.600978672504425 9.107026863098145 2425710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010937225911766291 23.259794235229492 0.5815169930458068 9.234551620483398 2645710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010704702092334628 17.406066513061525 0.678896701335907 8.583508396148682 2865710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006215599621646107 25.787374687194824 0.538902747631073 9.580225467681885 3085710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006212845374830067 32.3949592590332 0.6373878210783005 8.878841400146484 3305710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00458344203652814 31.07008800506592 0.6025709271430969 9.051756763458252 3525710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00228835652815178 22.136112403869628 0.5063762456178665 9.998536205291748 3745710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008500134470523335 19.56773977279663 0.6509709775447845 9.55811243057251 3965549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009544789209030569 14.724406242370605 0.6063234388828278 8.929895782470703 4185549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003596488983021118 31.035231018066405 0.36043810844421387 11.47782850265503 4405549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00032510282326256854 21.855596923828124 0.48108712732791903 11.142078971862793 4625549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010455572119099088 17.70651807785034 0.5605967313051223 10.560750007629395 4845549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005666932702297345 19.27711639404297 0.4372900426387787 11.632467269897461 5065549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006274316008784808 17.3908899307251 0.4491502046585083 11.099293518066407 5285549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00085852385382168 13.77367115020752 0.36543775498867037 11.192580890655517 5505549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009018155629746616 27.941970443725587 0.22963415682315827 11.889638710021973 5725549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000769852668963722 17.912950038909912 0.25361632704734804 12.170215225219726 5945549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004074751283042133 24.76018829345703 0.2946361482143402 12.068901538848877 6165549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005858527933014557 30.756803512573242 0.27164842635393144 12.658486938476562 6385549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005636220521409996 27.237203788757324 0.24710755795240402 12.49486846923828 6605549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003534348923858488 30.9454195022583 0.2677194580435753 12.734574317932129 6825549 0
/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/imitation_fine_tune.py:379: RuntimeWarning: invalid value encountered in divide
  legal_probs = legal_probs / np.sum(legal_probs)
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00033760409351089036 24.740302658081056 0.12155805677175521 12.862061882019043 7045549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008709140078281052 19.96724853515625 0.290641812980175 12.464968299865722 7265549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005110381403937936 29.47446937561035 0.17792173624038696 13.481560611724854 7484716 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004376349474114249 17.830641746520996 0.2824024587869644 12.668142986297607 7704716 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00014545397561960272 30.371484565734864 0.1306081235408783 13.153350830078125 7924716 0
Recovering previous policy with expected return of 72.88557213930348. Long term value was 71.833 and short term was 71.73.


Pure best response payoff estimated to be 77.29 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 83.9 seconds to finish estimate with resulting utilities: [154.325   2.475]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 137.73 seconds to finish estimate with resulting utilities: [116.87   52.345]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 140.07 seconds to finish estimate with resulting utilities: [67.16  82.065]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 139.59 seconds to finish estimate with resulting utilities: [87.615 73.595]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 140.25 seconds to finish estimate with resulting utilities: [59.205 54.595]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 140.16 seconds to finish estimate with resulting utilities: [58.405 55.345]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 139.47 seconds to finish estimate with resulting utilities: [58.385 58.035]
Computing meta_strategies
Exited RRD with total regret 1.2782604098732406 that was less than regret lambda 1.285714285714286 after 254 iterations 
REGRET STEPS:  15
NEW LAMBDA 1.1428571428571432
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    48.93           4.11           2.83           2.31           2.10           2.33           2.48      
    1    194.74          97.27          56.35          55.54          52.39          54.82          52.34      
    2    185.06          130.45          36.02          83.35          81.20          82.22          82.06      
    3    156.44          115.58          78.48          50.88          73.49          72.09          73.59      
    4    151.50          117.41          72.57          84.95          58.61          56.84          54.59      
    5    151.57          116.92          66.59          87.06          58.03          59.12          55.34      
    6    154.32          116.87          67.16          87.61          59.20          58.41          58.21      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    48.93          194.74          185.06          156.44          151.50          151.57          154.32      
    1     4.11          97.27          130.45          115.58          117.41          116.92          116.87      
    2     2.83          56.35          36.02          78.48          72.57          66.59          67.16      
    3     2.31          55.54          83.35          50.88          84.95          87.06          87.61      
    4     2.10          52.39          81.20          73.49          58.61          58.03          59.20      
    5     2.33          54.82          82.22          72.09          56.84          59.12          58.41      
    6     2.48          52.34          82.06          73.59          54.59          55.34          58.21      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0    97.87          198.85          187.90          158.75          153.60          153.90          156.80      
    1    198.85          194.53          186.80          171.12          169.80          171.74          169.22      
    2    187.90          186.80          72.03          161.83          153.77          148.81          149.22      
    3    158.75          171.12          161.83          101.75          158.44          159.16          161.21      
    4    153.60          169.80          153.77          158.44          117.22          114.88          113.80      
    5    153.90          171.74          148.81          159.16          114.88          118.24          113.75      
    6    156.80          169.22          149.22          161.21          113.80          113.75          116.42      

 

Metagame probabilities: 
Player #0: 0.0001  0.0057  0.2991  0.2508  0.1574  0.1283  0.1585  
Player #1: 0.0001  0.0057  0.2991  0.2508  0.1574  0.1283  0.1585  
Iteration : 6
Time so far: 56595.287428855896
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-07-17 11:29:21.573543: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13713 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027444419264793397 41.83062171936035 0.5165073066949845 10.66864242553711 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029299161583185195 13.737881755828857 0.5981107711791992 9.546842861175538 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027712587825953962 13.521789264678954 0.6186028182506561 9.278659629821778 450808 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02952433004975319 16.723659896850585 0.6809641301631928 8.919197273254394 670808 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02743449658155441 14.965025424957275 0.6664304256439209 8.731097602844239 889957 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025489339232444765 17.48912353515625 0.6747273504734039 7.992068481445313 1109142 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02191500850021839 15.340684413909912 0.6331434130668641 8.563257884979247 1329142 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019412032328546047 15.370030975341797 0.6534102559089661 8.07721791267395 1548393 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02186151724308729 13.721041297912597 0.7445689141750336 7.6348248481750485 1768316 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013480940647423268 23.9623592376709 0.5076241374015809 9.309615516662598 1988316 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014073020592331887 16.237653827667238 0.58973628282547 8.998798274993897 2208316 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013120808824896812 19.204348754882812 0.6264136075973511 8.696121311187744 2428316 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010219023283571004 17.43680477142334 0.5964205801486969 9.582941341400147 2648316 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008565692882984876 17.657574272155763 0.5058058112859726 9.570304203033448 2868316 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007399826031178236 24.932737159729005 0.587200504541397 9.24652271270752 3087339 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00480896788649261 29.6317346572876 0.4955758512020111 10.143633842468262 3307096 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0038671110291033984 18.764232063293456 0.5526234716176986 9.628635215759278 3527096 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0026764706009998916 25.69473476409912 0.5209410220384598 9.979031753540038 3746216 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011821054577012546 13.895238208770753 0.5845471382141113 9.918779659271241 3966216 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012062155059538783 31.295330429077147 0.4237205058336258 10.027311897277832 4186216 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00013735767934122122 22.46650276184082 0.3848602920770645 10.292333793640136 4406216 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011450152448560403 13.136471939086913 0.4924199044704437 9.492666530609132 4626216 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007704936026129872 14.181453037261964 0.41520269513130187 10.164741516113281 4846216 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005971771484468036 38.79836654663086 0.3329163759946823 11.189258670806884 5066216 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010584144467429724 17.968885517120363 0.20447641015052795 11.658077907562255 5286216 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001073946850374341 16.082964515686037 0.26159414649009705 10.285446453094483 5506216 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010295679290720726 18.047315406799317 0.23770418465137483 11.340683364868164 5725507 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000724294593965169 35.97721214294434 0.15205212607979773 12.57911787033081 5945177 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006889644217153545 29.054507064819337 0.19062262773513794 12.177282524108886 6164384 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00014505164872389287 30.11020736694336 0.21497315615415574 11.322746753692627 6383643 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021797032306494655 23.017032432556153 0.10227862000465393 11.991308212280273 6603296 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011034000337531325 35.62697429656983 0.03528560381382704 13.302660465240479 6823296 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007320121165321325 12.511950206756591 0.09816578626632691 11.165382194519044 7042510 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  4.204984652460553e-05 18.705678176879882 0.03296533077955246 12.640003395080566 7261789 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00056976243686222 16.173643779754638 0.08201991841197014 11.527381134033202 7481789 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.005073592159897089 16.811344051361083 0.1316223256289959 11.616149806976319 7701789 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004296658473322168 28.49916591644287 0.10632960572838783 12.088891887664795 7920230 0
Recovering previous policy with expected return of 68.02487562189054. Long term value was 67.69 and short term was 68.095.


Pure best response payoff estimated to be 76.7 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 84.41 seconds to finish estimate with resulting utilities: [153.44    2.665]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 140.7 seconds to finish estimate with resulting utilities: [116.37  55.41]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 141.33 seconds to finish estimate with resulting utilities: [69.895 83.22 ]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 141.42 seconds to finish estimate with resulting utilities: [87.43 71.24]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 141.15 seconds to finish estimate with resulting utilities: [58.35  57.955]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 140.86 seconds to finish estimate with resulting utilities: [58.085 60.195]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 141.32 seconds to finish estimate with resulting utilities: [56.415 59.08 ]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 139.73 seconds to finish estimate with resulting utilities: [58.895 56.115]
Computing meta_strategies
Exited RRD with total regret 1.1347250055809752 that was less than regret lambda 1.1428571428571432 after 307 iterations 
REGRET STEPS:  15
NEW LAMBDA 1.0000000000000004
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    48.93           4.11           2.83           2.31           2.10           2.33           2.48           2.67      
    1    194.74          97.27          56.35          55.54          52.39          54.82          52.34          55.41      
    2    185.06          130.45          36.02          83.35          81.20          82.22          82.06          83.22      
    3    156.44          115.58          78.48          50.88          73.49          72.09          73.59          71.24      
    4    151.50          117.41          72.57          84.95          58.61          56.84          54.59          57.95      
    5    151.57          116.92          66.59          87.06          58.03          59.12          55.34          60.20      
    6    154.32          116.87          67.16          87.61          59.20          58.41          58.21          59.08      
    7    153.44          116.37          69.89          87.43          58.35          58.09          56.41          57.51      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    48.93          194.74          185.06          156.44          151.50          151.57          154.32          153.44      
    1     4.11          97.27          130.45          115.58          117.41          116.92          116.87          116.37      
    2     2.83          56.35          36.02          78.48          72.57          66.59          67.16          69.89      
    3     2.31          55.54          83.35          50.88          84.95          87.06          87.61          87.43      
    4     2.10          52.39          81.20          73.49          58.61          58.03          59.20          58.35      
    5     2.33          54.82          82.22          72.09          56.84          59.12          58.41          58.09      
    6     2.48          52.34          82.06          73.59          54.59          55.34          58.21          56.41      
    7     2.67          55.41          83.22          71.24          57.95          60.20          59.08          57.51      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0    97.87          198.85          187.90          158.75          153.60          153.90          156.80          156.10      
    1    198.85          194.53          186.80          171.12          169.80          171.74          169.22          171.78      
    2    187.90          186.80          72.03          161.83          153.77          148.81          149.22          153.12      
    3    158.75          171.12          161.83          101.75          158.44          159.16          161.21          158.67      
    4    153.60          169.80          153.77          158.44          117.22          114.88          113.80          116.31      
    5    153.90          171.74          148.81          159.16          114.88          118.24          113.75          118.28      
    6    156.80          169.22          149.22          161.21          113.80          113.75          116.42          115.50      
    7    156.10          171.78          153.12          158.67          116.31          118.28          115.50          115.01      

 

Metagame probabilities: 
Player #0: 0.0001  0.0025  0.3022  0.2429  0.1181  0.0979  0.1163  0.1198  
Player #1: 0.0001  0.0025  0.3022  0.2429  0.1181  0.0979  0.1163  0.1198  
Iteration : 7
Time so far: 66717.8434047699
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-07-17 14:18:04.473620: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15691 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024427978321909904 45.62146224975586 0.47619508802890775 11.41926975250244 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028054822608828543 13.177857494354248 0.5774618208408355 10.038148212432862 230103 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03241453617811203 12.104791927337647 0.7162381410598755 9.091793251037597 449401 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02931175772100687 12.33292751312256 0.68555988073349 8.866833782196045 666563 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03220263328403235 16.436481475830078 0.8010331273078919 7.580975103378296 883507 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019516255147755145 17.951906967163087 0.5073157757520675 9.236522769927978 1098308 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023717890866100787 20.25881690979004 0.6776971399784089 8.784088325500488 1314321 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021933963522315025 12.006529808044434 0.6812537908554077 7.9000931739807125 1522979 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01892125653102994 15.219432163238526 0.6732259273529053 8.043675088882447 1733499 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018236918747425078 17.938167095184326 0.6773979246616364 8.124125576019287 1949813 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015112359076738358 18.550606727600098 0.6584058046340943 8.152162218093872 2160970 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014919268153607845 15.487800979614258 0.6941850125789643 7.9958251953125 2371118 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010028398316353559 22.428017997741698 0.4928835779428482 9.910101795196534 2583839 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010076593561097979 17.491006565093993 0.6144397258758545 8.879186630249023 2796399 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007875991286709905 26.615297698974608 0.5495831310749054 9.264899349212646 3008203 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007334355590865016 21.9836217880249 0.6513067781925201 8.798938274383545 3221495 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004543584026396275 25.03539409637451 0.5814615160226821 9.70380859375 3432061 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0028044193983078 19.230957984924316 0.48301776349544523 10.699989700317383 3644264 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009603676880942658 18.116746425628662 0.4922332763671875 10.280740547180176 3856577 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000778263526444789 50.32547454833984 0.23428588211536408 13.24450969696045 4068601 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00015282144813681952 29.99299259185791 0.3116750419139862 11.680184364318848 4281081 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005805039108963683 18.467695808410646 0.38732183873653414 11.238749885559082 4495566 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005713051505153999 11.96371603012085 0.38216415643692014 10.074686813354493 4707489 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007296226729522459 19.71385316848755 0.2903326630592346 12.457622718811034 4918869 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00044453300324676093 27.841121101379393 0.22542587220668792 13.38891897201538 5127604 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00029136459343135355 23.553569793701172 0.3105007439851761 12.113449954986573 5341680 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014538005882059224 19.895694541931153 0.16654916107654572 12.772936248779297 5552555 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00045592062269861344 35.5361141204834 0.14347367808222772 12.860789394378662 5760969 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00013702213618671522 16.83361358642578 0.043750330433249475 13.05813570022583 5970938 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00023439539181708825 17.177737331390382 0.038636007346212864 13.3600830078125 6179648 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003350603212311398 23.76671733856201 0.03519518710672855 13.24028148651123 6389959 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006238321744604036 18.48580207824707 0.033903705701231954 13.424288177490235 6601285 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002265625495056156 21.442882347106934 0.02881313469260931 13.569140720367432 6813004 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007842924316719291 18.984926986694337 0.02570649068802595 14.051812934875489 7024313 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004979090415872633 32.8280668258667 0.02445453703403473 14.203588199615478 7236579 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  2.305879388586618e-05 18.471830940246583 0.029027213901281358 14.05255765914917 7449308 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00028499915497377516 14.931323146820068 0.024349001049995423 14.264895343780518 7660771 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00045377320639090614 23.341390800476074 0.0280354805290699 13.390237522125243 7872413 0
Fatal Python error: Segmentation fault

Current thread 0x0000149e94a03b80 (most recent call first):
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/rl_environment.py", line 330 in step
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/rl_oracle_cooperative.py", line 481 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/rl_oracle.py", line 223 in _rollout
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/rl_oracle_cooperative.py", line 190 in __call__
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/psro_v2.py", line 413 in update_agents
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 201 in iteration
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 403 in gpsro_looper
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 482 in main
  File "/home/anrigu/.local/lib/python3.10/site-packages/absl/app.py", line 254 in _run_main
  File "/home/anrigu/.local/lib/python3.10/site-packages/absl/app.py", line 308 in run
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 485 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, google._upb._message, tensorflow.python.framework.fast_tensor_util, charset_normalizer.md, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, PIL._imaging, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label, cvxopt.base, cvxopt.blas, cvxopt.lapack (total: 117)
/var/spool/slurmd.spool/job55931175/slurm_script: line 34: 2731642 Segmentation fault      (core dumped) python3 -u psro_v2_simultaneous.py --game_name=harvest --save_folder_path="../examples/data/harvest/vanilla_ppo_full/($trial)_$parameter_id" --meta_strategy_method=prd_collab --gpsro_iterations=30 --consensus_imitation=False --regret_lambda_init=$curr_lambda --regret_steps=$curr_steps --policy_constraint_steps=20 --joint_action=False --rewards_joint=False --fine_tune=True --clear_trajectories=False --consensus_hidden_layer_size=50 --consensus_n_hidden_layers=2 --min_buffer_size_fine_tune=10000 --epochs_ppo=10 --minibatches_ppo=50 --number_training_steps=8000000 --symmetric_game=True --sims_per_entry=200 --eps_clip=.1 --eps_clip_value=.5 --entropy_decay_duration=.5 --ppo_entropy=.05 --fine_tune_policy_lr=.00003 --fine_tune_value_lr=.0003 --transfer_policy_minimum_entropy=0 --recovery_window=200 --hidden_layer_size=50 --n_hidden_layers=2 --batch_size=64 --dqn_learning_rate=3e-4 --update_target_network_every=500 --learn_every=1 --max_buffer_size=100000 --epsilon_decay_duration=100000 --min_buffer_size_to_learn=20000 --regret_calculation_steps=1500000
