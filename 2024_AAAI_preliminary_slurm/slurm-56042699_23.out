Job Id listed below:
56042737

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

2023-07-18 13:26:20.640424: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-18 13:26:21.133080: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 13:26:23.606158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0718 13:26:29.021771 22711643298688 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x14a7ae2e2d40>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x14a7ae2e2d40>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-07-18 13:26:29.356688: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-07-18 13:26:29.678836: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.7 seconds to finish estimate with resulting utilities: [47.23  49.985]
Exited RRD with total regret 0.0 that was less than regret lambda 5.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    48.61      

 

Player 1 Payoff matrix: 

           0      
    0    48.61      

 

Social Welfare Sum Matrix: 

           0      
    0    97.22      

 

Iteration : 0
Time so far: 0.0001323223114013672
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-07-18 13:26:49.293487: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.11906572058796883 22.37213020324707 2.05533447265625 0.002484630001708865 10560 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09863033369183541 18.268814945220946 1.875607419013977 0.27605838477611544 216960 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09336848333477973 15.624717807769775 1.87596515417099 0.3190219640731812 419880 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08676552250981331 14.612192249298095 1.8257913827896117 0.4564388424158096 621586 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07988721951842308 12.300573825836182 1.7817553520202636 0.5834863424301148 823871 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07264067456126214 14.224509048461915 1.7715620398521423 0.6350804746150971 1025518 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06512633562088013 17.43146743774414 1.7202018976211548 0.7011812567710877 1227629 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06207373924553394 18.579626274108886 1.6604797840118408 0.8790310800075531 1430650 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05448493249714374 17.686445426940917 1.6299595713615418 0.8997053444385529 1636165 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04421755000948906 20.790753746032713 1.5423298597335815 1.1369335293769836 1843556 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.041071562096476555 22.5183895111084 1.45927836894989 1.3439083695411682 2053411 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0343706214800477 23.63380012512207 1.3912370800971985 1.4389477133750916 2261703 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027609326131641864 22.97447090148926 1.3321495056152344 1.5841121554374695 2471803 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022265377268195153 22.98407573699951 1.3017180204391479 1.588214135169983 2681315 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01766482600942254 18.210525512695312 1.2107557296752929 1.8662472248077393 2891051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01447189887985587 24.984165954589844 1.1744070529937745 1.9890931248664856 3101211 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010681080678477884 25.98792552947998 1.113092029094696 2.1174638271331787 3311125 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007844984345138074 23.141097831726075 1.0596938610076905 2.31699686050415 3522921 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004070315230637789 21.304381370544434 0.9931058943271637 2.444619369506836 3735922 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015611219278071076 22.436038208007812 0.9378188848495483 2.624901533126831 3949646 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010763433121610433 24.82887783050537 0.8836611926555633 2.942587637901306 4167374 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019186633246135898 22.86269359588623 0.7715019702911377 3.222364377975464 4381769 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019353285213583149 25.64106140136719 0.6937023520469665 3.608854627609253 4595229 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004984590894309803 25.647522163391113 0.6614900231361389 3.91085147857666 4811764 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006938535050721839 21.36389026641846 0.5927947461605072 4.115411758422852 5028310 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000581697843153961 24.717312240600585 0.5678429067134857 4.428755378723144 5247095 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00020153912046225742 25.830688858032225 0.5602959334850312 4.659608221054077 5463277 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009595806299330434 25.15135097503662 0.5322672188282013 4.849835443496704 5679784 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0003126972063910216 21.655611991882324 0.48468008637428284 5.167263126373291 5895941 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010522263852180912 26.07946491241455 0.500569686293602 5.364321088790893 6114687 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009756361178006046 24.25972557067871 0.5166730463504792 5.450729656219482 6331672 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0001280456781387329 22.742779350280763 0.4526780009269714 5.749255561828614 6548847 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00045202027540653945 22.05016632080078 0.4129112660884857 5.934494972229004 6767072 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016569927878663294 24.03543930053711 0.3684355437755585 6.180981063842774 6984794 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011123077885713429 24.67056007385254 0.4267501175403595 6.391765403747558 7201037 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001878558166208677 25.110196685791017 0.3896077126264572 6.5458655834198 7419597 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013640492979902774 21.008331108093262 0.37760820984840393 6.7983794689178465 7639217 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008652527743834071 24.742687606811522 0.3738959342241287 6.905748414993286 7858230 0


Pure best response payoff estimated to be 193.885 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 92.54 seconds to finish estimate with resulting utilities: [185.71   3.81]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 154.06 seconds to finish estimate with resulting utilities: [91.195 90.22 ]
Computing meta_strategies
Exited RRD with total regret 4.8469585690502015 that was less than regret lambda 5.0 after 35 iterations 
REGRET STEPS:  25
NEW LAMBDA 4.791666666666667
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    48.61           3.81      
    1    185.71          90.71      

 

Player 1 Payoff matrix: 

           0              1      
    0    48.61          185.71      
    1     3.81          90.71      

 

Social Welfare Sum Matrix: 

           0              1      
    0    97.22          189.52      
    1    189.52          181.41      

 

Metagame probabilities: 
Player #0: 0.0275  0.9725  
Player #1: 0.0275  0.9725  
Iteration : 1
Time so far: 6955.502647399902
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-07-18 15:22:44.887375: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035835998505353926 88.51034774780274 0.6838774859905243 6.667645597457886 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0385543592274189 15.066198253631592 0.7978516280651092 5.63058385848999 230826 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03350942321121693 28.8223575592041 0.7317501187324524 5.796151447296142 449387 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03294911347329617 17.611809825897218 0.7832331359386444 5.341375064849854 667420 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0329144187271595 20.675377655029298 0.8222533524036407 5.35794095993042 883523 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03424227349460125 22.681535530090333 0.9285739600658417 4.8002057552337645 1100788 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031396086886525156 23.3420166015625 0.9221090972423553 4.309263277053833 1317407 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02876286208629608 17.97205686569214 0.8605112254619598 5.1937586784362795 1533089 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02840021587908268 25.80747871398926 0.9378743529319763 4.4279101371765135 1746969 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026550656370818614 19.13216552734375 0.9811274707317352 4.6911008834838865 1961882 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02434269730001688 14.808279895782471 0.9969324052333832 4.451646375656128 2178260 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02013286668807268 19.716635704040527 0.9612369477748871 4.578879976272583 2393279 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018661732226610182 17.492460632324217 0.9789523899555206 4.635816955566407 2609729 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01630673324689269 17.896794986724853 0.9790307760238648 4.471537494659424 2824778 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012793882936239242 19.71820411682129 0.9030904114246369 4.899973440170288 3036209 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010462422715499997 17.963307189941407 0.9005208194255829 4.931288433074951 3253662 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007706332649104297 20.061249923706054 0.949588942527771 4.546462059020996 3473290 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004548403201624751 18.337637519836427 0.8875518798828125 4.908509731292725 3691319 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017190929967910052 20.002786064147948 0.7180295705795288 5.586903285980225 3908689 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012408308597514407 19.319273376464842 0.7713837921619415 5.758793067932129 4126016 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009424953779671342 24.42332820892334 0.6809072613716125 5.784166383743286 4344102 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016923886927543209 20.948361587524413 0.679316395521164 5.73542594909668 4563194 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018856852635508402 23.608399391174316 0.5586057782173157 6.214541912078857 4782447 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016405549133196474 20.34529266357422 0.6020618319511414 5.998774766921997 5001677 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016306158271618188 22.601966285705565 0.4422684371471405 6.498607540130616 5221677 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012530187521406333 17.818120098114015 0.6679144144058228 6.105911493301392 5440767 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010372876393375918 16.573364162445067 0.5965122163295746 6.6644734859466555 5659724 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011282295490673278 16.87713975906372 0.5486488401889801 6.424350023269653 5879724 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001764159754384309 17.67023115158081 0.4456493973731995 6.7583800792694095 6098886 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012881948350695893 17.427296543121336 0.44711524844169614 7.200359344482422 6318886 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009159001405350864 17.314864444732667 0.492034438252449 6.807693338394165 6538886 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -1.253311347682029e-05 12.03079080581665 0.4820513814687729 7.023219060897827 6758678 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00014170074646244757 18.5572527885437 0.494629567861557 7.143377161026001 6978678 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  2.8924837715749162e-05 13.978934574127198 0.42195804715156554 7.238618850708008 7197984 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00126192578318296 23.063411140441893 0.5222582817077637 7.255549812316895 7417077 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00014676744176540525 18.84643726348877 0.5286848932504654 7.454565620422363 7636227 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006042456559953279 15.48172960281372 0.4474159061908722 7.9727356910705565 7856227 0


Pure best response payoff estimated to be 125.705 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 92.13 seconds to finish estimate with resulting utilities: [168.93   3.02]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 156.05 seconds to finish estimate with resulting utilities: [125.5    54.935]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 155.56 seconds to finish estimate with resulting utilities: [56.7  56.76]
Computing meta_strategies
Exited RRD with total regret 4.743769265656482 that was less than regret lambda 4.791666666666667 after 94 iterations 
REGRET STEPS:  25
NEW LAMBDA 4.583333333333334
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    48.61           3.81           3.02      
    1    185.71          90.71          54.94      
    2    168.93          125.50          56.73      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    48.61          185.71          168.93      
    1     3.81          90.71          125.50      
    2     3.02          54.94          56.73      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    97.22          189.52          171.95      
    1    189.52          181.41          180.44      
    2    171.95          180.44          113.46      

 

Metagame probabilities: 
Player #0: 0.0003  0.2412  0.7585  
Player #1: 0.0003  0.2412  0.7585  
Iteration : 2
Time so far: 16339.140468120575
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-07-18 17:59:08.670565: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018726778216660023 43.35258750915527 0.3434780389070511 9.74169225692749 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.038133681192994116 13.570097064971923 0.8066393792629242 7.223595237731933 229405 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03445329237729311 14.776945114135742 0.7742147505283355 7.384170484542847 448743 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03186820074915886 19.014189529418946 0.7471344590187072 7.00443058013916 668133 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03107286337763071 16.718098163604736 0.769277161359787 6.960732364654541 888133 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02739144302904606 16.103168296813966 0.7480309486389161 6.83546462059021 1106630 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030267519317567347 13.464532375335693 0.8725791990756988 6.335152053833008 1326389 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02858929932117462 18.406946182250977 0.8954964280128479 6.215872097015381 1545666 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024106977880001067 11.897409820556641 0.8262921214103699 6.9809352397918705 1765666 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020226508378982544 17.920830249786377 0.7701190412044525 6.824476814270019 1985519 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018506823759526014 12.762037372589111 0.8388940751552582 6.832901954650879 2204759 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01834650132805109 21.825074195861816 0.8088668465614319 6.981349039077759 2424338 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017067681439220904 27.681939125061035 0.8943871974945068 6.35879111289978 2641804 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012160373479127884 14.360063076019287 0.8039675772190094 6.99900450706482 2861357 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011825035326182842 12.824468612670898 0.9115480422973633 6.647363901138306 3080441 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006301651755347848 14.298258590698243 0.7194149315357208 7.381421661376953 3299411 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005060592945665121 17.620828437805176 0.8134865164756775 6.648665094375611 3518928 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0038339205435477196 13.488316345214844 0.8280295372009278 6.538004779815674 3737563 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015457994464668444 17.00666160583496 0.8194535851478577 6.749926805496216 3956803 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006544460182340117 12.264356327056884 0.6252477049827576 7.640642976760864 4174663 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013168324680009392 15.314569187164306 0.7162720918655395 7.226570749282837 4393380 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005658692106408125 21.632814788818358 0.5962629735469818 7.6727691173553465 4612042 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014859962364425882 21.129951095581056 0.4852264016866684 7.143450117111206 4831340 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012706324821920134 19.389426040649415 0.40894769728183744 8.277397108078002 5049379 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004433701495145215 13.7927885055542 0.5043126374483109 7.369767570495606 5268066 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012259002302016596 12.80699872970581 0.3711660861968994 7.96549916267395 5487457 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006279035293118796 12.834931564331054 0.33705922961235046 8.406227684020996 5704525 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001211777945400172 14.506702899932861 0.3021566838026047 8.902481269836425 5923612 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006693381244986085 14.7581130027771 0.34104377329349517 8.313252305984497 6139926 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008659568604343804 16.672289657592774 0.24399620592594146 8.40407657623291 6356400 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006728494219714776 19.824068069458008 0.40705959796905516 8.53869562149048 6572659 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003352477053340408 21.596984100341796 0.2412731170654297 8.822197246551514 6790692 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00022531767026521266 17.642055416107176 0.31810681223869325 8.351145935058593 7006756 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002396137577306945 11.437687015533447 0.3583818346261978 9.07584638595581 7226291 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -9.898859425447882e-05 21.256567764282227 0.2546482071280479 9.707570457458496 7445258 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002779311351332581 19.665012931823732 0.3189534038305283 9.047522163391113 7663865 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00023635793768335135 13.079903411865235 0.4168715596199036 8.483806991577149 7882596 0


Pure best response payoff estimated to be 80.515 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 90.39 seconds to finish estimate with resulting utilities: [139.61   2.53]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 154.51 seconds to finish estimate with resulting utilities: [104.065  49.65 ]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 156.91 seconds to finish estimate with resulting utilities: [71.135 77.705]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 151.94 seconds to finish estimate with resulting utilities: [74.115 71.31 ]
Computing meta_strategies
Exited RRD with total regret 4.579078785325592 that was less than regret lambda 4.583333333333334 after 152 iterations 
REGRET STEPS:  25
NEW LAMBDA 4.375000000000001
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0    48.61           3.81           3.02           2.53      
    1    185.71          90.71          54.94          49.65      
    2    168.93          125.50          56.73          77.70      
    3    139.61          104.06          71.14          72.71      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0    48.61          185.71          168.93          139.61      
    1     3.81          90.71          125.50          104.06      
    2     3.02          54.94          56.73          71.14      
    3     2.53          49.65          77.70          72.71      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0    97.22          189.52          171.95          142.14      
    1    189.52          181.41          180.44          153.72      
    2    171.95          180.44          113.46          148.84      
    3    142.14          153.72          148.84          145.43      

 

Metagame probabilities: 
Player #0: 0.0001  0.0364  0.4633  0.5002  
Player #1: 0.0001  0.0364  0.4633  0.5002  
Iteration : 3
Time so far: 26064.401380062103
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-07-18 20:41:14.036983: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7779 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02656536679714918 57.853935623168944 0.5177203834056854 9.320211601257324 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031161670014262198 13.95856351852417 0.6402528584003448 8.560278034210205 222038 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030765781924128533 13.428874015808105 0.67168487906456 8.217456197738647 431081 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024476598389446734 13.562352085113526 0.581945151090622 8.92212142944336 637443 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030216004326939584 14.807606887817382 0.7618841290473938 7.827170896530151 848467 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030586051382124425 14.312284564971923 0.8081580877304078 7.386096477508545 1058245 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02846898268908262 13.662749671936036 0.7941387712955474 7.640971326828003 1266322 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028273971751332283 14.464891815185547 0.8532313883304596 7.249697351455689 1473730 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024313349463045596 13.502310085296632 0.7929740428924561 7.643459892272949 1681767 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02394293136894703 14.81993808746338 0.8407428443431855 7.235806131362915 1889714 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020348752103745937 17.14203510284424 0.7812480449676513 7.393993806838989 2096570 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01889167483896017 12.497793769836425 0.8316360652446747 7.103940868377686 2305867 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016770039033144713 15.659892272949218 0.8763936579227447 7.109103488922119 2513119 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01242360081523657 17.235785293579102 0.7580634415149688 7.405916452407837 2722675 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0104249925352633 15.415427970886231 0.7577647209167481 7.437910556793213 2932013 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010636374540627002 11.96216869354248 0.8694158315658569 7.220741319656372 3138246 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006509752292186022 18.902790069580078 0.7151155591011047 7.878545045852661 3345406 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0037321384297683835 12.94524335861206 0.6736657798290253 8.268595552444458 3553829 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0028657816583290696 17.541144561767577 0.7620593547821045 8.101278066635132 3762283 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019157398928655311 14.268471908569335 0.623034393787384 8.924622631072998 3971715 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007090311046340503 13.513433170318603 0.6199620068073273 8.610201549530029 4179459 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014124987545073963 13.205656147003173 0.6299815654754639 8.763590621948243 4385588 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013100297685014084 16.824431228637696 0.6333976447582245 8.756247615814209 4594246 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004903885046587675 14.359714221954345 0.6101825535297394 8.524382495880127 4805615 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007984644566022325 12.590080738067627 0.5456508278846741 8.76628303527832 5014736 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011443373237852938 13.545418643951416 0.48164819180965424 8.561601924896241 5220844 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012003124575130641 22.015532875061034 0.42643625736236573 9.263573837280273 5431421 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003044265829657888 18.494546985626222 0.29775025248527526 9.559010887145996 5640878 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010486687227967196 15.320571994781494 0.31993511617183684 8.902726554870606 5850632 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012467503314837814 20.582969856262206 0.27391689121723173 9.460680484771729 6061081 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  4.4738671567756684e-05 19.39817066192627 0.3534481257200241 9.482841968536377 6270574 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00024159928234439575 17.702799606323243 0.17319350689649582 10.319669342041015 6479836 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001648908175411634 18.909724044799805 0.13421716317534446 10.299375438690186 6689644 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007683783420361579 24.593564987182617 0.19031960070133208 10.017517852783204 6896910 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0002344951585655508 14.374961566925048 0.15375551134347915 10.131836032867431 7105523 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000962333740608301 24.390985679626464 0.10401586294174195 10.795106220245362 7316064 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004062772037286777 24.45328598022461 0.139468714594841 10.706329345703125 7526237 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011315537958580535 24.458915519714356 0.1118976779282093 11.237173652648925 7735926 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00138882013343391 17.13886365890503 0.1252600856125355 10.425006198883057 7945945 0
Recovering previous policy with expected return of 64.14427860696517. Long term value was 58.713 and short term was 55.02.


Pure best response payoff estimated to be 64.24 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 90.55 seconds to finish estimate with resulting utilities: [139.495   2.19 ]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 157.35 seconds to finish estimate with resulting utilities: [104.51  51.11]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 157.4 seconds to finish estimate with resulting utilities: [68.645 74.92 ]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 155.68 seconds to finish estimate with resulting utilities: [73.115 73.205]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 155.54 seconds to finish estimate with resulting utilities: [73.405 72.175]
Computing meta_strategies
Exited RRD with total regret 4.357838051795284 that was less than regret lambda 4.375000000000001 after 82 iterations 
REGRET STEPS:  25
NEW LAMBDA 4.166666666666668
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0    48.61           3.81           3.02           2.53           2.19      
    1    185.71          90.71          54.94          49.65          51.11      
    2    168.93          125.50          56.73          77.70          74.92      
    3    139.61          104.06          71.14          72.71          73.20      
    4    139.50          104.51          68.64          73.11          72.79      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0    48.61          185.71          168.93          139.61          139.50      
    1     3.81          90.71          125.50          104.06          104.51      
    2     3.02          54.94          56.73          71.14          68.64      
    3     2.53          49.65          77.70          72.71          73.11      
    4     2.19          51.11          74.92          73.20          72.79      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0    97.22          189.52          171.95          142.14          141.69      
    1    189.52          181.41          180.44          153.72          155.62      
    2    171.95          180.44          113.46          148.84          143.56      
    3    142.14          153.72          148.84          145.43          146.32      
    4    141.69          155.62          143.56          146.32          145.58      

 

Metagame probabilities: 
Player #0: 0.0005  0.0733  0.3428  0.3  0.2835  
Player #1: 0.0005  0.0733  0.3428  0.3  0.2835  
Iteration : 4
Time so far: 36074.34580540657
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-07-18 23:28:04.162139: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9757 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027984840609133244 42.94613075256348 0.5434561520814896 9.081042098999024 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03870500735938549 12.407132244110107 0.8154163956642151 7.354213666915894 225273 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02680883016437292 13.873669338226318 0.589361160993576 8.481292915344238 435511 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03219844419509173 13.848850440979003 0.7566292881965637 7.3032999515533445 645623 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024308643490076064 14.595404624938965 0.5836155116558075 7.982949686050415 855276 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024418550357222558 19.399277114868163 0.6372436821460724 8.448315334320068 1063623 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025448559038341045 12.023230171203613 0.7441653907299042 7.822968482971191 1272240 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025620706751942633 11.957193851470947 0.7960714757442474 6.93199200630188 1481862 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023879816010594367 16.51559762954712 0.793212616443634 7.515755701065063 1688257 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02230281997472048 14.655645561218261 0.816848236322403 7.100434732437134 1893390 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01827153321355581 15.490283489227295 0.7540608882904053 7.226902484893799 2102198 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0162311814725399 17.61069326400757 0.7242169797420501 7.925236797332763 2308563 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016152275167405604 11.26027317047119 0.8304919600486755 7.487838649749756 2516852 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011004392337054015 18.36112003326416 0.6978753566741943 7.8607330322265625 2725511 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01059854794293642 15.95218391418457 0.738018137216568 7.858750486373902 2932907 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007560424227267504 15.874462127685547 0.6733915507793427 8.12627091407776 3139831 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01290968181565404 20.813277435302734 0.7666681051254273 7.638709640502929 3349046 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004036623565480113 15.737617492675781 0.7385514676570892 7.452960109710693 3556386 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0037559094838798047 17.06074571609497 0.7492088496685028 7.576259422302246 3763907 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005020468030124903 16.248253536224365 0.6369354248046875 8.205736541748047 3970587 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006590425473405048 18.443837928771973 0.4893873482942581 9.725572872161866 4179791 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006043418807166745 17.87534523010254 0.4816961884498596 9.28601140975952 4389291 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009309465778642334 24.57398338317871 0.274501696228981 9.575026607513427 4598284 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006559668647241779 15.391668319702148 0.2790785253047943 8.955435848236084 4806623 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008479592332150787 18.8693489074707 0.1551969066262245 9.269961547851562 5016016 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008372252916160506 16.063368034362792 0.19380476027727128 9.227195644378662 5222688 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010887937993175 15.487014389038086 0.218415305018425 9.47340030670166 5432175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -1.146044633060228e-06 13.697446632385255 0.16193408966064454 9.692429542541504 5640981 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000510849172133021 14.07563362121582 0.19066132605075836 9.185382556915282 5849448 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009884541963401717 20.067839050292967 0.16823630183935165 9.579647541046143 6058352 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0042853018385358155 19.098600387573242 0.11952540576457978 10.288877010345459 6267871 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001026429023477249 20.858301162719727 0.1347445733845234 10.223369693756103 6475759 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005334552348358557 15.484780883789062 0.13758162334561347 9.894813442230225 6687104 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001362042035907507 13.417720603942872 0.12343689128756523 9.743817138671876 6895959 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000253620398871135 16.266191291809083 0.09313593655824662 10.394024181365968 7103732 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001950617617694661 23.69366798400879 0.12319211885333062 9.952766799926758 7311978 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007311264402233064 18.185554218292236 0.09100711867213249 10.395066452026366 7518004 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010753869672043947 15.844580364227294 0.12414976879954338 9.462662315368652 7726870 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013743250106927007 12.36302728652954 0.09847490638494491 10.032550048828124 7933156 0
Recovering previous policy with expected return of 69.67661691542288. Long term value was 57.831 and short term was 55.48.


Pure best response payoff estimated to be 70.115 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 89.95 seconds to finish estimate with resulting utilities: [136.915   2.455]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 155.48 seconds to finish estimate with resulting utilities: [104.385  49.825]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 159.89 seconds to finish estimate with resulting utilities: [71.005 75.79 ]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 157.73 seconds to finish estimate with resulting utilities: [74.275 73.81 ]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 157.73 seconds to finish estimate with resulting utilities: [75.24  73.425]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 157.37 seconds to finish estimate with resulting utilities: [74.165 75.55 ]
Computing meta_strategies
Exited RRD with total regret 4.160465275721194 that was less than regret lambda 4.166666666666668 after 88 iterations 
REGRET STEPS:  25
NEW LAMBDA 3.9583333333333344
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0    48.61           3.81           3.02           2.53           2.19           2.46      
    1    185.71          90.71          54.94          49.65          51.11          49.83      
    2    168.93          125.50          56.73          77.70          74.92          75.79      
    3    139.61          104.06          71.14          72.71          73.20          73.81      
    4    139.50          104.51          68.64          73.11          72.79          73.42      
    5    136.91          104.39          71.00          74.28          75.24          74.86      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0    48.61          185.71          168.93          139.61          139.50          136.91      
    1     3.81          90.71          125.50          104.06          104.51          104.39      
    2     3.02          54.94          56.73          71.14          68.64          71.00      
    3     2.53          49.65          77.70          72.71          73.11          74.28      
    4     2.19          51.11          74.92          73.20          72.79          75.24      
    5     2.46          49.83          75.79          73.81          73.42          74.86      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0    97.22          189.52          171.95          142.14          141.69          139.37      
    1    189.52          181.41          180.44          153.72          155.62          154.21      
    2    171.95          180.44          113.46          148.84          143.56          146.80      
    3    142.14          153.72          148.84          145.43          146.32          148.09      
    4    141.69          155.62          143.56          146.32          145.58          148.66      
    5    139.37          154.21          146.80          148.09          148.66          149.72      

 

Metagame probabilities: 
Player #0: 0.0003  0.0436  0.2618  0.2293  0.2168  0.2483  
Player #1: 0.0003  0.0436  0.2618  0.2293  0.2168  0.2483  
Iteration : 5
Time so far: 46301.15060853958
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-07-19 02:18:31.099696: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11735 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027962381206452846 70.2595817565918 0.5380930334329606 8.449437856674194 10480 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03428038656711578 14.82454833984375 0.706733787059784 7.61828646659851 223586 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030314465425908564 13.622004127502441 0.6540045022964478 7.217707014083862 438714 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02565543185919523 21.021625137329103 0.5965484201908111 7.950957155227661 648118 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03267855085432529 13.81953468322754 0.8271645069122314 6.475284242630005 860000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03186298776417971 12.291725349426269 0.834242331981659 6.639806890487671 1068450 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028083990141749383 14.583942699432374 0.7944777011871338 6.5225756645202635 1275785 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02590374518185854 17.510898780822753 0.8050574421882629 6.629118871688843 1483879 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024134961143136024 17.589209365844727 0.7944070518016815 6.871598911285401 1693053 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022204277105629444 15.632575225830077 0.805871707201004 6.581974077224731 1900970 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019667389616370202 14.32081642150879 0.770431649684906 7.132192087173462 2108909 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017743718437850477 13.09925537109375 0.7934798240661621 7.124836158752442 2315855 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015387406200170517 15.629300594329834 0.7828924059867859 7.285478258132935 2523787 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01411619335412979 15.359227180480957 0.8487874329090118 6.53585524559021 2731976 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01131855733692646 16.294487762451173 0.7599196910858155 6.906953382492065 2939552 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009944505663588643 16.201292705535888 0.7844134986400604 7.13883638381958 3149149 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007545577734708786 11.870401859283447 0.8158998727798462 6.779523086547852 3357470 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00411987213883549 12.904562664031982 0.7595360517501831 6.979854536056519 3566859 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004007678653579205 16.64334554672241 0.7138079524040222 7.879228925704956 3777219 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000977694959146902 28.357496643066405 0.6296502411365509 8.17327880859375 3985569 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0001666911586653441 14.68569917678833 0.5883521318435669 7.883751773834229 4193547 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014547080936608836 16.80754623413086 0.5649263083934783 7.8035284042358395 4401711 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00013442997014863068 15.507152080535889 0.5978090107440949 7.600765037536621 4610758 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001060014817630872 16.192723655700682 0.5110235899686814 7.787722206115722 4818047 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016087014984805137 16.92819137573242 0.44658096730709074 8.388972568511964 5027123 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00032088805310195314 21.575981903076173 0.5105421543121338 8.330341053009032 5235108 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005724287722841837 15.501513385772705 0.5398354530334473 8.272570657730103 5444761 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004652793786590337 13.410584926605225 0.578078156709671 7.868229484558105 5651108 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007023346370260697 12.78971242904663 0.5067670583724976 8.296993732452393 5859798 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000503801362356171 16.704618644714355 0.37045609951019287 8.752981281280517 6067834 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00038136224065965507 20.10863800048828 0.4233032941818237 8.736255359649657 6275222 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010574774030828848 16.085921382904054 0.42684643864631655 8.773412704467773 6483218 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001278411882231012 14.558348369598388 0.46424480378627775 7.962965774536133 6690912 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005247885972494259 17.101852798461913 0.3799351692199707 8.417428874969483 6899066 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00022001370671205224 14.476511573791504 0.44801593124866484 8.244908475875855 7107729 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006806157691244153 14.977452659606934 0.3330848217010498 8.92012529373169 7317312 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010947947594104335 15.506319618225097 0.3960278630256653 8.394502353668212 7526354 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00038614612712990494 12.709042072296143 0.2579871967434883 9.375821495056153 7732534 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002816338080447167 23.277873992919922 0.31903029680252076 9.295223426818847 7938912 0
Recovering previous policy with expected return of 70.29353233830845. Long term value was 56.263 and short term was 55.515.


Pure best response payoff estimated to be 78.46 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 89.56 seconds to finish estimate with resulting utilities: [136.14   2.21]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 157.62 seconds to finish estimate with resulting utilities: [104.945  51.315]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 157.08 seconds to finish estimate with resulting utilities: [68.955 74.33 ]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 156.55 seconds to finish estimate with resulting utilities: [75.72  73.305]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 157.32 seconds to finish estimate with resulting utilities: [73.45 74.21]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 156.03 seconds to finish estimate with resulting utilities: [72.105 75.4  ]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 155.57 seconds to finish estimate with resulting utilities: [73.47 74.9 ]
Computing meta_strategies
Exited RRD with total regret 3.9318571749764573 that was less than regret lambda 3.9583333333333344 after 82 iterations 
REGRET STEPS:  25
NEW LAMBDA 3.750000000000001
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    48.61           3.81           3.02           2.53           2.19           2.46           2.21      
    1    185.71          90.71          54.94          49.65          51.11          49.83          51.31      
    2    168.93          125.50          56.73          77.70          74.92          75.79          74.33      
    3    139.61          104.06          71.14          72.71          73.20          73.81          73.31      
    4    139.50          104.51          68.64          73.11          72.79          73.42          74.21      
    5    136.91          104.39          71.00          74.28          75.24          74.86          75.40      
    6    136.14          104.94          68.95          75.72          73.45          72.11          74.19      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    48.61          185.71          168.93          139.61          139.50          136.91          136.14      
    1     3.81          90.71          125.50          104.06          104.51          104.39          104.94      
    2     3.02          54.94          56.73          71.14          68.64          71.00          68.95      
    3     2.53          49.65          77.70          72.71          73.11          74.28          75.72      
    4     2.19          51.11          74.92          73.20          72.79          75.24          73.45      
    5     2.46          49.83          75.79          73.81          73.42          74.86          72.11      
    6     2.21          51.31          74.33          73.31          74.21          75.40          74.19      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0    97.22          189.52          171.95          142.14          141.69          139.37          138.35      
    1    189.52          181.41          180.44          153.72          155.62          154.21          156.26      
    2    171.95          180.44          113.46          148.84          143.56          146.80          143.28      
    3    142.14          153.72          148.84          145.43          146.32          148.09          149.03      
    4    141.69          155.62          143.56          146.32          145.58          148.66          147.66      
    5    139.37          154.21          146.80          148.09          148.66          149.72          147.50      
    6    138.35          156.26          143.28          149.03          147.66          147.50          148.37      

 

Metagame probabilities: 
Player #0: 0.0003  0.0379  0.2105  0.1849  0.1795  0.2024  0.1845  
Player #1: 0.0003  0.0379  0.2105  0.1849  0.1795  0.2024  0.1845  
Iteration : 6
Time so far: 56697.429883003235
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-07-19 05:11:47.503175: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13713 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026961838267743586 56.25618324279785 0.5254094332456589 9.18395118713379 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03499156516045332 14.851795196533203 0.709999817609787 8.07857985496521 228137 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031585204601287845 14.043847560882568 0.6845225751399994 7.5964102268219 442330 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029848641902208328 18.8752384185791 0.698292714357376 7.775849437713623 660872 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03070554379373789 14.810182285308837 0.7579119861125946 7.206432867050171 876161 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029106546193361282 14.000588130950927 0.7864284873008728 6.676651954650879 1094414 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028876855783164502 15.52927532196045 0.8147815108299256 6.694351100921631 1304683 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0278619147837162 12.561479187011718 0.8617437183856964 6.375262832641601 1514556 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023763421550393106 21.289022064208986 0.791939914226532 7.122030878067017 1721428 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02294965796172619 13.522445678710938 0.8552408993244172 6.712842082977295 1930336 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019950667582452297 18.547063636779786 0.8365249276161194 7.113578605651855 2141714 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015952901169657707 17.14433345794678 0.7537174880504608 7.296570062637329 2350212 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013132744375616312 23.03312816619873 0.6832666039466858 7.839121723175049 2559155 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013922897167503834 13.4504714012146 0.8985841751098633 6.4648620128631595 2766076 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011936399526894093 17.8242488861084 0.8089607536792756 6.973709201812744 2973712 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009362865937873721 13.885291957855225 0.7320057153701782 7.41608362197876 3179925 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005970241595059633 21.665750312805176 0.6658664226531983 7.740384578704834 3385826 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0037076207343488933 17.60229663848877 0.7577456176280976 7.634013032913208 3592932 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0028090807143598797 13.828577899932862 0.7348221898078918 7.406188011169434 3800208 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  3.718184307217598e-05 20.96256465911865 0.709838330745697 7.705374622344971 4007330 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009033602400450036 15.581467628479004 0.6853399038314819 7.940209627151489 4214478 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -4.261784051777795e-05 14.42243595123291 0.6579465210437775 7.416878986358642 4422048 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00021130492241354658 18.595458507537842 0.522592568397522 8.879366779327393 4630043 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012611063473741523 14.56577377319336 0.6174417972564697 7.906596517562866 4837990 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001159860561892856 16.176205921173096 0.6945818066596985 7.584905767440796 5045364 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004457255607121624 14.111668586730957 0.5156662166118622 8.404488849639893 5255337 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013960217576823198 14.07246503829956 0.4785355031490326 8.6003812789917 5466657 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012822380565921776 15.920522499084473 0.5315792232751846 8.562015914916993 5675992 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007192767945525703 15.91295108795166 0.5477789223194123 8.66376838684082 5883699 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011685912824759726 21.323720359802245 0.41191559135913847 9.423592662811279 6091794 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008125338150421158 19.0028413772583 0.43241368532180785 8.606826496124267 6299794 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013010253867832943 18.43843173980713 0.4101274996995926 9.164075946807861 6505890 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010657016071490943 18.340512180328368 0.3219864308834076 9.585441780090331 6713784 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005668828627676703 15.209733486175537 0.14065211042761802 9.895092582702636 6920670 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004175443990561689 35.21641502380371 0.17143712639808656 10.80447301864624 7132377 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007823750294846832 16.9728702545166 0.3380507409572601 9.705399799346925 7340147 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00030552296302630564 14.984951496124268 0.26175855845212936 10.01294755935669 7548530 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003354566846610396 20.09393424987793 0.3182047545909882 10.07440118789673 7756666 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008375633988180198 20.81783962249756 0.34112235009670255 9.889030838012696 7965322 0
Recovering previous policy with expected return of 76.1044776119403. Long term value was 64.408 and short term was 64.94.


Pure best response payoff estimated to be 81.675 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 93.84 seconds to finish estimate with resulting utilities: [141.45   2.16]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 154.76 seconds to finish estimate with resulting utilities: [103.61   50.125]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 160.25 seconds to finish estimate with resulting utilities: [66.52  75.105]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 157.77 seconds to finish estimate with resulting utilities: [72.92  73.895]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 159.7 seconds to finish estimate with resulting utilities: [74.155 76.13 ]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 157.3 seconds to finish estimate with resulting utilities: [74.215 70.26 ]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 159.3 seconds to finish estimate with resulting utilities: [75.68  74.205]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 158.5 seconds to finish estimate with resulting utilities: [74.405 73.135]
Computing meta_strategies
Exited RRD with total regret 3.707689534846054 that was less than regret lambda 3.750000000000001 after 50 iterations 
REGRET STEPS:  25
NEW LAMBDA 3.5416666666666674
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    48.61           3.81           3.02           2.53           2.19           2.46           2.21           2.16      
    1    185.71          90.71          54.94          49.65          51.11          49.83          51.31          50.12      
    2    168.93          125.50          56.73          77.70          74.92          75.79          74.33          75.11      
    3    139.61          104.06          71.14          72.71          73.20          73.81          73.31          73.89      
    4    139.50          104.51          68.64          73.11          72.79          73.42          74.21          76.13      
    5    136.91          104.39          71.00          74.28          75.24          74.86          75.40          70.26      
    6    136.14          104.94          68.95          75.72          73.45          72.11          74.19          74.20      
    7    141.45          103.61          66.52          72.92          74.16          74.22          75.68          73.77      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    48.61          185.71          168.93          139.61          139.50          136.91          136.14          141.45      
    1     3.81          90.71          125.50          104.06          104.51          104.39          104.94          103.61      
    2     3.02          54.94          56.73          71.14          68.64          71.00          68.95          66.52      
    3     2.53          49.65          77.70          72.71          73.11          74.28          75.72          72.92      
    4     2.19          51.11          74.92          73.20          72.79          75.24          73.45          74.16      
    5     2.46          49.83          75.79          73.81          73.42          74.86          72.11          74.22      
    6     2.21          51.31          74.33          73.31          74.21          75.40          74.19          75.68      
    7     2.16          50.12          75.11          73.89          76.13          70.26          74.20          73.77      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0    97.22          189.52          171.95          142.14          141.69          139.37          138.35          143.61      
    1    189.52          181.41          180.44          153.72          155.62          154.21          156.26          153.74      
    2    171.95          180.44          113.46          148.84          143.56          146.80          143.28          141.62      
    3    142.14          153.72          148.84          145.43          146.32          148.09          149.03          146.81      
    4    141.69          155.62          143.56          146.32          145.58          148.66          147.66          150.28      
    5    139.37          154.21          146.80          148.09          148.66          149.72          147.50          144.48      
    6    138.35          156.26          143.28          149.03          147.66          147.50          148.37          149.88      
    7    143.61          153.74          141.62          146.81          150.28          144.48          149.88          147.54      

 

Metagame probabilities: 
Player #0: 0.0031  0.0588  0.1723  0.1528  0.1531  0.1556  0.1529  0.1516  
Player #1: 0.0031  0.0588  0.1723  0.1528  0.1531  0.1556  0.1529  0.1516  
Iteration : 7
Time so far: 67284.46296620369
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-07-19 08:08:14.852556: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15691 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027140652015805244 55.98388328552246 0.5248451054096221 8.72072992324829 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029511998407542706 24.664991569519042 0.6130217432975769 8.894114208221435 227275 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03362152371555567 14.201587295532226 0.7430426478385925 8.032181310653687 437105 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029308940470218658 18.748352813720704 0.6942410349845887 7.78636360168457 646495 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0291975200176239 16.098345470428466 0.7372975647449493 7.963291931152344 856587 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028856689110398293 14.442401695251466 0.7778671681880951 7.684316682815552 1065080 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02698671855032444 18.27113494873047 0.7591830015182495 7.564058256149292 1274232 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026587345451116563 18.052744483947755 0.8097643315792084 7.174940299987793 1484276 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02315573673695326 18.774009704589844 0.7967014133930206 7.641375923156739 1690777 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024394140020012854 14.081382179260254 0.9169610202312469 7.1085632801055905 1898598 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021856018900871278 14.65888032913208 0.8768108665943146 7.281495141983032 2106397 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016836680006235838 14.273665046691894 0.787420904636383 7.613701963424683 2316084 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015160540584474802 14.682250022888184 0.8086617231369019 7.926586532592774 2524233 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01328900046646595 18.013505268096925 0.8010530948638916 7.643146514892578 2732201 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010810818150639534 17.762016105651856 0.6834658741950989 8.096279859542847 2939236 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009100954793393613 16.540553283691406 0.7591740190982819 7.394238805770874 3150752 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007129785977303982 13.52084789276123 0.8203492045402527 7.330635643005371 3356822 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00436437843600288 19.255495071411133 0.6593916118144989 8.128668069839478 3566193 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0034395202994346617 17.298974895477294 0.6857804238796235 8.238718557357789 3774602 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001370525505626574 23.69917984008789 0.7420599520206451 7.84564962387085 3981601 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001104536520142574 16.099951648712157 0.7498486518859864 8.006091785430907 4187140 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007200514373835176 21.956056785583495 0.4312427699565887 8.454592609405518 4396721 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002758294211162138 20.448674201965332 0.6054940044879913 8.625821781158447 4603675 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007620245552971028 15.123834991455078 0.6627056896686554 8.23449125289917 4810910 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006535756052471698 21.474511909484864 0.5155635416507721 8.861005496978759 5019228 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004952491894073318 16.33793525695801 0.610941469669342 8.697256374359132 5227420 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00043862842067028397 14.97619161605835 0.504701766371727 9.039203071594239 5434945 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011273401731159538 15.659588432312011 0.4941618353128433 8.836264324188232 5643492 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00015021363069536164 24.168572616577148 0.4330539286136627 8.99250602722168 5852105 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00016253458743449302 16.57612476348877 0.4512655198574066 9.255155277252197 6060323 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005029082974942866 26.088025283813476 0.4528954982757568 9.353737258911133 6268207 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000638730326318182 16.024192142486573 0.51400605738163 9.007988452911377 6475709 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012069245312886778 23.98053321838379 0.4756588876247406 9.62836503982544 6684253 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002794693522446323 17.07574119567871 0.46390323638916015 9.900918579101562 6892215 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -1.0754803952295333e-05 13.882280349731445 0.4489963620901108 9.370076847076415 7100759 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005411192600149661 20.838061332702637 0.4214767336845398 10.023927307128906 7307509 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001460387109545991 18.183279609680177 0.42081246674060824 9.96562385559082 7514485 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011425185342886835 21.7107364654541 0.5066070646047592 9.588333797454833 7721929 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007148647739086301 24.10717906951904 0.4356535911560059 9.60804033279419 7931135 0
Recovering previous policy with expected return of 75.56218905472637. Long term value was 60.843 and short term was 65.45.


Pure best response payoff estimated to be 75.665 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (8, 0)
Current player 0 and current strategies (8, 0) took 93.04 seconds to finish estimate with resulting utilities: [140.63    2.225]
Estimating current strategies:  (8, 1)
Current player 0 and current strategies (8, 1) took 160.13 seconds to finish estimate with resulting utilities: [104.735  51.03 ]
Estimating current strategies:  (8, 2)
Current player 0 and current strategies (8, 2) took 157.72 seconds to finish estimate with resulting utilities: [67.12 74.9 ]
Estimating current strategies:  (8, 3)
Current player 0 and current strategies (8, 3) took 155.62 seconds to finish estimate with resulting utilities: [71.32  73.825]
Estimating current strategies:  (8, 4)
Current player 0 and current strategies (8, 4) took 159.09 seconds to finish estimate with resulting utilities: [73.18 73.76]
Estimating current strategies:  (8, 5)
Current player 0 and current strategies (8, 5) took 155.53 seconds to finish estimate with resulting utilities: [72.725 72.475]
Estimating current strategies:  (8, 6)
Current player 0 and current strategies (8, 6) took 157.7 seconds to finish estimate with resulting utilities: [73.69  72.995]
Estimating current strategies:  (8, 7)
Current player 0 and current strategies (8, 7) took 211.35 seconds to finish estimate with resulting utilities: [73.015 71.075]
Estimating current strategies:  (8, 8)
Current player 0 and current strategies (8, 8) took 215.92 seconds to finish estimate with resulting utilities: [75.66 73.45]
Computing meta_strategies
Exited RRD with total regret 3.5215506279221387 that was less than regret lambda 3.5416666666666674 after 53 iterations 
REGRET STEPS:  25
NEW LAMBDA 3.333333333333334
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    48.61           3.81           3.02           2.53           2.19           2.46           2.21           2.16           2.23      
    1    185.71          90.71          54.94          49.65          51.11          49.83          51.31          50.12          51.03      
    2    168.93          125.50          56.73          77.70          74.92          75.79          74.33          75.11          74.90      
    3    139.61          104.06          71.14          72.71          73.20          73.81          73.31          73.89          73.83      
    4    139.50          104.51          68.64          73.11          72.79          73.42          74.21          76.13          73.76      
    5    136.91          104.39          71.00          74.28          75.24          74.86          75.40          70.26          72.47      
    6    136.14          104.94          68.95          75.72          73.45          72.11          74.19          74.20          73.00      
    7    141.45          103.61          66.52          72.92          74.16          74.22          75.68          73.77          71.08      
    8    140.63          104.73          67.12          71.32          73.18          72.72          73.69          73.02          74.56      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    48.61          185.71          168.93          139.61          139.50          136.91          136.14          141.45          140.63      
    1     3.81          90.71          125.50          104.06          104.51          104.39          104.94          103.61          104.73      
    2     3.02          54.94          56.73          71.14          68.64          71.00          68.95          66.52          67.12      
    3     2.53          49.65          77.70          72.71          73.11          74.28          75.72          72.92          71.32      
    4     2.19          51.11          74.92          73.20          72.79          75.24          73.45          74.16          73.18      
    5     2.46          49.83          75.79          73.81          73.42          74.86          72.11          74.22          72.72      
    6     2.21          51.31          74.33          73.31          74.21          75.40          74.19          75.68          73.69      
    7     2.16          50.12          75.11          73.89          76.13          70.26          74.20          73.77          73.02      
    8     2.23          51.03          74.90          73.83          73.76          72.47          73.00          71.08          74.56      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8      
    0    97.22          189.52          171.95          142.14          141.69          139.37          138.35          143.61          142.85      
    1    189.52          181.41          180.44          153.72          155.62          154.21          156.26          153.74          155.76      
    2    171.95          180.44          113.46          148.84          143.56          146.80          143.28          141.62          142.02      
    3    142.14          153.72          148.84          145.43          146.32          148.09          149.03          146.81          145.14      
    4    141.69          155.62          143.56          146.32          145.58          148.66          147.66          150.28          146.94      
    5    139.37          154.21          146.80          148.09          148.66          149.72          147.50          144.48          145.20      
    6    138.35          156.26          143.28          149.03          147.66          147.50          148.37          149.88          146.69      
    7    143.61          153.74          141.62          146.81          150.28          144.48          149.88          147.54          144.09      
    8    142.85          155.76          142.02          145.14          146.94          145.20          146.69          144.09          149.11      

 

Metagame probabilities: 
Player #0: 0.0022  0.0473  0.1509  0.1346  0.1348  0.1358  0.134  0.1312  0.1293  
Player #1: 0.0022  0.0473  0.1509  0.1346  0.1348  0.1358  0.134  0.1312  0.1293  
Iteration : 8
Time so far: 78412.6656908989
Approximating Best Response
Training best response:  True 0.06
rtg values check:  (?,) (?, 1)
2023-07-19 11:13:43.932214: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_62/weights_2/Adam_1/Assign' id:17669 op device:{requested: '', assigned: ''} def:{{{node mlp_62/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_62/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_62/weights_2/Adam_1, mlp_62/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03168147876858711 43.645242309570314 0.6358082294464111 8.938366603851318 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034345078840851785 12.576594924926757 0.7151622176170349 8.074007892608643 222272 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03655223958194256 13.426170921325683 0.8093015432357789 7.52367787361145 433176 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03187255430966616 13.894181632995606 0.7428143680095672 7.650446367263794 641603 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027812633477151395 22.70899314880371 0.7066066324710846 7.966718435287476 850452 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03183029964566231 20.5413516998291 0.8277935087680817 7.240575933456421 1059155 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029705983027815817 14.665160846710204 0.8152365982532501 7.0403478145599365 1268473 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025269229710102082 15.702308082580567 0.7794249832630158 7.356604051589966 1476502 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023305751010775567 18.516038131713866 0.7584120452404022 7.6173069953918455 1687340 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021589977480471135 21.599005699157715 0.7750666677951813 7.788746643066406 1897107 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022322389483451843 17.630507469177246 0.8172662436962128 7.030802154541016 2107356 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0194321327842772 12.104000091552734 0.8955311357975007 6.70417103767395 2314681 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01652279607951641 14.691837978363036 0.8050204634666442 7.294235134124756 2523628 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014536022488027812 15.29143934249878 0.849096804857254 6.782722663879395 2730228 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00927118556573987 16.585493755340575 0.7175182282924653 8.02432985305786 2938706 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010794127453118562 13.64230728149414 0.8178644061088562 7.14871997833252 3148526 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006313541671261191 16.598798274993896 0.7599093735218048 7.631283617019653 3355665 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005728353932499886 15.858662414550782 0.787453430891037 7.545980167388916 3563121 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022148269694298507 20.651947021484375 0.5685625970363617 8.594697284698487 3772760 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002009569939400535 16.089982509613037 0.6939852654933929 8.03741102218628 3980905 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014046236599824624 18.69658546447754 0.7050577521324157 7.918931293487549 4190073 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009976179203931678 18.773225021362304 0.605149632692337 8.068647241592407 4398876 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015265714784618468 17.870466423034667 0.44708254337310793 8.372429656982423 4607316 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010641678411047906 20.12461566925049 0.6424744188785553 8.532126522064209 4817255 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019262523141151179 16.97486619949341 0.4463397443294525 8.436597156524659 5024597 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004574831087666098 23.207892417907715 0.5122690498828888 9.517855548858643 5232593 0
slurmstepd: error: *** JOB 56042737 ON gl3390 CANCELLED AT 2023-07-19T13:00:25 ***
