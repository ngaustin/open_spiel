Job Id listed below:
56061901

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

2023-07-18 20:11:03.877657: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-18 20:11:04.369725: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 20:11:06.509418: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0718 20:11:11.199373 23291669126016 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x152eba6a2c80>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x152eba6a2c80>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-07-18 20:11:11.519394: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-07-18 20:11:11.841922: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.73 seconds to finish estimate with resulting utilities: [50.205 46.63 ]
Exited RRD with total regret 0.0 that was less than regret lambda 5.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    48.42      

 

Player 1 Payoff matrix: 

           0      
    0    48.42      

 

Social Welfare Sum Matrix: 

           0      
    0    96.84      

 

Iteration : 0
Time so far: 0.00012612342834472656
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-07-18 20:11:31.458574: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.12301192581653594 26.977576637268065 2.0527270317077635 0.0008475817856378853 10143 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09733032733201981 16.917537689208984 1.8517902374267579 0.213906030356884 216587 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0918008491396904 16.495661544799805 1.836086344718933 0.2692842811346054 419322 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08920823708176613 23.953191566467286 1.8071398377418517 0.3568631440401077 621629 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.078730246424675 14.470599269866943 1.7985357284545898 0.4244261771440506 823557 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07031201124191284 20.111751747131347 1.7332519888877869 0.5734446585178375 1027166 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07103859931230545 18.971925354003908 1.730559229850769 0.6306433796882629 1229762 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06188748776912689 18.275735473632814 1.700862181186676 0.7318782150745392 1433248 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.053499435633420946 16.715856742858886 1.6437241196632386 0.8957078278064727 1637946 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.050643788650631905 18.73334331512451 1.568867015838623 1.0526067912578583 1843482 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03787833862006664 20.137907791137696 1.481381607055664 1.2415671706199647 2052423 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03234076593071222 22.326735496520996 1.3962971687316894 1.5009051084518432 2262210 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028819932974874972 19.283715343475343 1.3330545783042909 1.708230936527252 2469193 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022447737120091914 22.847939491271973 1.2541817903518677 1.923328948020935 2678142 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019461500644683837 23.524822616577147 1.24647057056427 1.9532800555229186 2887788 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013953555747866631 22.61273059844971 1.1583680033683776 2.1772796869277955 3097577 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010376380383968353 26.49218330383301 1.1320572733879088 2.391170334815979 3310066 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007948836870491504 25.961444091796874 1.0741575241088868 2.528354048728943 3523313 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0033330928068608045 24.68430347442627 0.9604761958122253 2.896752882003784 3737616 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014510426932247355 24.16661434173584 0.925398337841034 3.1880759954452516 3949287 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018985417566000252 22.392198753356933 0.8284928739070893 3.476509952545166 4162843 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000783883297117427 24.074755477905274 0.6994858026504517 3.905227851867676 4377946 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00031595826440025123 26.58042793273926 0.649502444267273 4.299206972122192 4592271 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015482711605727673 21.522677612304687 0.6004017531871796 4.8863935470581055 4805129 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0023501187199144622 26.06479835510254 0.5672934532165528 5.2239251136779785 5021359 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00028496786544565114 25.116680335998534 0.5496846258640289 5.687603712081909 5233424 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011474944651126862 20.8904541015625 0.5043394774198532 6.427767610549926 5448995 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016346175398211926 27.7771089553833 0.48708663284778597 6.6175305366516115 5666494 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008003202397958376 23.179639625549317 0.4760950207710266 6.371907567977905 5882419 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001391383237205446 24.363289833068848 0.41334764659404755 6.949758911132813 6100220 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015329439309425652 29.91015968322754 0.3909193128347397 7.311440563201904 6317416 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010545786630245858 26.402389335632325 0.38872129619121554 7.481118822097779 6534253 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00039878104653325865 26.92960147857666 0.388028085231781 7.71385760307312 6752431 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012731432914733886 28.105448150634764 0.37550156116485595 7.262303113937378 6969703 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006515490997117013 22.448624420166016 0.342323499917984 7.33900556564331 7186184 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009699910006020218 25.85412731170654 0.3341931104660034 7.60082745552063 7404243 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00021249239362077787 26.710037803649904 0.32094543278217313 7.678381061553955 7620728 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001474702765699476 25.600657653808593 0.32527331709861756 7.96091594696045 7836231 0


Pure best response payoff estimated to be 192.655 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 93.68 seconds to finish estimate with resulting utilities: [186.615   4.47 ]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 155.59 seconds to finish estimate with resulting utilities: [95.19 94.51]
Computing meta_strategies
Exited RRD with total regret 4.960420989143813 that was less than regret lambda 5.0 after 34 iterations 
REGRET STEPS:  25
NEW LAMBDA 4.791666666666667
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    48.42           4.47      
    1    186.62          94.85      

 

Player 1 Payoff matrix: 

           0              1      
    0    48.42          186.62      
    1     4.47          94.85      

 

Social Welfare Sum Matrix: 

           0              1      
    0    96.84          191.09      
    1    191.09          189.70      

 

Metagame probabilities: 
Player #0: 0.0271  0.9729  
Player #1: 0.0271  0.9729  
Iteration : 1
Time so far: 6710.716669082642
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-07-18 22:03:22.295555: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020069520082324745 75.09265213012695 0.37613046616315843 10.160860729217529 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0283087769523263 17.10108709335327 0.5758083283901214 8.458379650115967 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028908298723399638 19.47440700531006 0.6283937931060791 7.915039348602295 449720 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023669840767979622 21.401714706420897 0.5461836516857147 8.018595790863037 667083 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024654193967580795 17.13066005706787 0.6064400434494018 7.418611097335815 884977 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02660353071987629 17.223765754699706 0.702920264005661 6.54350790977478 1097745 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030222608894109725 18.526799774169923 0.8388611972332001 6.110326051712036 1316170 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027537881396710873 23.431664085388185 0.869779771566391 6.539069604873657 1532847 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02655469048768282 23.584571075439452 0.8864108920097351 6.336119842529297 1749643 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021776196639984848 21.694454193115234 0.8667608499526978 6.464591407775879 1966617 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019546099938452244 17.337516593933106 0.8296573162078857 5.850500869750976 2181834 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01678593084216118 19.99669189453125 0.7867989003658294 6.099214506149292 2393165 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014880784880369901 23.00055065155029 0.7828090965747834 6.371105813980103 2607231 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01442608111537993 19.01298770904541 0.9391383051872253 6.1039231300354 2823136 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010898358467966319 24.239319610595704 0.7908747792243958 6.712738943099976 3040175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00867505744099617 16.47866439819336 0.8300686001777648 6.509571838378906 3257418 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008387358859181404 14.498833084106446 0.8612780928611755 6.1012654304504395 3475070 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003996249358169735 19.500182819366454 0.7168697535991668 6.5296142578125 3693346 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014206739841029048 24.949486351013185 0.6463877737522126 6.648180437088013 3911040 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015515750885242597 21.266745948791502 0.6570946156978608 6.542580604553223 4129304 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  6.327663504634984e-05 19.267214012145995 0.6077776432037354 7.022243165969849 4348943 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014969688723795115 31.17574119567871 0.5429135918617248 7.312415981292725 4568943 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001511103956727311 13.67977523803711 0.6055268287658692 7.002257680892944 4788069 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007061147945933044 21.563030815124513 0.60059974193573 7.330537557601929 5007809 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007560822283267044 16.34077043533325 0.5618577361106872 7.400843667984009 5225614 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001173342834226787 20.485720443725587 0.5315042346715927 7.371306848526001 5445175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000923697161488235 18.282579231262208 0.5845899105072021 7.334411096572876 5664367 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007198280713055283 19.586334228515625 0.47053433656692506 7.560435819625854 5884367 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001082247230806388 18.028009796142577 0.5148594111204148 7.441753244400024 6104175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001048659520165529 19.775145530700684 0.47216937243938445 7.855215167999267 6322510 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00034516975974838713 16.069986152648926 0.4815850704908371 7.929265069961548 6540903 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00032499270819243977 15.96078405380249 0.3440656751394272 8.027984809875488 6758898 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00023527638477389702 19.868460845947265 0.3970529079437256 7.815774011611938 6978392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005493898759596049 18.092332077026366 0.41093615889549256 8.386565494537354 7196502 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007037611652776832 20.21118450164795 0.3794377535581589 8.505524063110352 7415579 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008027046533243265 16.917816257476808 0.38279204070568085 8.188148403167725 7634216 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005459720792714507 22.959619522094727 0.390043506026268 8.568445014953614 7852435 0


Pure best response payoff estimated to be 129.915 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 91.2 seconds to finish estimate with resulting utilities: [160.755   2.705]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 156.43 seconds to finish estimate with resulting utilities: [126.76  44.25]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 158.02 seconds to finish estimate with resulting utilities: [26.46  24.475]
Computing meta_strategies
Exited RRD with total regret 4.723183770693609 that was less than regret lambda 4.791666666666667 after 56 iterations 
REGRET STEPS:  25
NEW LAMBDA 4.583333333333334
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    48.42           4.47           2.71      
    1    186.62          94.85          44.25      
    2    160.75          126.76          25.47      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    48.42          186.62          160.75      
    1     4.47          94.85          126.76      
    2     2.71          44.25          25.47      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    96.84          191.09          163.46      
    1    191.09          189.70          171.01      
    2    163.46          171.01          50.94      

 

Metagame probabilities: 
Player #0: 0.0067  0.4541  0.5392  
Player #1: 0.0067  0.4541  0.5392  
Iteration : 2
Time so far: 15935.538188457489
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-07-19 00:37:07.244633: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011038154130801558 33.81213531494141 0.1991424337029457 11.384862232208253 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027801899053156377 18.721603202819825 0.5704818427562713 7.848042726516724 227854 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02826826237142086 21.28840789794922 0.6246884405612946 7.033149909973145 441638 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032587217167019844 14.245190906524659 0.776476514339447 6.441195869445801 653320 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031241787038743495 20.5075439453125 0.7710112690925598 6.155395126342773 863761 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031101765111088753 21.377500152587892 0.8298785388469696 5.665979051589966 1072795 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026617244444787504 18.13634099960327 0.7880346477031708 5.969314432144165 1283753 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02964726518839598 22.359064483642577 0.9458110272884369 5.102975606918335 1492554 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025272342935204507 14.40768404006958 0.8302137970924377 6.012283992767334 1703595 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024435972236096858 20.796915817260743 0.9046950697898865 5.187413120269776 1912832 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017924968991428612 19.467453956604004 0.7304822146892548 6.502055358886719 2123310 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022375809215009212 18.143188858032225 1.0021587014198303 4.8802101612091064 2332511 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018319341354072094 22.42794551849365 0.9065233826637268 5.563369274139404 2539539 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013463124632835388 14.225636291503907 0.7968194961547852 6.127154397964477 2749648 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011166059691458941 21.99072685241699 0.7755251586437225 6.243319129943847 2958791 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009798268927261234 24.780399894714357 0.8064933657646179 6.375545787811279 3166792 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006687152711674571 23.2363582611084 0.8268753349781036 5.597157907485962 3373542 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004291294096037746 14.165168476104736 0.7458963215351104 6.097812128067017 3580983 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0037156675825826824 11.271210193634033 0.8268936693668365 5.929151773452759 3788190 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00044959833467146383 16.773361587524413 0.6975595414638519 7.341633605957031 3994487 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010311792357242666 12.506516170501708 0.7238260209560394 6.2725417137146 4200822 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014559442352037877 16.727066707611083 0.6331838726997375 6.5808464050292965 4410577 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014709178565681213 15.1384521484375 0.4902952313423157 7.275995874404908 4618323 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011402648931834846 11.38567132949829 0.4496168464422226 7.321997404098511 4825204 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009643595411034766 11.6822660446167 0.46340457499027254 7.523102951049805 5031061 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007750084369035903 21.88598747253418 0.3664841651916504 7.580019807815551 5239265 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00017511244077468292 14.497683715820312 0.32465877532958987 7.195224237442017 5445910 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000762274737644475 10.109516048431397 0.2480516567826271 7.67891297340393 5654287 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007109183192369528 18.971142768859863 0.17912522107362747 8.745241165161133 5862223 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008513782027876005 14.379675960540771 0.17867451310157775 8.540806198120118 6069369 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003237934914601226 15.470927143096924 0.16089709103107452 8.344450950622559 6276278 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -5.8973132217943204e-05 17.094486427307128 0.10410925447940826 8.982413864135742 6481899 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009405357501236722 16.34419527053833 0.160676496475935 8.347804594039918 6689004 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0003114815423032269 15.437021827697754 0.13420648351311684 8.871769237518311 6896626 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005829476169310511 22.846480941772462 0.1481916517019272 10.137322807312012 7102158 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007503520842874423 17.27109851837158 0.14616255164146424 9.215736484527588 7308766 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009274946791265392 13.992066764831543 0.1728238731622696 9.141684913635254 7515952 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008876328414771706 26.021209526062012 0.14806512445211412 10.074914646148681 7722478 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005529656657017768 11.175396156311034 0.1105827696621418 9.7061261177063 7929064 0
Recovering previous policy with expected return of 73.04477611940298. Long term value was 39.715 and short term was 40.32.


Pure best response payoff estimated to be 77.82 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 94.56 seconds to finish estimate with resulting utilities: [164.845   2.47 ]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 158.75 seconds to finish estimate with resulting utilities: [127.145  43.575]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 160.93 seconds to finish estimate with resulting utilities: [27.835 28.725]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 159.04 seconds to finish estimate with resulting utilities: [24.03  27.335]
Computing meta_strategies
Exited RRD with total regret 4.570189788863104 that was less than regret lambda 4.583333333333334 after 33 iterations 
REGRET STEPS:  25
NEW LAMBDA 4.375000000000001
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0    48.42           4.47           2.71           2.47      
    1    186.62          94.85          44.25          43.58      
    2    160.75          126.76          25.47          28.73      
    3    164.84          127.14          27.84          25.68      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0    48.42          186.62          160.75          164.84      
    1     4.47          94.85          126.76          127.14      
    2     2.71          44.25          25.47          27.84      
    3     2.47          43.58          28.73          25.68      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0    96.84          191.09          163.46          167.31      
    1    191.09          189.70          171.01          170.72      
    2    163.46          171.01          50.94          56.56      
    3    167.31          170.72          56.56          51.37      

 

Metagame probabilities: 
Player #0: 0.0328  0.3415  0.3109  0.3148  
Player #1: 0.0328  0.3415  0.3109  0.3148  
Iteration : 3
Time so far: 25561.792006492615
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-07-19 03:17:33.585982: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7779 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0084506472107023 35.910868072509764 0.16928713470697404 11.093054580688477 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03156622592359781 20.277790832519532 0.6373407065868377 7.35463171005249 227413 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03272310569882393 20.319152069091796 0.7095057964324951 6.944846200942993 440165 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035021179169416425 20.069293975830078 0.8192167639732361 6.561232662200927 653748 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034567064791917804 15.67278413772583 0.8433626234531403 6.732513284683227 866500 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03864776603877544 16.8213641166687 0.9911762177944183 5.832037925720215 1078322 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029772756434977055 17.12741689682007 0.8533006012439728 6.634951829910278 1287194 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029968107864260674 12.82652006149292 0.9180338740348816 5.837827968597412 1496304 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025731808692216872 22.47377281188965 0.8624680936336517 6.520302104949951 1704136 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02586002554744482 10.8626127243042 0.9716438472270965 5.963673734664917 1911168 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018914994038641452 16.613698291778565 0.8084292531013488 6.6935545921325685 2119154 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018340877443552017 12.323784351348877 0.8307178318500519 6.811037206649781 2328690 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016818076279014348 10.13988447189331 0.8340019047260284 6.546821784973145 2534122 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015169804450124502 8.422943353652954 0.8961318492889404 6.361041402816772 2740191 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0105651312507689 12.51033582687378 0.748580527305603 6.917847394943237 2946387 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008796002948656678 12.643256759643554 0.708792120218277 7.644079875946045 3151442 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0075428451411426066 8.29379506111145 0.8459297239780426 6.930237293243408 3354777 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005371278454549611 9.388723278045655 0.8265212714672089 7.074280881881714 3560395 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0030574457021430137 8.395551252365113 0.7651952564716339 7.6044481754302975 3764999 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00014759066252736374 10.246121597290038 0.6363123595714569 8.15709047317505 3970977 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009226328496879432 9.427815246582032 0.6856072425842286 7.789147090911865 4175269 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002133935925667174 12.007703304290771 0.5260638445615768 8.658418273925781 4379396 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011199778798982151 7.4909333229064945 0.6546373128890991 8.3066255569458 4581425 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014008503232616932 11.383895874023438 0.693838357925415 8.521129608154297 4784557 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006102783350797836 8.07601718902588 0.608357983827591 8.689781188964844 4988285 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005981548565614503 8.906435632705689 0.5093807011842728 8.990105152130127 5193332 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008996275588287972 7.757381725311279 0.40297919511795044 9.639336681365966 5396153 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00020676851709140466 7.522877836227417 0.39998314082622527 9.184778308868408 5598403 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005974971572868526 7.712022066116333 0.3928074210882187 10.214186096191407 5802435 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007482594708562828 8.655327081680298 0.2849248588085175 10.078504085540771 6006158 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -2.1136032773938496e-05 6.5215753555297855 0.3361849457025528 9.972319412231446 6208967 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -1.0726586333476007e-05 9.514734840393066 0.4455182939767838 10.018443298339843 6412085 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00023210904910229147 9.919692897796631 0.38835930824279785 10.233685207366943 6614440 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004957727738656104 7.077840185165405 0.38959757089614866 10.289972400665283 6817741 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008659918472403661 9.019326496124268 0.2593527600169182 10.820886135101318 7022257 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001132065434649121 7.26954460144043 0.21703117936849595 10.344803810119629 7225562 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003451684628089424 8.674266386032105 0.10254968851804733 10.89797306060791 7428716 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -3.4393960959278047e-06 8.000187873840332 0.2516315892338753 10.375155067443847 7631574 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00034792822989402337 7.652483034133911 0.18744802474975586 11.010131454467773 7836618 0
Recovering previous policy with expected return of 63.39303482587065. Long term value was 15.983 and short term was 15.885.


Pure best response payoff estimated to be 70.175 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 96.84 seconds to finish estimate with resulting utilities: [166.31    2.595]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 160.21 seconds to finish estimate with resulting utilities: [124.68  48.02]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 162.34 seconds to finish estimate with resulting utilities: [29.64 27.93]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 160.83 seconds to finish estimate with resulting utilities: [28.8  28.39]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 161.34 seconds to finish estimate with resulting utilities: [27.79  26.905]
Computing meta_strategies
Exited RRD with total regret 4.295770331817188 that was less than regret lambda 4.375000000000001 after 61 iterations 
REGRET STEPS:  25
NEW LAMBDA 4.166666666666668
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0    48.42           4.47           2.71           2.47           2.60      
    1    186.62          94.85          44.25          43.58          48.02      
    2    160.75          126.76          25.47          28.73          27.93      
    3    164.84          127.14          27.84          25.68          28.39      
    4    166.31          124.68          29.64          28.80          27.35      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0    48.42          186.62          160.75          164.84          166.31      
    1     4.47          94.85          126.76          127.14          124.68      
    2     2.71          44.25          25.47          27.84          29.64      
    3     2.47          43.58          28.73          25.68          28.80      
    4     2.60          48.02          27.93          28.39          27.35      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0    96.84          191.09          163.46          167.31          168.91      
    1    191.09          189.70          171.01          170.72          172.70      
    2    163.46          171.01          50.94          56.56          57.57      
    3    167.31          170.72          56.56          51.37          57.19      
    4    168.91          172.70          57.57          57.19          54.70      

 

Metagame probabilities: 
Player #0: 0.0063  0.3065  0.225  0.229  0.2332  
Player #1: 0.0063  0.3065  0.225  0.229  0.2332  
Iteration : 4
Time so far: 35488.52026152611
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-07-19 06:03:00.514853: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9757 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011591658974066377 35.391770362854004 0.2342747688293457 10.996768856048584 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.033242799900472166 14.75103588104248 0.6812158703804017 7.442801237106323 228856 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03631215803325176 12.7360595703125 0.7987939357757569 6.419606256484985 444571 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.040686968713998795 11.87982234954834 0.938926762342453 5.544443082809448 658208 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03889220841228962 10.663279628753662 0.965547525882721 5.321662712097168 867822 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03977606743574143 10.02616548538208 1.0556006610393525 5.078560924530029 1074934 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034524430707097056 11.163218879699707 0.9589885473251343 5.675037097930908 1282838 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030090131796896458 10.504358386993408 0.9394582569599151 5.796788454055786 1489275 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02617114707827568 10.906638717651367 0.8327146232128143 5.681875419616699 1694426 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024387140572071076 6.386644601821899 0.8841084241867065 5.853319978713989 1897760 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023071992211043835 6.600964307785034 0.9518347263336182 5.890832853317261 2102737 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015017777401953935 10.449416065216065 0.6894361495971679 7.284909582138061 2306800 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013104209676384926 13.103398418426513 0.6611350893974304 7.535320854187011 2511692 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008797326358035207 13.775509071350097 0.5498225569725037 8.30513687133789 2714836 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012232596799731255 7.316028451919555 0.8711405098438263 6.606212902069092 2916930 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009975519590079784 8.743511962890626 0.784619027376175 7.055144739151001 3120565 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0060837755911052225 7.970870494842529 0.7612016141414643 7.07382173538208 3323500 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004110097419470549 6.237593030929565 0.7407322764396668 7.1235583305358885 3526194 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003084777342155576 7.226495218276978 0.8087330639362336 6.9230283260345455 3729185 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012260208779480309 7.84912519454956 0.6902414977550506 7.670255470275879 3932110 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012877496250439436 7.6116269588470455 0.6477025985717774 7.746022939682007 4134528 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008785065758274869 9.336302757263184 0.5390217900276184 8.134545946121216 4338200 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005166017552255652 7.388700294494629 0.6302193999290466 7.871645927429199 4541766 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005313711706548929 12.0934419631958 0.4950800031423569 8.634153842926025 4745248 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00010250082559650763 7.6251296043396 0.6179360151290894 8.542825984954835 4949568 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003063514419181956 9.570576000213624 0.535801911354065 9.37761573791504 5153098 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001206154836108908 7.1716712474822994 0.49503996670246125 9.59399700164795 5356701 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000711846815829631 8.453451919555665 0.4182689219713211 10.142584800720215 5560179 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007086723308020737 20.520965576171875 0.4214771568775177 10.206768989562988 5763666 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00016016465124266688 9.378600883483887 0.2897580176591873 10.906736850738525 5967610 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  4.4495391193777323e-07 9.599325180053711 0.19220888912677764 9.284385681152344 6171383 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008010399484192021 9.45436143875122 0.12275848239660263 9.624119186401368 6374963 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006640241168497596 8.757686662673951 0.11854372546076775 9.981060695648193 6578725 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -8.931662450777367e-05 9.243659114837646 0.12114094495773316 10.525725650787354 6780656 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00021493209060281516 6.963542795181274 0.11971153318881989 10.088705062866211 6983496 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006507282330858288 9.376178216934203 0.08809114396572112 10.476501178741454 7187420 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006080645358451875 9.321904754638672 0.06411986723542214 11.296331310272217 7389771 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00036640942489611915 7.795978021621704 0.10265177339315415 10.318999481201171 7592619 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005204882253792675 11.308095169067382 0.09114662632346153 11.738466739654541 7795937 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001268771369359456 13.12629680633545 0.07637574411928653 12.038689613342285 8000954 0
Recovering previous policy with expected return of 67.8905472636816. Long term value was 16.455 and short term was 16.38.


Pure best response payoff estimated to be 69.22 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 93.51 seconds to finish estimate with resulting utilities: [162.81   2.43]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 157.75 seconds to finish estimate with resulting utilities: [126.71  44.7 ]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 159.8 seconds to finish estimate with resulting utilities: [26.805 25.875]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 159.12 seconds to finish estimate with resulting utilities: [27.355 27.36 ]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 159.71 seconds to finish estimate with resulting utilities: [27.635 26.29 ]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 159.02 seconds to finish estimate with resulting utilities: [25.38  27.295]
Computing meta_strategies
Exited RRD with total regret 4.156705664239283 that was less than regret lambda 4.166666666666668 after 98 iterations 
REGRET STEPS:  25
NEW LAMBDA 3.9583333333333344
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0    48.42           4.47           2.71           2.47           2.60           2.43      
    1    186.62          94.85          44.25          43.58          48.02          44.70      
    2    160.75          126.76          25.47          28.73          27.93          25.88      
    3    164.84          127.14          27.84          25.68          28.39          27.36      
    4    166.31          124.68          29.64          28.80          27.35          26.29      
    5    162.81          126.71          26.80          27.36          27.64          26.34      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0    48.42          186.62          160.75          164.84          166.31          162.81      
    1     4.47          94.85          126.76          127.14          124.68          126.71      
    2     2.71          44.25          25.47          27.84          29.64          26.80      
    3     2.47          43.58          28.73          25.68          28.80          27.36      
    4     2.60          48.02          27.93          28.39          27.35          27.64      
    5     2.43          44.70          25.88          27.36          26.29          26.34      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0    96.84          191.09          163.46          167.31          168.91          165.24      
    1    191.09          189.70          171.01          170.72          172.70          171.41      
    2    163.46          171.01          50.94          56.56          57.57          52.68      
    3    167.31          170.72          56.56          51.37          57.19          54.72      
    4    168.91          172.70          57.57          57.19          54.70          53.92      
    5    165.24          171.41          52.68          54.72          53.92          52.67      

 

Metagame probabilities: 
Player #0: 0.0009  0.3022  0.1704  0.1781  0.1769  0.1716  
Player #1: 0.0009  0.3022  0.1704  0.1781  0.1769  0.1716  
Iteration : 5
Time so far: 45809.79151105881
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-07-19 08:55:01.956282: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11735 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00820714165456593 33.89828872680664 0.15123295411467552 11.922992706298828 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03385500647127628 15.558025932312011 0.7259747207164764 7.802116823196411 226849 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.039767158776521684 13.280065631866455 0.8818096876144409 6.638111209869384 441280 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031330217607319355 13.769163513183594 0.7179908931255341 7.191477203369141 657970 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031388603150844574 19.357360649108887 0.8091505467891693 6.812803125381469 873797 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03254924770444632 14.93585138320923 0.8625217854976654 6.805771255493164 1084109 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02956774067133665 19.75655860900879 0.8346006751060486 6.252442216873169 1296174 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029387779347598553 15.00114688873291 0.882215940952301 6.024816799163818 1505388 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024238039366900922 14.322543525695801 0.8102536201477051 6.906818103790283 1714610 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02355595547705889 11.376102924346924 0.8575299561023713 6.417880201339722 1922266 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02010559905320406 15.08336238861084 0.8317004263401031 6.14636549949646 2130350 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01977624576538801 12.189739990234376 0.8838605642318725 6.102276611328125 2338822 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01589601207524538 10.109938812255859 0.8531169772148133 6.653411436080932 2544940 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012163642700761557 13.730686664581299 0.7509237051010131 7.328356552124023 2748739 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011290826741605997 7.464461946487427 0.8393541753292084 7.268457555770874 2953329 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00821162317879498 6.202111101150512 0.8178417444229126 7.378321313858033 3157357 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006894294312223792 7.909983682632446 0.7849944293498993 7.552570343017578 3360377 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005983321275562048 8.274349308013916 0.9303093075752258 7.148078680038452 3564239 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0026178028201684356 6.90545654296875 0.817520260810852 7.570134830474854 3768249 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007928352570161223 8.86043348312378 0.8202881813049316 7.768109893798828 3970424 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014320147161924978 8.69691219329834 0.6600516140460968 8.310367345809937 4173427 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010792385844979435 6.4691736698150635 0.6471413254737854 8.602186679840088 4377593 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004849703922445769 9.071807765960694 0.6544818222522736 8.897195053100585 4581435 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001760672140517272 11.074286842346192 0.5823358654975891 9.135454177856445 4784180 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00024859513287083245 10.493703937530517 0.49339039623737335 9.227256107330323 4987719 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005102578541118419 10.867988014221192 0.38256099820137024 9.453275108337403 5190610 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -6.525243661599233e-05 11.270702457427978 0.14784969240427018 10.722231006622314 5393231 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  7.683348831051262e-05 8.967366361618042 0.21183716058731078 10.193822860717773 5597637 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009319196724391077 7.598265647888184 0.238987298309803 9.72818603515625 5801409 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006265090283704922 7.786927270889282 0.3655774831771851 9.72201099395752 6006625 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007821358914952726 6.312425899505615 0.37271488904953004 9.519339561462402 6210576 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002821074580424465 9.985649490356446 0.34871315360069277 9.928986072540283 6413721 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008544604246708331 8.013614797592163 0.41401875019073486 9.608223056793213 6617297 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00017734038701746612 7.277502202987671 0.30639372766017914 10.540864944458008 6821747 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008479927229927853 8.06640977859497 0.25794955641031264 10.600401496887207 7026052 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00280762212787522 9.302802085876465 0.11216147691011429 11.482185649871827 7230235 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004037834863993339 6.893871736526489 0.08172724023461342 11.68953218460083 7435209 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006246280274353922 8.980892515182495 0.06884883418679237 11.69411859512329 7640418 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004919474265079771 10.042144966125488 0.07262113839387893 11.9061448097229 7844128 0
Recovering previous policy with expected return of 60.69651741293532. Long term value was 16.207 and short term was 17.59.


Pure best response payoff estimated to be 71.465 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 96.87 seconds to finish estimate with resulting utilities: [164.075   2.53 ]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 159.04 seconds to finish estimate with resulting utilities: [127.435  46.485]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 161.73 seconds to finish estimate with resulting utilities: [27.835 28.26 ]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 162.37 seconds to finish estimate with resulting utilities: [27.625 27.665]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 162.7 seconds to finish estimate with resulting utilities: [27.135 26.055]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 162.77 seconds to finish estimate with resulting utilities: [26.435 27.57 ]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 160.49 seconds to finish estimate with resulting utilities: [28.49 24.8 ]
Computing meta_strategies
Exited RRD with total regret 3.9331864113131516 that was less than regret lambda 3.9583333333333344 after 129 iterations 
REGRET STEPS:  25
NEW LAMBDA 3.750000000000001
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    48.42           4.47           2.71           2.47           2.60           2.43           2.53      
    1    186.62          94.85          44.25          43.58          48.02          44.70          46.48      
    2    160.75          126.76          25.47          28.73          27.93          25.88          28.26      
    3    164.84          127.14          27.84          25.68          28.39          27.36          27.66      
    4    166.31          124.68          29.64          28.80          27.35          26.29          26.05      
    5    162.81          126.71          26.80          27.36          27.64          26.34          27.57      
    6    164.07          127.44          27.84          27.62          27.14          26.43          26.64      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    48.42          186.62          160.75          164.84          166.31          162.81          164.07      
    1     4.47          94.85          126.76          127.14          124.68          126.71          127.44      
    2     2.71          44.25          25.47          27.84          29.64          26.80          27.84      
    3     2.47          43.58          28.73          25.68          28.80          27.36          27.62      
    4     2.60          48.02          27.93          28.39          27.35          27.64          27.14      
    5     2.43          44.70          25.88          27.36          26.29          26.34          26.43      
    6     2.53          46.48          28.26          27.66          26.05          27.57          26.64      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0    96.84          191.09          163.46          167.31          168.91          165.24          166.60      
    1    191.09          189.70          171.01          170.72          172.70          171.41          173.92      
    2    163.46          171.01          50.94          56.56          57.57          52.68          56.09      
    3    167.31          170.72          56.56          51.37          57.19          54.72          55.29      
    4    168.91          172.70          57.57          57.19          54.70          53.92          53.19      
    5    165.24          171.41          52.68          54.72          53.92          52.67          54.00      
    6    166.60          173.92          56.09          55.29          53.19          54.00          53.29      

 

Metagame probabilities: 
Player #0: 0.0002  0.3065  0.1377  0.1427  0.1358  0.1368  0.1404  
Player #1: 0.0002  0.3065  0.1377  0.1427  0.1358  0.1368  0.1404  
Iteration : 6
Time so far: 56503.6739487648
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-07-19 11:53:16.111443: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13713 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01370760090649128 91.13922119140625 0.24615837186574935 10.210228157043456 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03505683243274689 12.664179611206055 0.7035108506679535 7.463153266906739 227522 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0391970407217741 13.510320281982422 0.8400624513626098 6.769698762893677 441815 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03550535663962364 14.754545211791992 0.8253283202648163 6.488637256622314 654219 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031069451943039894 15.543127250671386 0.7493332386016845 7.167657423019409 865872 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03815575409680605 10.028100872039795 0.9925058722496033 6.161127185821533 1075909 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04008502401411533 14.869015121459961 0.9042270064353943 6.026961326599121 1283531 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032671361602842806 23.509282875061036 0.9364360332489013 5.498117542266845 1492822 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026515414379537104 12.641538047790528 0.8797463297843933 6.074211168289184 1703503 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025603643245995046 13.076619720458984 0.9059181094169617 6.287493705749512 1913848 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028158052079379558 13.047911167144775 0.8487453043460846 6.06852650642395 2122573 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016470688208937646 11.11159906387329 0.8187358021736145 6.717414236068725 2327826 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01848694644868374 11.826868057250977 0.9327504217624665 6.1016017436981205 2533879 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016580091416835786 15.694743728637695 0.7619106292724609 6.696743488311768 2737564 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014314068853855133 11.633906650543214 0.7538449823856354 6.983295154571533 2942282 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00968662602826953 6.7558619499206545 0.892555958032608 6.333567714691162 3146476 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007424992090091109 9.34623737335205 0.8402940988540649 6.979388999938965 3349816 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004817182221449912 8.151461267471314 0.8254304528236389 6.7332662582397464 3553814 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0034551058313809337 9.519290447235107 0.7213984668254853 7.591500282287598 3757326 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008769297783146612 10.621921920776368 0.6756721794605255 7.955800628662109 3962422 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006740141401678556 8.406245756149293 0.49970867931842805 8.839958953857423 4167431 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005165021199900366 8.60852246284485 0.5631033837795257 8.299020767211914 4372380 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.001216474300599657 8.144276571273803 0.4811104446649551 8.334063148498535 4576781 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00073669286139193 6.707316589355469 0.32703945636749265 8.161545515060425 4780084 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003170654919813387 13.549854183197022 0.3619983643293381 9.293732643127441 4985809 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005530096612346824 6.829711198806763 0.3483376920223236 9.226566600799561 5188788 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000620046865515178 9.630670166015625 0.28750252425670625 9.365421581268311 5392387 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001090520493744407 14.308310794830323 0.07099954262375832 10.795001888275147 5596367 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002224776068032952 9.374409770965576 0.13429349884390832 10.01553716659546 5799530 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001131074342993088 10.185598659515382 0.0688223909586668 11.353570556640625 6003254 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0022837281474494377 7.905087184906006 0.04374515935778618 12.222122192382812 6207709 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014688384311739356 9.220484924316406 0.07752084210515023 11.76142520904541 6411205 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008575552928959951 9.524173164367676 0.06151434853672981 11.530793285369873 6614257 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005022134828323033 6.906010866165161 0.06683416627347469 10.958992290496827 6816901 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001057896073325537 6.861545419692993 0.09546882435679435 10.544024658203124 7019851 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008617355029855389 8.648194074630737 0.08362576588988305 10.153568649291993 7224360 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  6.907316301294487e-05 8.590100812911988 0.04096930008381605 11.555948543548585 7427311 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00043967707870251616 12.378812503814697 0.06326710321009159 11.425559711456298 7630141 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010203910613199696 12.237930297851562 0.07255674861371517 10.681502437591552 7832669 0
Recovering previous policy with expected return of 62.99004975124378. Long term value was 16.837 and short term was 17.92.


Pure best response payoff estimated to be 66.875 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 95.97 seconds to finish estimate with resulting utilities: [162.405   2.55 ]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 160.62 seconds to finish estimate with resulting utilities: [128.755  44.775]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 162.09 seconds to finish estimate with resulting utilities: [27.86  29.565]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 161.68 seconds to finish estimate with resulting utilities: [28.615 27.125]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 161.48 seconds to finish estimate with resulting utilities: [26.805 28.555]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 162.17 seconds to finish estimate with resulting utilities: [25.505 24.755]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 162.15 seconds to finish estimate with resulting utilities: [26.1   27.995]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 160.37 seconds to finish estimate with resulting utilities: [26.005 26.12 ]
Computing meta_strategies
Exited RRD with total regret 3.7386585814545157 that was less than regret lambda 3.750000000000001 after 151 iterations 
REGRET STEPS:  25
NEW LAMBDA 3.5416666666666674
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    48.42           4.47           2.71           2.47           2.60           2.43           2.53           2.55      
    1    186.62          94.85          44.25          43.58          48.02          44.70          46.48          44.77      
    2    160.75          126.76          25.47          28.73          27.93          25.88          28.26          29.57      
    3    164.84          127.14          27.84          25.68          28.39          27.36          27.66          27.12      
    4    166.31          124.68          29.64          28.80          27.35          26.29          26.05          28.55      
    5    162.81          126.71          26.80          27.36          27.64          26.34          27.57          24.75      
    6    164.07          127.44          27.84          27.62          27.14          26.43          26.64          28.00      
    7    162.41          128.75          27.86          28.61          26.80          25.50          26.10          26.06      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    48.42          186.62          160.75          164.84          166.31          162.81          164.07          162.41      
    1     4.47          94.85          126.76          127.14          124.68          126.71          127.44          128.75      
    2     2.71          44.25          25.47          27.84          29.64          26.80          27.84          27.86      
    3     2.47          43.58          28.73          25.68          28.80          27.36          27.62          28.61      
    4     2.60          48.02          27.93          28.39          27.35          27.64          27.14          26.80      
    5     2.43          44.70          25.88          27.36          26.29          26.34          26.43          25.50      
    6     2.53          46.48          28.26          27.66          26.05          27.57          26.64          26.10      
    7     2.55          44.77          29.57          27.12          28.55          24.75          28.00          26.06      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0    96.84          191.09          163.46          167.31          168.91          165.24          166.60          164.96      
    1    191.09          189.70          171.01          170.72          172.70          171.41          173.92          173.53      
    2    163.46          171.01          50.94          56.56          57.57          52.68          56.09          57.42      
    3    167.31          170.72          56.56          51.37          57.19          54.72          55.29          55.74      
    4    168.91          172.70          57.57          57.19          54.70          53.92          53.19          55.36      
    5    165.24          171.41          52.68          54.72          53.92          52.67          54.00          50.26      
    6    166.60          173.92          56.09          55.29          53.19          54.00          53.29          54.09      
    7    164.96          173.53          57.42          55.74          55.36          50.26          54.09          52.12      

 

Metagame probabilities: 
Player #0: 0.0001  0.3059  0.119  0.1179  0.1141  0.1079  0.1181  0.117  
Player #1: 0.0001  0.3059  0.119  0.1179  0.1141  0.1079  0.1181  0.117  
Iteration : 7
Time so far: 66979.02494716644
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-07-19 14:47:51.341932: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15691 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017465583235025405 77.81391143798828 0.3395159035921097 9.187270450592042 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025919261761009692 16.127404499053956 0.5399970352649689 8.499578094482422 226350 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035420744493603704 14.371888637542725 0.770622992515564 7.260250091552734 440743 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02993932254612446 19.283882522583006 0.7012934029102326 7.733255243301391 654461 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030756534077227116 14.69524793624878 0.7708160400390625 7.381321573257447 870450 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02715786248445511 16.35188055038452 0.7192406296730042 7.299219131469727 1083496 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028602904826402663 15.870849132537842 0.81299529671669 7.380518531799316 1294175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028535370901226997 19.209905242919923 0.8865037560462952 6.571658086776734 1500837 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023093676939606667 18.801165199279787 0.7922421276569367 7.302729845046997 1712123 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023482421599328517 17.397351837158205 0.8426320552825928 7.07373685836792 1922309 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02422786857932806 10.858822059631347 0.9964755475521088 6.58160662651062 2130558 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01986848618835211 14.193767642974853 0.9143275916576385 6.528030586242676 2339108 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01597293596714735 12.615099906921387 0.7918890357017517 7.346765518188477 2550218 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01142253763973713 10.845781517028808 0.7075235724449158 7.907321357727051 2761351 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011024964600801468 10.674491214752198 0.7673904895782471 7.879357957839966 2971093 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009049523063004017 13.415993118286133 0.7126090228557587 7.583812665939331 3178252 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007777847908437252 12.537132930755615 0.7775156080722809 7.229200172424316 3385825 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004430873761884868 14.220626163482667 0.7523974120616913 7.9809722900390625 3591473 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0026276536111254243 11.786490726470948 0.6273805439472199 8.238716077804565 3802103 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00019725599740922918 10.695843505859376 0.7416679322719574 8.22691330909729 4011076 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014758339966647327 11.484735870361328 0.5956124305725098 8.16916732788086 4219664 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009475469245444401 11.580717277526855 0.6409556686878204 7.912462568283081 4426434 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0001286436825466808 9.72340793609619 0.5694088220596314 8.88768892288208 4632053 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005071930665508262 10.806227111816407 0.5421129941940308 8.48754529953003 4841173 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005679422843968496 14.021339321136475 0.43058248460292814 8.586162090301514 5047698 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -5.887736479053274e-05 15.646930122375489 0.3411421746015549 9.341656303405761 5255624 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  8.178275311365724e-06 14.34394884109497 0.327670818567276 9.692492198944091 5463842 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -4.782270116265863e-06 11.15389928817749 0.39993845820426943 9.528692531585694 5673226 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010249682614812627 12.436671447753906 0.2523761197924614 9.217617321014405 5881805 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013975103560369463 11.468198585510255 0.19663868546485902 9.618887996673584 6088940 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012415440127369949 12.38078498840332 0.10782730504870415 9.627969932556152 6295890 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009663637756602839 19.620466709136963 0.18160742074251174 9.338314437866211 6504193 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00042451837070984767 22.54884033203125 0.13559111058712006 10.447060298919677 6712186 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00013848586822859943 15.469155216217041 0.1344022586941719 10.007983589172364 6920945 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006635579877183773 17.654702186584473 0.12765143439173698 10.462880897521973 7131248 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005309384729116573 16.505428886413576 0.12202246859669685 10.830789947509766 7338340 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005108322715386748 9.71847414970398 0.17525260597467424 10.701121139526368 7545376 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -6.009645003359765e-05 15.763820934295655 0.23286591321229935 11.195873069763184 7751952 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001959543806151487 12.686856555938721 0.2708329111337662 10.898512840270996 7960137 0
Recovering previous policy with expected return of 62.67661691542288. Long term value was 39.924 and short term was 40.08.


Pure best response payoff estimated to be 70.57 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (8, 0)
Current player 0 and current strategies (8, 0) took 96.03 seconds to finish estimate with resulting utilities: [166.19    2.475]
Estimating current strategies:  (8, 1)
Current player 0 and current strategies (8, 1) took 157.56 seconds to finish estimate with resulting utilities: [125.77  47.88]
Estimating current strategies:  (8, 2)
Current player 0 and current strategies (8, 2) took 160.78 seconds to finish estimate with resulting utilities: [26.875 28.025]
Estimating current strategies:  (8, 3)
Current player 0 and current strategies (8, 3) took 159.76 seconds to finish estimate with resulting utilities: [25.905 27.37 ]
Estimating current strategies:  (8, 4)
Current player 0 and current strategies (8, 4) took 160.93 seconds to finish estimate with resulting utilities: [26.55  26.575]
Estimating current strategies:  (8, 5)
Current player 0 and current strategies (8, 5) took 160.57 seconds to finish estimate with resulting utilities: [27.44  26.755]
Estimating current strategies:  (8, 6)
Current player 0 and current strategies (8, 6) took 160.58 seconds to finish estimate with resulting utilities: [26.97  27.455]
Estimating current strategies:  (8, 7)
Current player 0 and current strategies (8, 7) took 159.44 seconds to finish estimate with resulting utilities: [27.205 25.39 ]
Estimating current strategies:  (8, 8)
Current player 0 and current strategies (8, 8) took 158.66 seconds to finish estimate with resulting utilities: [27.4  26.76]
Computing meta_strategies
Exited RRD with total regret 3.5393805571693377 that was less than regret lambda 3.5416666666666674 after 170 iterations 
REGRET STEPS:  25
NEW LAMBDA 3.333333333333334
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    48.42           4.47           2.71           2.47           2.60           2.43           2.53           2.55           2.48      
    1    186.62          94.85          44.25          43.58          48.02          44.70          46.48          44.77          47.88      
    2    160.75          126.76          25.47          28.73          27.93          25.88          28.26          29.57          28.02      
    3    164.84          127.14          27.84          25.68          28.39          27.36          27.66          27.12          27.37      
    4    166.31          124.68          29.64          28.80          27.35          26.29          26.05          28.55          26.57      
    5    162.81          126.71          26.80          27.36          27.64          26.34          27.57          24.75          26.75      
    6    164.07          127.44          27.84          27.62          27.14          26.43          26.64          28.00          27.45      
    7    162.41          128.75          27.86          28.61          26.80          25.50          26.10          26.06          25.39      
    8    166.19          125.77          26.88          25.91          26.55          27.44          26.97          27.20          27.08      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    48.42          186.62          160.75          164.84          166.31          162.81          164.07          162.41          166.19      
    1     4.47          94.85          126.76          127.14          124.68          126.71          127.44          128.75          125.77      
    2     2.71          44.25          25.47          27.84          29.64          26.80          27.84          27.86          26.88      
    3     2.47          43.58          28.73          25.68          28.80          27.36          27.62          28.61          25.91      
    4     2.60          48.02          27.93          28.39          27.35          27.64          27.14          26.80          26.55      
    5     2.43          44.70          25.88          27.36          26.29          26.34          26.43          25.50          27.44      
    6     2.53          46.48          28.26          27.66          26.05          27.57          26.64          26.10          26.97      
    7     2.55          44.77          29.57          27.12          28.55          24.75          28.00          26.06          27.20      
    8     2.48          47.88          28.02          27.37          26.57          26.75          27.45          25.39          27.08      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8      
    0    96.84          191.09          163.46          167.31          168.91          165.24          166.60          164.96          168.66      
    1    191.09          189.70          171.01          170.72          172.70          171.41          173.92          173.53          173.65      
    2    163.46          171.01          50.94          56.56          57.57          52.68          56.09          57.42          54.90      
    3    167.31          170.72          56.56          51.37          57.19          54.72          55.29          55.74          53.28      
    4    168.91          172.70          57.57          57.19          54.70          53.92          53.19          55.36          53.12      
    5    165.24          171.41          52.68          54.72          53.92          52.67          54.00          50.26          54.20      
    6    166.60          173.92          56.09          55.29          53.19          54.00          53.29          54.09          54.42      
    7    164.96          173.53          57.42          55.74          55.36          50.26          54.09          52.12          52.59      
    8    168.66          173.65          54.90          53.28          53.12          54.20          54.42          52.59          54.16      

 

Metagame probabilities: 
Player #0: 0.0001  0.315  0.1037  0.1017  0.096  0.0922  0.1022  0.0984  0.0907  
Player #1: 0.0001  0.315  0.1037  0.1017  0.096  0.0922  0.1022  0.0984  0.0907  
Iteration : 8
Time so far: 77650.93548750877
Approximating Best Response
Training best response:  True 0.06
rtg values check:  (?,) (?, 1)
2023-07-19 17:45:43.340313: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_62/weights_2/Adam_1/Assign' id:17669 op device:{requested: '', assigned: ''} def:{{{node mlp_62/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_62/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_62/weights_2/Adam_1, mlp_62/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012184594199061394 37.00380172729492 0.2535035878419876 11.207033348083495 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02823492232710123 12.942065906524657 0.5809984177350997 8.791081142425536 229511 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024093961156904698 23.444840812683104 0.5188001036643982 8.535325050354004 446003 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031845710799098016 15.419876194000244 0.7375124335289002 7.3147735595703125 659692 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03072710782289505 12.902994441986085 0.7372544586658478 7.452283763885498 871612 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024429812096059322 11.902873420715332 0.6553861677646637 8.261865425109864 1080830 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030057872459292412 17.216867446899414 0.8423590898513794 6.8885095596313475 1290073 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030627724155783652 16.906835746765136 0.9759790420532226 5.612217426300049 1498714 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027475257590413093 14.570868682861327 0.9216009438037872 6.451252555847168 1709151 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02154669463634491 13.573006916046143 0.8031772315502167 6.89115800857544 1916526 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02022509407252073 16.133061695098878 0.8425643146038055 6.4514275074005125 2125211 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018801872059702872 11.392010307312011 0.8438823640346527 6.485860443115234 2335362 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016107913572341204 8.695445013046264 0.8171195268630982 6.865392017364502 2542985 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012744666170328855 12.947170162200928 0.7664152920246124 6.74951205253601 2753467 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009063643869012594 11.516620540618897 0.6943775296211243 7.5800446510314945 2962085 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007925611967220903 9.10215883255005 0.7788277506828308 6.617748832702636 3171691 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006258760299533606 13.705067920684815 0.7400259435176849 6.820261240005493 3377442 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004758488340303302 9.830817317962646 0.7687353432178498 7.104502820968628 3582076 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0027557863271795214 14.881628608703613 0.5548611789941787 8.23856053352356 3790535 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00043041426797572056 14.758323669433594 0.672744071483612 7.483757019042969 3999680 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009782129811355845 14.648072624206543 0.5507804483175278 8.053216648101806 4207706 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002736222894355933 13.250818920135497 0.4744406282901764 8.131370353698731 4415731 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003772807031054981 15.7925950050354 0.4534625470638275 7.846714878082276 4622248 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009122422590735369 12.214085674285888 0.23561400920152664 8.139923810958862 4829671 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008981203878647648 16.355003833770752 0.13213840126991272 9.19969596862793 5038652 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008825062453979626 15.265303802490234 0.11372633576393128 9.515127563476563 5245383 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007138671309803613 12.248522567749024 0.10116811320185662 9.066077136993409 5451286 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011350346932886169 14.509413623809815 0.13284721300005914 9.379011058807373 5657431 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004569544333207887 13.704641532897949 0.08208489269018174 10.01646909713745 5864007 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003478041573544033 12.110616493225098 0.06738640367984772 9.917322158813477 6071643 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003295493621408241 24.992384719848634 0.06396556496620179 10.663945770263672 6278787 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007658991969947237 15.732951450347901 0.06989897936582565 9.748620891571045 6488422 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000137417133373674 12.321148681640626 0.06622852496802807 9.791327953338623 6694507 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013664749145391396 13.551286315917968 0.06982972472906113 10.476408958435059 6902697 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006696647353237495 18.652126026153564 0.0659810770303011 10.01012306213379 7111798 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005373166954086628 10.182191371917725 0.055469290167093274 11.077058029174804 7322306 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008001593334483915 12.886472511291505 0.06949378848075867 10.124706554412843 7532197 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00039935594541020694 18.504724311828614 0.06969112604856491 10.720155715942383 7739872 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00022615186317125334 16.5978964805603 0.06949127167463302 10.52468376159668 7949083 0
Recovering previous policy with expected return of 57.86069651741293. Long term value was 44.669 and short term was 48.005.


Pure best response payoff estimated to be 69.525 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (9, 0)
Current player 0 and current strategies (9, 0) took 192.89 seconds to finish estimate with resulting utilities: [162.65   2.3 ]
Estimating current strategies:  (9, 1)
Current player 0 and current strategies (9, 1) took 319.38 seconds to finish estimate with resulting utilities: [124.58   46.445]
Estimating current strategies:  (9, 2)
Current player 0 and current strategies (9, 2) took 322.42 seconds to finish estimate with resulting utilities: [30.66 27.19]
Estimating current strategies:  (9, 3)
Current player 0 and current strategies (9, 3) took 322.9 seconds to finish estimate with resulting utilities: [29.61  26.525]
Estimating current strategies:  (9, 4)
Current player 0 and current strategies (9, 4) took 325.36 seconds to finish estimate with resulting utilities: [29.75  28.615]
Estimating current strategies:  (9, 5)
Current player 0 and current strategies (9, 5) took 327.78 seconds to finish estimate with resulting utilities: [25.26  25.605]
Estimating current strategies:  (9, 6)
Current player 0 and current strategies (9, 6) took 323.1 seconds to finish estimate with resulting utilities: [30.39  28.585]
Estimating current strategies:  (9, 7)
Current player 0 and current strategies (9, 7) took 319.81 seconds to finish estimate with resulting utilities: [26.85  28.175]
Estimating current strategies:  (9, 8)
Current player 0 and current strategies (9, 8) took 322.58 seconds to finish estimate with resulting utilities: [26.865 27.78 ]
Estimating current strategies:  (9, 9)
Current player 0 and current strategies (9, 9) took 306.2 seconds to finish estimate with resulting utilities: [26.66  26.985]
Computing meta_strategies
Exited RRD with total regret 3.2980200462294746 that was less than regret lambda 3.333333333333334 after 188 iterations 
REGRET STEPS:  25
NEW LAMBDA 3.1250000000000004
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    48.42           4.47           2.71           2.47           2.60           2.43           2.53           2.55           2.48           2.30      
    1    186.62          94.85          44.25          43.58          48.02          44.70          46.48          44.77          47.88          46.45      
    2    160.75          126.76          25.47          28.73          27.93          25.88          28.26          29.57          28.02          27.19      
    3    164.84          127.14          27.84          25.68          28.39          27.36          27.66          27.12          27.37          26.52      
    4    166.31          124.68          29.64          28.80          27.35          26.29          26.05          28.55          26.57          28.61      
    5    162.81          126.71          26.80          27.36          27.64          26.34          27.57          24.75          26.75          25.61      
    6    164.07          127.44          27.84          27.62          27.14          26.43          26.64          28.00          27.45          28.59      
    7    162.41          128.75          27.86          28.61          26.80          25.50          26.10          26.06          25.39          28.18      
    8    166.19          125.77          26.88          25.91          26.55          27.44          26.97          27.20          27.08          27.78      
    9    162.65          124.58          30.66          29.61          29.75          25.26          30.39          26.85          26.86          26.82      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    48.42          186.62          160.75          164.84          166.31          162.81          164.07          162.41          166.19          162.65      
    1     4.47          94.85          126.76          127.14          124.68          126.71          127.44          128.75          125.77          124.58      
    2     2.71          44.25          25.47          27.84          29.64          26.80          27.84          27.86          26.88          30.66      
    3     2.47          43.58          28.73          25.68          28.80          27.36          27.62          28.61          25.91          29.61      
    4     2.60          48.02          27.93          28.39          27.35          27.64          27.14          26.80          26.55          29.75      
    5     2.43          44.70          25.88          27.36          26.29          26.34          26.43          25.50          27.44          25.26      
    6     2.53          46.48          28.26          27.66          26.05          27.57          26.64          26.10          26.97          30.39      
    7     2.55          44.77          29.57          27.12          28.55          24.75          28.00          26.06          27.20          26.85      
    8     2.48          47.88          28.02          27.37          26.57          26.75          27.45          25.39          27.08          26.86      
    9     2.30          46.45          27.19          26.52          28.61          25.61          28.59          28.18          27.78          26.82      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    96.84          191.09          163.46          167.31          168.91          165.24          166.60          164.96          168.66          164.95      
    1    191.09          189.70          171.01          170.72          172.70          171.41          173.92          173.53          173.65          171.03      
    2    163.46          171.01          50.94          56.56          57.57          52.68          56.09          57.42          54.90          57.85      
    3    167.31          170.72          56.56          51.37          57.19          54.72          55.29          55.74          53.28          56.13      
    4    168.91          172.70          57.57          57.19          54.70          53.92          53.19          55.36          53.12          58.36      
    5    165.24          171.41          52.68          54.72          53.92          52.67          54.00          50.26          54.20          50.87      
    6    166.60          173.92          56.09          55.29          53.19          54.00          53.29          54.09          54.42          58.98      
    7    164.96          173.53          57.42          55.74          55.36          50.26          54.09          52.12          52.59          55.03      
    8    168.66          173.65          54.90          53.28          53.12          54.20          54.42          52.59          54.16          54.64      
    9    164.95          171.03          57.85          56.13          58.36          50.87          58.98          55.03          54.64          53.64      

 

Metagame probabilities: 
Player #0: 0.0001  0.319  0.0889  0.0863  0.0837  0.0769  0.0902  0.0868  0.0785  0.0895  
Player #1: 0.0001  0.319  0.0889  0.0863  0.0837  0.0769  0.0902  0.0868  0.0785  0.0895  
Iteration : 9
Time so far: 91672.19628334045
Approximating Best Response
Training best response:  True 0.05500000000000001
rtg values check:  (?,) (?, 1)
2023-07-19 21:39:27.144861: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_69/weights_2/Adam_1/Assign' id:19647 op device:{requested: '', assigned: ''} def:{{{node mlp_69/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_69/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_69/weights_2/Adam_1, mlp_69/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013447006698697805 52.951134490966794 0.24797760397195817 10.960954475402833 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03525673411786556 22.077346420288087 0.7364356100559235 7.214124774932861 224522 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03540431335568428 20.401807403564455 0.7769108593463898 6.663632869720459 438757 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03413659371435642 19.193948364257814 0.7937861979007721 6.691224050521851 651351 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03646353259682655 11.26997709274292 0.8868008434772492 6.721436929702759 861184 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03437089994549751 10.53844175338745 0.9272745490074158 6.239135360717773 1071465 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03088493663817644 13.26490650177002 0.8853818833827972 6.287749099731445 1279805 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03290055580437183 10.950091552734374 1.0477216839790344 5.580811977386475 1489175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028089531138539313 10.20268211364746 0.9320484638214112 6.390575504302978 1697512 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024864500388503075 10.799898147583008 0.7939633727073669 7.059337902069092 1904949 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02067431379109621 7.932572317123413 0.8403202652931213 6.776959085464478 2110703 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01820125859230757 6.811863422393799 0.8404423117637634 6.931622076034546 2317075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016953607369214297 13.24324598312378 0.822233134508133 7.104325628280639 2519705 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011819884646683931 12.392483043670655 0.6803286910057068 8.05633316040039 2724883 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011280698515474797 8.921943044662475 0.7832457780838012 7.176636838912964 2927462 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008827758161351085 6.724808979034424 0.7770779848098754 7.0437311172485355 3130364 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00637624766677618 9.025941801071166 0.7259275078773498 7.793447828292846 3333767 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004444768896792084 10.587985515594482 0.5486362367868424 8.904470729827882 3536783 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0036127573577687143 9.634077072143555 0.7444589972496033 7.701823568344116 3740921 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011009696609107777 8.177425193786622 0.6804959177970886 8.275919818878174 3945066 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -3.597462200559676e-05 9.82535457611084 0.6923411071300507 7.906791305541992 4147459 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005268575834634249 7.104227113723755 0.6168014347553253 8.40490198135376 4350820 0
slurmstepd: error: *** JOB 56061901 ON gl3423 CANCELLED AT 2023-07-20T00:13:58 ***
