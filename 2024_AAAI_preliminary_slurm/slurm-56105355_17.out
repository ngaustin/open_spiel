Job Id listed below:
56105536

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

2023-07-20 14:27:59.032008: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-20 14:28:07.424526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0720 14:28:25.273899 22627977476992 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x149433502d40>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x149433502d40>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-07-20 14:28:26.049468: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-07-20 14:28:26.946083: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 20.36 seconds to finish estimate with resulting utilities: [50.91  48.665]
Exited RRD with total regret 0.0 that was less than regret lambda 10.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    49.79      

 

Player 1 Payoff matrix: 

           0      
    0    49.79      

 

Social Welfare Sum Matrix: 

           0      
    0    99.57      

 

Iteration : 0
Time so far: 0.0002262592315673828
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-07-20 14:28:48.960188: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.11686486899852752 31.71558246612549 2.0647777557373046 0.000734885095153004 10616 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09998552054166794 15.70855131149292 1.876138985157013 0.23691458404064178 216260 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09372291415929794 16.217895126342775 1.874222457408905 0.3126634269952774 419104 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08481417894363404 16.70651741027832 1.8543726921081543 0.4159471929073334 621553 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08121960908174515 17.141998958587646 1.8064512372016908 0.5329579055309296 823816 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07898886874318123 19.150613212585448 1.7862132430076598 0.6930857479572297 1026670 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06765958853065968 16.209982776641844 1.7568308472633363 0.7900941610336304 1230008 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06478628925979138 19.614112854003906 1.7169945001602174 0.8012109458446502 1432851 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04902155511081219 16.133348655700683 1.652464497089386 1.01138214468956 1636705 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.047448035702109335 18.151351261138917 1.5407283067703248 1.2566604018211365 1843364 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04262029007077217 17.611666679382324 1.4614314675331115 1.3713454127311706 2051153 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.033952876552939416 24.062289810180665 1.4167436599731444 1.4249940037727356 2260860 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027769216150045396 23.451021003723145 1.3860774040222168 1.5071863770484923 2468193 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02393606510013342 22.190755081176757 1.3137636661529541 1.662854790687561 2676250 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019633390195667743 23.44423179626465 1.3066510558128357 1.5945732116699218 2884064 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014361557271331548 22.741263771057127 1.1764962553977967 1.9586801052093505 3094151 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011833744216710329 20.379078102111816 1.1140802264213563 2.241945219039917 3306971 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00629860651679337 24.604724502563478 1.052659773826599 2.463925766944885 3520611 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005929747596383095 23.536436462402342 1.0034063518047334 2.6765095949172975 3732598 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016681426612194627 21.058124160766603 0.8976955235004425 2.8938777685165404 3949233 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001053296821191907 19.887916374206544 0.8091594457626343 3.2647446632385253 4164804 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019847527233650907 25.082416343688966 0.7462467610836029 3.5642643928527833 4382258 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001081655584857799 21.821325874328615 0.6741273045539856 3.7787296772003174 4598920 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007760533626424149 24.867975616455077 0.6420282900333405 3.9351977586746214 4815862 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008840347931254655 24.28304786682129 0.5890478372573853 4.173437595367432 5034828 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009529744158498943 22.74020137786865 0.560276472568512 4.440372467041016 5252221 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000504467825521715 23.286846160888672 0.4992231249809265 4.606539535522461 5469345 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012227665865793824 26.732239723205566 0.4701241433620453 4.6518967628479 5688430 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014825517711869907 22.31926727294922 0.46595939695835115 4.931361818313599 5905450 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016569229293963872 24.50244274139404 0.4628192961215973 4.9309440612792965 6123380 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022415005798393393 26.37861099243164 0.47262275218963623 5.022927618026733 6339330 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001080566809650918 26.895006942749024 0.44943458437919614 5.249894762039185 6556705 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003469834562565666 23.62971420288086 0.44439627826213834 5.317913436889649 6775504 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001090227828535717 25.935098075866698 0.4268141478300095 5.450253200531006 6992392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008308168631629087 23.374747276306152 0.40573471784591675 5.579224109649658 7211279 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005316107337421272 27.119712829589844 0.39699670374393464 5.623541641235351 7429391 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009121435228735209 23.905514907836913 0.3715019315481186 5.922436618804932 7647766 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.000219070803723298 27.930003356933593 0.34673686921596525 6.060717487335205 7862943 0


Pure best response payoff estimated to be 191.67 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 83.36 seconds to finish estimate with resulting utilities: [191.145   4.47 ]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 138.39 seconds to finish estimate with resulting utilities: [94.545 94.815]
Computing meta_strategies
Exited RRD with total regret 9.481047961903926 that was less than regret lambda 10.0 after 27 iterations 
REGRET STEPS:  25
NEW LAMBDA 9.583333333333334
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    49.79           4.47      
    1    191.15          94.68      

 

Player 1 Payoff matrix: 

           0              1      
    0    49.79          191.15      
    1     4.47          94.68      

 

Social Welfare Sum Matrix: 

           0              1      
    0    99.57          195.62      
    1    195.62          189.36      

 

Metagame probabilities: 
Player #0: 0.0511  0.9489  
Player #1: 0.0511  0.9489  
Iteration : 1
Time so far: 9030.34922170639
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-07-20 16:59:18.816643: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031393803842365745 64.45068740844727 0.604334020614624 7.034048795700073 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04597814790904522 16.402943801879882 0.9217188775539398 5.727717447280884 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035193661227822304 22.176131820678712 0.7610652208328247 5.600541019439698 448096 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035068460181355475 23.855843925476073 0.791581779718399 5.8157360553741455 662493 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03362222108989954 25.40346450805664 0.8421775996685028 5.137372398376465 872886 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.037020500376820566 17.715772724151613 0.9809187054634094 4.710048961639404 1085994 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032735339179635045 17.147421169281007 0.9686883389949799 4.788295888900757 1302483 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030765457823872567 15.104150867462158 0.9887784540653228 4.749117994308472 1516574 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029798737913370132 18.922604179382326 1.0086529016494752 4.536504745483398 1730221 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025030509755015373 22.75258865356445 0.9126975655555725 5.2162083148956295 1940978 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023565877974033356 20.175447463989258 0.9127969682216645 4.7785646438598635 2151683 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020840377174317838 19.053059768676757 0.9385649979114532 5.039293050765991 2361502 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01894696382805705 17.42952060699463 1.0137624502182008 4.481775999069214 2575739 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01688905293121934 18.423015213012697 0.9813312768936158 4.713690996170044 2789526 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013248767517507077 14.678753089904784 0.9526673316955566 5.022779655456543 3003254 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01119992444291711 20.999825096130373 0.9929402232170105 4.727556896209717 3218231 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0068332487251609566 21.415738105773926 0.8824722766876221 5.330042982101441 3434772 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004562899330630898 17.78235149383545 0.9230841219425201 5.176774549484253 3652143 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003963615058455616 15.321521472930907 0.8658236384391784 5.2634453773498535 3867540 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001299453157116659 26.33528881072998 0.7670907258987427 5.355251693725586 4085720 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018653105420526118 19.682338905334472 0.8135638177394867 5.591693258285522 4304324 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009006384570966475 18.202253150939942 0.6956139385700226 6.17093071937561 4521128 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012516391376266255 17.89346981048584 0.6975182175636292 5.928804731369018 4739739 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017103549107559957 21.024941062927248 0.5573164701461792 6.939863920211792 4959739 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013929700740845873 21.165422248840333 0.6238423943519592 6.064861869812011 5178723 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013507766532711684 21.609296989440917 0.5409651279449463 6.629524898529053 5398298 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005819168873131275 18.513759994506835 0.5670912623405456 6.7533745765686035 5617677 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -8.967784087872132e-05 15.897942733764648 0.4869859963655472 6.6181741714477536 5836254 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014286572535638698 19.7406436920166 0.44700327813625335 6.882130527496338 6054499 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00046919360320316625 15.512065219879151 0.4670206308364868 6.846758890151977 6274374 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00041067390848184007 19.37995262145996 0.43844548165798186 7.144666433334351 6494374 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00037620447692461313 18.439792823791503 0.4719441682100296 7.250233459472656 6714374 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010490348518942482 22.39357509613037 0.4707318305969238 7.283695077896118 6932464 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001436138179633417 17.657628440856932 0.5189893126487732 7.157769584655762 7152464 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021221296272415204 18.634352493286134 0.5991440296173096 7.327473306655884 7372464 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011966023303102702 19.522654247283935 0.5019870162010193 7.1846521377563475 7592300 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007524805288994685 18.632332801818848 0.5726355671882629 7.470382022857666 7812062 0


Pure best response payoff estimated to be 138.65 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 82.81 seconds to finish estimate with resulting utilities: [183.415   3.48 ]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 136.88 seconds to finish estimate with resulting utilities: [134.7   51.06]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 136.07 seconds to finish estimate with resulting utilities: [47.35  47.635]
Computing meta_strategies
Exited RRD with total regret 9.514501643553544 that was less than regret lambda 9.583333333333334 after 47 iterations 
REGRET STEPS:  25
NEW LAMBDA 9.166666666666668
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    49.79           4.47           3.48      
    1    191.15          94.68          51.06      
    2    183.41          134.70          47.49      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    49.79          191.15          183.41      
    1     4.47          94.68          134.70      
    2     3.48          51.06          47.49      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    99.57          195.62          186.89      
    1    195.62          189.36          185.76      
    2    186.89          185.76          94.98      

 

Metagame probabilities: 
Player #0: 0.0093  0.3494  0.6413  
Player #1: 0.0093  0.3494  0.6413  
Iteration : 2
Time so far: 20650.0333673954
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-07-20 20:12:58.825239: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022511471435427664 58.64923286437988 0.38629135489463806 9.74010591506958 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04229552522301674 17.745387458801268 0.875933188199997 7.008394432067871 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04235593006014824 19.65306854248047 0.9175672054290771 6.432234048843384 450944 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03579034432768822 17.649866580963135 0.8453108310699463 6.539278411865235 669775 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03191171586513519 23.518202018737792 0.7970992147922515 6.853804349899292 885869 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030303308553993703 22.120406341552734 0.828690379858017 6.448249483108521 1102790 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03218153826892376 19.85320701599121 0.9102133929729461 6.156441068649292 1317975 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025909654796123505 19.48395881652832 0.8106268227100373 6.400577592849731 1533690 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02618764005601406 19.402381896972656 0.9128714203834534 5.86681866645813 1751409 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019980665110051633 21.553192710876466 0.781190299987793 6.878144216537476 1968173 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020134674571454524 19.2686975479126 0.8038564562797547 6.2558738708496096 2181181 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020826926827430724 29.101168060302733 0.9094854056835174 5.551426124572754 2396554 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016065493877977132 15.251111125946045 0.8949556291103363 5.748862171173096 2609955 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015100255142897367 18.567233276367187 0.9624828398227692 5.579734897613525 2824574 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010386769846081733 23.513522720336915 0.7936833381652832 6.050934553146362 3041606 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009270024299621583 25.99128704071045 0.8683784604072571 6.0443055629730225 3258498 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0062878037802875045 19.34060173034668 0.8402147471904755 5.930886125564575 3474953 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004416820988990366 17.520019340515137 0.8322017312049865 6.204782867431641 3692487 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0020246951782610266 21.309287452697752 0.8567719221115112 5.869978713989258 3908953 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006742439931258559 21.711189651489256 0.6899521350860596 6.4392212390899655 4128849 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005373828578740358 30.909267616271972 0.656706589460373 6.771955299377441 4346454 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001223549539281521 16.143135643005373 0.49783133864402773 7.454421329498291 4558177 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007743970279989298 25.921960258483885 0.3889457255601883 7.789418935775757 4774328 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001130152097903192 26.60767021179199 0.38932892084121706 8.074354124069213 4990770 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009889891924103722 19.089179039001465 0.47977272868156434 7.566535902023316 5209356 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009703346659080125 22.782493591308594 0.3877346307039261 8.22691297531128 5426913 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008196670969482511 20.905343437194823 0.3766328305006027 8.143502283096314 5645691 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006842738570412621 29.859484481811524 0.3400522440671921 9.012996482849122 5865023 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001084598567103967 23.13882122039795 0.3416511297225952 9.062589073181153 6084378 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004878162522800267 30.33165798187256 0.25971323996782303 9.355286788940429 6304378 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018828775367637717 21.029137992858885 0.3378018975257874 8.58260202407837 6524378 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008136721968185157 17.70601396560669 0.4319987952709198 9.162894821166992 6743932 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007682362978812307 15.664801597595215 0.520574551820755 8.583428478240966 6963198 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008738084579817951 20.93698215484619 0.4179201781749725 8.98082618713379 7182003 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005096066393889487 22.839263916015625 0.42818128764629365 9.26203670501709 7401618 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009308170373515168 23.460575675964357 0.39912348091602323 8.858702659606934 7621486 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012715711454802658 21.09699401855469 0.42966482043266296 8.598061847686768 7838806 0


Pure best response payoff estimated to be 97.265 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 82.16 seconds to finish estimate with resulting utilities: [163.365   2.825]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 136.42 seconds to finish estimate with resulting utilities: [124.09   46.325]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 139.52 seconds to finish estimate with resulting utilities: [75.705 74.9  ]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 138.6 seconds to finish estimate with resulting utilities: [50.045 46.455]
Computing meta_strategies
Exited RRD with total regret 9.05303292408334 that was less than regret lambda 9.166666666666668 after 41 iterations 
REGRET STEPS:  25
NEW LAMBDA 8.750000000000002
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0    49.79           4.47           3.48           2.83      
    1    191.15          94.68          51.06          46.33      
    2    183.41          134.70          47.49          74.90      
    3    163.37          124.09          75.70          48.25      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0    49.79          191.15          183.41          163.37      
    1     4.47          94.68          134.70          124.09      
    2     3.48          51.06          47.49          75.70      
    3     2.83          46.33          74.90          48.25      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0    99.57          195.62          186.89          166.19      
    1    195.62          189.36          185.76          170.42      
    2    186.89          185.76          94.98          150.61      
    3    166.19          170.42          150.61          96.50      

 

Metagame probabilities: 
Player #0: 0.0122  0.2029  0.4164  0.3685  
Player #1: 0.0122  0.2029  0.4164  0.3685  
Iteration : 3
Time so far: 31452.199949979782
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-07-20 23:13:00.913365: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7779 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013438083324581384 39.29824714660644 0.24563418924808503 11.688601303100587 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03840691149234772 14.505857944488525 0.8142874062061309 6.672469711303711 225161 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.038260245323181154 18.10293617248535 0.8311236798763275 6.6857575416564945 437048 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03410966377705336 16.95298261642456 0.870738685131073 6.2069878578186035 647342 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035982051864266396 16.2711238861084 0.873399692773819 6.3576373100280765 857075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03585738688707352 16.75756597518921 0.9492136120796204 6.227209520339966 1067595 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029509628564119338 22.742943954467773 0.8186318576335907 6.634861946105957 1277858 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03006441667675972 20.515421867370605 0.9228923082351684 6.0637476444244385 1489571 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023538944497704505 18.432199478149414 0.7973742842674255 6.852984809875489 1699313 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023263117857277395 18.154637622833253 0.8601521611213684 6.716443967819214 1908747 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019091654010117053 22.536507034301756 0.7544313848018647 6.98297233581543 2119334 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016756345331668854 27.70195064544678 0.7395138561725616 7.271820831298828 2330142 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017128976341336966 21.62071647644043 0.8786497592926026 6.357520818710327 2539646 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015723519306629897 17.89179801940918 0.8884597480297088 6.282775211334228 2751814 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013701493758708239 15.272576236724854 0.9078993678092957 6.430256271362305 2962892 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008256344869732857 20.642482948303222 0.6072314441204071 8.309327363967896 3173373 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007130615226924419 20.441703033447265 0.796590381860733 6.827550792694092 3382682 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005937241576611995 16.642698574066163 0.7415342688560486 7.405514144897461 3590915 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002317037316970527 26.78861198425293 0.623976755142212 8.089022016525268 3802591 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013641494821058585 26.697136688232423 0.6248109459877014 8.299831676483155 4011080 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008332300843903795 24.82427558898926 0.5326407551765442 7.558947134017944 4221347 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0012121789579396137 18.228901290893553 0.7185714185237885 7.183467102050781 4429312 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005199830886340351 20.051049995422364 0.5452007114887237 8.112814283370971 4639383 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -7.105753575160634e-05 23.99345531463623 0.5501421719789505 8.185324239730836 4849361 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015846558439079673 19.305182647705077 0.5113825649023056 8.092408752441406 5058819 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013426960416836665 13.789296340942382 0.5469496816396713 7.6576722145080565 5268758 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007893071116995998 19.014239883422853 0.5774377644062042 7.498024034500122 5478965 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008664748136652633 24.974048805236816 0.5399550020694732 7.688823509216308 5688849 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006663485917670186 24.454395484924316 0.4449531674385071 9.415118598937989 5896854 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002552634584571933 19.79846782684326 0.41070530414581297 8.7490797996521 6105421 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00045029777829768135 26.773836517333983 0.4121065318584442 8.92402000427246 6318452 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015789242082973943 18.588922882080077 0.4000344067811966 8.541025257110595 6529944 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005820352584123611 22.828095626831054 0.2936944752931595 9.302205085754395 6740363 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016897720226552338 19.74538125991821 0.3844322502613068 8.671234703063964 6950385 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00014075576546019876 26.36237335205078 0.35405872464179994 9.67803602218628 7162364 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010528742473979945 26.885731506347657 0.3383484840393066 9.211294460296632 7371676 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012564720382215455 24.585861587524413 0.37754223942756654 8.979359912872315 7581861 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006028580417478224 18.86147270202637 0.39169599413871764 9.460787868499756 7790738 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00031944696675054727 24.583198165893556 0.28556292951107026 9.069986820220947 7999172 0


Pure best response payoff estimated to be 85.855 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 62.29 seconds to finish estimate with resulting utilities: [130.055   2.96 ]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 111.61 seconds to finish estimate with resulting utilities: [100.13   39.325]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 129.84 seconds to finish estimate with resulting utilities: [74.42  57.615]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 122.28 seconds to finish estimate with resulting utilities: [78.585 63.95 ]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 121.48 seconds to finish estimate with resulting utilities: [39.01 38.86]
Computing meta_strategies
Exited RRD with total regret 8.687886542480157 that was less than regret lambda 8.750000000000002 after 38 iterations 
REGRET STEPS:  25
NEW LAMBDA 8.333333333333336
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0    49.79           4.47           3.48           2.83           2.96      
    1    191.15          94.68          51.06          46.33          39.33      
    2    183.41          134.70          47.49          74.90          57.62      
    3    163.37          124.09          75.70          48.25          63.95      
    4    130.06          100.13          74.42          78.58          38.94      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0    49.79          191.15          183.41          163.37          130.06      
    1     4.47          94.68          134.70          124.09          100.13      
    2     3.48          51.06          47.49          75.70          74.42      
    3     2.83          46.33          74.90          48.25          78.58      
    4     2.96          39.33          57.62          63.95          38.94      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0    99.57          195.62          186.89          166.19          133.02      
    1    195.62          189.36          185.76          170.42          139.45      
    2    186.89          185.76          94.98          150.61          132.03      
    3    166.19          170.42          150.61          96.50          142.53      
    4    133.02          139.45          132.03          142.53          77.87      

 

Metagame probabilities: 
Player #0: 0.0142  0.1552  0.305  0.2892  0.2363  
Player #1: 0.0142  0.1552  0.305  0.2892  0.2363  
Iteration : 4
Time so far: 41326.80964946747
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-07-21 01:57:35.778939: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9757 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025299661234021185 50.037231063842775 0.455178901553154 9.739858818054199 10013 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035743994638323784 21.258906364440918 0.7268440783023834 7.8603216171264645 218867 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029899687506258488 25.62358913421631 0.6543395936489105 8.400419330596923 428241 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03380663245916367 25.728247261047365 0.7624513149261475 7.568544006347656 636969 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028608211316168308 19.921613121032713 0.6985342502593994 8.812808322906495 845885 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022255998849868775 33.57926445007324 0.5847464501857758 9.220320510864259 1056555 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02510457132011652 29.304612159729004 0.6812796294689178 8.535434246063232 1267478 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0258021617308259 28.85706310272217 0.7892942011356354 7.607538557052612 1478810 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021006613783538342 36.891775894165036 0.6283208549022674 8.126927757263184 1688267 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021179860830307005 28.812822341918945 0.7530722439289093 7.709287786483765 1896015 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016795165743678807 30.791855239868163 0.6685283482074738 8.090057182312012 2104223 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01667962037026882 21.238605880737303 0.7391653656959534 7.501154851913452 2312848 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013853394892066717 19.99187068939209 0.7007835447788239 7.449813747406006 2521739 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012069318257272244 23.2380033493042 0.6707915663719177 7.6367881298065186 2730138 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010230942023918032 30.694206047058106 0.6441795587539673 8.313662958145141 2941537 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010370532888919115 18.113842296600343 0.7804437518119812 7.4346014022827145 3150694 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007806947221979499 21.08319664001465 0.796005654335022 7.528474807739258 3359828 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004270353878382593 20.677202033996583 0.6442148745059967 8.23915433883667 3570707 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0028146125609055162 16.385210037231445 0.6964092075824737 7.678503704071045 3780509 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019253653852501884 20.608883666992188 0.5407458037137985 8.389046573638916 3990485 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007879246986703947 22.50638790130615 0.6050256729125977 8.189303588867187 4200255 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014641035231761634 21.5763729095459 0.5778543531894684 7.690329217910767 4409994 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00020125794035266153 22.40306701660156 0.45712596774101255 8.104702758789063 4620849 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008493122513755225 22.850458717346193 0.28657522052526474 9.070131587982178 4830779 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00106709763786057 22.39718532562256 0.35858577489852905 8.829251956939697 5040689 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -7.8532602674386e-05 32.83055419921875 0.3413633406162262 9.491927433013917 5252588 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007406861332128756 24.363265991210938 0.3799665629863739 9.345787620544433 5463266 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013945827609859406 19.808151054382325 0.48337228000164034 8.481663417816161 5673203 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007706991076702252 20.929499053955077 0.4442477971315384 8.813713550567627 5883687 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008863706827469287 19.76958065032959 0.3987264335155487 9.020457649230957 6094557 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014233423498808406 24.936299133300782 0.22247186303138733 9.893609523773193 6303794 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00010084786590596196 16.513219261169432 0.3029239624738693 8.89065179824829 6514014 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00034229308439535087 21.690254592895506 0.40691969096660613 9.265825843811035 6725487 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00037582803633995354 22.98592128753662 0.29278149008750914 9.06433801651001 6934902 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004406335181556642 31.68310489654541 0.1711154043674469 10.16987133026123 7144704 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000545156910084188 14.683008003234864 0.13887881189584733 9.700513935089111 7355783 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0036662193757365457 32.63546447753906 0.20248352140188217 10.622648906707763 7563218 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00111282438156195 21.547770500183105 0.11769088208675385 10.486852359771728 7773140 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003986461633758154 30.785314559936523 0.082699204236269 11.32197208404541 7985059 0


Pure best response payoff estimated to be 84.325 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 61.04 seconds to finish estimate with resulting utilities: [127.35   3.24]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 106.71 seconds to finish estimate with resulting utilities: [99.925 39.83 ]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 126.31 seconds to finish estimate with resulting utilities: [69.45 50.37]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 116.68 seconds to finish estimate with resulting utilities: [80.635 58.375]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 116.23 seconds to finish estimate with resulting utilities: [63.76  60.965]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 113.25 seconds to finish estimate with resulting utilities: [60.705 60.565]
Computing meta_strategies
Exited RRD with total regret 8.32490789898597 that was less than regret lambda 8.333333333333336 after 158 iterations 
REGRET STEPS:  25
NEW LAMBDA 7.916666666666669
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0    49.79           4.47           3.48           2.83           2.96           3.24      
    1    191.15          94.68          51.06          46.33          39.33          39.83      
    2    183.41          134.70          47.49          74.90          57.62          50.37      
    3    163.37          124.09          75.70          48.25          63.95          58.38      
    4    130.06          100.13          74.42          78.58          38.94          60.97      
    5    127.35          99.92          69.45          80.64          63.76          60.63      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0    49.79          191.15          183.41          163.37          130.06          127.35      
    1     4.47          94.68          134.70          124.09          100.13          99.92      
    2     3.48          51.06          47.49          75.70          74.42          69.45      
    3     2.83          46.33          74.90          48.25          78.58          80.64      
    4     2.96          39.33          57.62          63.95          38.94          63.76      
    5     3.24          39.83          50.37          58.38          60.97          60.63      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0    99.57          195.62          186.89          166.19          133.02          130.59      
    1    195.62          189.36          185.76          170.42          139.45          139.75      
    2    186.89          185.76          94.98          150.61          132.03          119.82      
    3    166.19          170.42          150.61          96.50          142.53          139.01      
    4    133.02          139.45          132.03          142.53          77.87          124.72      
    5    130.59          139.75          119.82          139.01          124.72          121.27      

 

Metagame probabilities: 
Player #0: 0.0001  0.0133  0.1527  0.2128  0.2089  0.4123  
Player #1: 0.0001  0.0133  0.1527  0.2128  0.2089  0.4123  
Iteration : 5
Time so far: 51346.65444302559
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-07-21 04:44:35.471301: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11735 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023667135648429392 57.42392768859863 0.45458764135837554 9.86138458251953 10275 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031622851453721526 20.11528034210205 0.6524093747138977 8.524455308914185 220825 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02788437660783529 22.369886207580567 0.6056911736726761 8.804347133636474 432865 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025241303630173205 34.04149227142334 0.5835886657238006 8.431735801696778 643171 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027706789225339888 23.3414644241333 0.6772673785686493 8.002317523956298 853884 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026061717979609968 17.131383323669432 0.6949430584907532 7.479083156585693 1067164 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02700093314051628 17.33387689590454 0.7568704843521118 7.360341215133667 1277866 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02742773313075304 14.954996585845947 0.864763593673706 6.717193078994751 1489951 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022889179550111292 19.996211051940918 0.7500027477741241 7.2819031238555905 1702106 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02163418158888817 15.443276405334473 0.7523960769176483 7.194374322891235 1911410 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016260273195803164 21.324655723571777 0.6812956213951111 8.053059911727905 2121689 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01466894755139947 23.9147159576416 0.6706727862358093 7.940896081924438 2331934 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012427607271820306 25.126490211486818 0.6576012909412384 8.207780027389527 2543722 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009908587206155062 23.945558547973633 0.5705615609884263 8.713722324371338 2756389 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010382499173283577 21.451494026184083 0.7588976681232452 7.3652918338775635 2968899 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00702994498424232 22.507497215270995 0.5845767557621002 8.623486232757568 3180272 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0055536606349051 30.195990371704102 0.6137322247028351 8.710811996459961 3388643 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004267585149500519 21.585068893432616 0.6155343294143677 8.52016739845276 3598969 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002490250067785382 21.519448661804198 0.6282665193080902 8.60869312286377 3809546 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011565517736016773 13.168975925445556 0.726911985874176 7.350424718856812 4021425 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015952832764014602 14.838597106933594 0.451055645942688 8.537055397033692 4230383 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005564036866417155 26.39796676635742 0.28181437849998475 9.219691181182862 4440362 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0009944972680386854 28.1835054397583 0.3097118645906448 9.467854309082032 4649452 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005664745862304698 24.049848365783692 0.4664906919002533 8.474097204208373 4857013 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0032545255846343933 24.159557914733888 0.458721798658371 9.028638172149659 5064856 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007616577597218566 39.336572265625 0.3253323405981064 10.767646217346192 5273270 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0023535917687695475 29.995413780212402 0.311763396859169 9.708543300628662 5485065 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007057178729155567 22.788752174377443 0.4299758553504944 9.145520210266113 5695230 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00021165245489100925 29.39809169769287 0.33036876320838926 10.063720417022704 5904013 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004003904869023245 25.460009384155274 0.29609171450138094 10.08902997970581 6114812 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -3.11751791741699e-06 17.05869588851929 0.3717216938734055 9.974417304992675 6324299 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015645732485609187 28.805703926086426 0.2541544273495674 9.968987083435058 6534680 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008469101449009031 18.05840892791748 0.34846945106983185 9.590922355651855 6746171 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012147623230703176 19.443905639648438 0.37663269937038424 9.622055053710938 6953958 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002929251073510386 16.709816455841064 0.29969796538352966 10.137051582336426 7164334 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002083611287525855 26.14193229675293 0.2913506686687469 10.569957542419434 7376455 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -7.896606548456475e-05 24.70195655822754 0.25830748677253723 10.957474899291991 7587575 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013288356451084838 13.37764835357666 0.14277413189411164 10.440290832519532 7797855 0


Pure best response payoff estimated to be 80.795 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 72.61 seconds to finish estimate with resulting utilities: [142.885   3.955]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 122.4 seconds to finish estimate with resulting utilities: [103.93   45.895]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 141.53 seconds to finish estimate with resulting utilities: [74.285 52.01 ]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 136.84 seconds to finish estimate with resulting utilities: [89.46 64.56]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 125.61 seconds to finish estimate with resulting utilities: [63.79 54.6 ]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 124.62 seconds to finish estimate with resulting utilities: [75.12  62.775]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 120.57 seconds to finish estimate with resulting utilities: [67.645 64.635]
Computing meta_strategies
Exited RRD with total regret 7.881020862350397 that was less than regret lambda 7.916666666666669 after 178 iterations 
REGRET STEPS:  25
NEW LAMBDA 7.500000000000002
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    49.79           4.47           3.48           2.83           2.96           3.24           3.96      
    1    191.15          94.68          51.06          46.33          39.33          39.83          45.90      
    2    183.41          134.70          47.49          74.90          57.62          50.37          52.01      
    3    163.37          124.09          75.70          48.25          63.95          58.38          64.56      
    4    130.06          100.13          74.42          78.58          38.94          60.97          54.60      
    5    127.35          99.92          69.45          80.64          63.76          60.63          62.77      
    6    142.88          103.93          74.28          89.46          63.79          75.12          66.14      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    49.79          191.15          183.41          163.37          130.06          127.35          142.88      
    1     4.47          94.68          134.70          124.09          100.13          99.92          103.93      
    2     3.48          51.06          47.49          75.70          74.42          69.45          74.28      
    3     2.83          46.33          74.90          48.25          78.58          80.64          89.46      
    4     2.96          39.33          57.62          63.95          38.94          63.76          63.79      
    5     3.24          39.83          50.37          58.38          60.97          60.63          75.12      
    6     3.96          45.90          52.01          64.56          54.60          62.77          66.14      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0    99.57          195.62          186.89          166.19          133.02          130.59          146.84      
    1    195.62          189.36          185.76          170.42          139.45          139.75          149.83      
    2    186.89          185.76          94.98          150.61          132.03          119.82          126.29      
    3    166.19          170.42          150.61          96.50          142.53          139.01          154.02      
    4    133.02          139.45          132.03          142.53          77.87          124.72          118.39      
    5    130.59          139.75          119.82          139.01          124.72          121.27          137.90      
    6    146.84          149.83          126.29          154.02          118.39          137.90          132.28      

 

Metagame probabilities: 
Player #0: 0.0001  0.0051  0.0494  0.1196  0.0698  0.1865  0.5696  
Player #1: 0.0001  0.0051  0.0494  0.1196  0.0698  0.1865  0.5696  
Iteration : 6
Time so far: 60925.98733353615
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-07-21 07:24:15.315901: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13713 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031064487248659133 68.65018310546876 0.5965999215841293 10.35415563583374 10384 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030086201056838034 18.948834800720213 0.6284883618354797 9.336048984527588 221044 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03369323909282684 18.471957397460937 0.7315586686134339 8.73124132156372 430554 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034270083718001845 16.421623134613036 0.8281692624092102 7.969400358200073 640913 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029993089847266674 18.12230224609375 0.7564315795898438 8.315723323822022 849920 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032059314846992495 16.808949756622315 0.8631651163101196 7.435613441467285 1060453 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025755477882921696 29.799499702453613 0.7286986827850341 8.532803916931153 1270079 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026449815183877946 13.672288036346435 0.8274579167366027 7.847642326354981 1479632 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025047156400978564 25.631833457946776 0.82628692984581 7.994905710220337 1690298 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018310696072876453 26.85066165924072 0.6863884449005127 8.765642642974854 1899048 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0192909536883235 15.318193912506104 0.8176721274852753 7.668915414810181 2109001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015398056246340275 18.685573196411134 0.722037273645401 8.45137815475464 2319886 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015003634337335824 15.930991172790527 0.8069410920143127 7.683359050750733 2530073 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010626306850463152 28.408511924743653 0.6487608850002289 9.335038948059083 2741411 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009009314561262726 21.39714298248291 0.6574497222900391 8.793586444854736 2949852 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007738849706947803 39.302616882324216 0.7377660214900971 8.359812927246093 3162048 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006800510128960014 23.098991012573244 0.7012931942939759 8.474041366577149 3374373 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003753832867369056 19.26532115936279 0.7979706466197968 8.066714859008789 3584428 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002271138195646927 22.493724822998047 0.5842141389846802 9.519073677062988 3794960 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0006388709240127355 15.831660175323487 0.7156701743602752 8.64096565246582 4004875 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008545240769308293 16.47072477340698 0.6372938454151154 8.91277256011963 4214670 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012316744177951477 25.4802490234375 0.46158879399299624 9.943782615661622 4425420 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00018554379057604818 35.75336627960205 0.37099865078926086 11.312899971008301 4635885 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012746090469590854 18.939927959442137 0.32427566945552827 10.069170188903808 4847648 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00026710262845881517 17.258092594146728 0.4917666643857956 10.32484998703003 5056110 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000587422629178036 29.680513000488283 0.45872409641742706 10.56037187576294 5265466 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004163631238043308 20.124284172058104 0.4673196643590927 10.504150676727296 5472080 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00034365898063697385 18.74725227355957 0.4905232608318329 10.531417083740234 5681586 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011706251592841 31.726253700256347 0.4192423850297928 11.664253330230713 5890575 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008615370919869747 14.963169574737549 0.5460347831249237 10.357451438903809 6100181 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014472673574346118 18.256738662719727 0.2914973348379135 11.016260814666747 6311051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010279938052008219 21.45645236968994 0.4149874925613403 10.681406688690185 6519961 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001072954152186867 16.05407428741455 0.24923399686813355 10.806753540039063 6730483 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008473141067952383 17.5735107421875 0.19226152896881105 10.627500534057617 6941670 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010176650859648362 29.985097122192382 0.12493149191141129 11.57845516204834 7152660 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005467562328703935 13.960444831848145 0.2600006341934204 10.796694946289062 7361887 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00030596495198551563 18.72354679107666 0.2966002345085144 11.178371810913086 7570452 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00026235964469378814 19.883724021911622 0.2711367726325989 11.385680866241454 7777418 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00022918273061804938 17.892779636383057 0.33767358362674715 10.822609806060791 7986038 0


Pure best response payoff estimated to be 85.19 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 66.77 seconds to finish estimate with resulting utilities: [130.925   3.45 ]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 122.3 seconds to finish estimate with resulting utilities: [103.635  47.765]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 139.41 seconds to finish estimate with resulting utilities: [72.845 56.105]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 134.52 seconds to finish estimate with resulting utilities: [90.88 65.75]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 117.27 seconds to finish estimate with resulting utilities: [61.545 49.58 ]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 117.43 seconds to finish estimate with resulting utilities: [72.41  58.535]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 127.01 seconds to finish estimate with resulting utilities: [82.38 67.62]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 117.43 seconds to finish estimate with resulting utilities: [68.305 70.93 ]
Computing meta_strategies
Exited RRD with total regret 7.467182370246519 that was less than regret lambda 7.500000000000002 after 198 iterations 
REGRET STEPS:  25
NEW LAMBDA 7.083333333333335
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    49.79           4.47           3.48           2.83           2.96           3.24           3.96           3.45      
    1    191.15          94.68          51.06          46.33          39.33          39.83          45.90          47.77      
    2    183.41          134.70          47.49          74.90          57.62          50.37          52.01          56.10      
    3    163.37          124.09          75.70          48.25          63.95          58.38          64.56          65.75      
    4    130.06          100.13          74.42          78.58          38.94          60.97          54.60          49.58      
    5    127.35          99.92          69.45          80.64          63.76          60.63          62.77          58.53      
    6    142.88          103.93          74.28          89.46          63.79          75.12          66.14          67.62      
    7    130.93          103.64          72.84          90.88          61.55          72.41          82.38          69.62      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    49.79          191.15          183.41          163.37          130.06          127.35          142.88          130.93      
    1     4.47          94.68          134.70          124.09          100.13          99.92          103.93          103.64      
    2     3.48          51.06          47.49          75.70          74.42          69.45          74.28          72.84      
    3     2.83          46.33          74.90          48.25          78.58          80.64          89.46          90.88      
    4     2.96          39.33          57.62          63.95          38.94          63.76          63.79          61.55      
    5     3.24          39.83          50.37          58.38          60.97          60.63          75.12          72.41      
    6     3.96          45.90          52.01          64.56          54.60          62.77          66.14          82.38      
    7     3.45          47.77          56.10          65.75          49.58          58.53          67.62          69.62      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0    99.57          195.62          186.89          166.19          133.02          130.59          146.84          134.38      
    1    195.62          189.36          185.76          170.42          139.45          139.75          149.83          151.40      
    2    186.89          185.76          94.98          150.61          132.03          119.82          126.29          128.95      
    3    166.19          170.42          150.61          96.50          142.53          139.01          154.02          156.63      
    4    133.02          139.45          132.03          142.53          77.87          124.72          118.39          111.12      
    5    130.59          139.75          119.82          139.01          124.72          121.27          137.90          130.94      
    6    146.84          149.83          126.29          154.02          118.39          137.90          132.28          150.00      
    7    134.38          151.40          128.95          156.63          111.12          130.94          150.00          139.24      

 

Metagame probabilities: 
Player #0: 0.0001  0.0021  0.0209  0.0693  0.0172  0.0625  0.2633  0.5645  
Player #1: 0.0001  0.0021  0.0209  0.0693  0.0172  0.0625  0.2633  0.5645  
Iteration : 7
Time so far: 71597.34725952148
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-07-21 10:22:06.808369: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15691 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029178893007338048 50.34727020263672 0.550742295384407 10.835215950012207 10044 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032115029729902746 15.925311279296874 0.6674594700336456 10.111604404449462 219497 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03251289110630751 20.04879264831543 0.6808196723461151 9.697337913513184 431017 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032523454539477824 22.054394149780272 0.7324198186397552 9.280220699310302 640998 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029154621809720994 15.279451656341553 0.7062324583530426 8.977305507659912 849757 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025236988626420497 21.40788516998291 0.6842406690120697 8.971497917175293 1059204 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027436023205518724 17.948620986938476 0.7676059424877166 8.647515106201173 1269258 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022149595059454442 19.414237022399902 0.6896956562995911 9.739337158203124 1479687 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02269061394035816 17.955169105529784 0.7810138285160064 8.69176664352417 1690228 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01995618212968111 17.773410701751708 0.7211829364299774 8.808457088470458 1898829 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018048870470374824 23.00832004547119 0.6589293539524078 9.064527225494384 2107959 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017080612294375897 15.73567008972168 0.7437009036540985 8.719244766235352 2316352 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017122298292815686 12.377213668823241 0.8641089916229248 8.004248762130738 2524936 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012703950051218271 19.201355171203613 0.7439433872699738 8.525713348388672 2734547 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011124962661415338 24.46542110443115 0.7256731510162353 9.167713356018066 2942338 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009314183704555034 23.20105972290039 0.7223018705844879 8.888110637664795 3153487 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00801506619900465 27.179254722595214 0.701768445968628 9.258174514770507 3361344 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003581086127087474 24.042687034606935 0.6093701004981995 10.118638610839843 3570752 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0032886162865906955 19.929711723327635 0.6833420634269715 9.487922286987304 3781235 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000235937416437082 15.646386814117431 0.8159610688686371 8.272375869750977 3989902 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00017133924629888497 30.92668685913086 0.5528300076723098 9.8003436088562 4198195 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011018113033969712 13.35201997756958 0.5112903952598572 9.615215587615968 4407685 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010373191798862536 14.691123962402344 0.6033487498760224 9.035645580291748 4615701 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001210026687476784 13.139300537109374 0.5202768921852112 10.16153039932251 4825204 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001540483016287908 14.244730377197266 0.5386270165443421 10.304651069641114 5033052 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  1.0900886991294101e-05 12.127572917938233 0.5503777295351029 9.542221832275391 5244473 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009312726600910537 30.418716812133788 0.3843067139387131 10.526645374298095 5453542 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008952966920332984 20.584428787231445 0.33455443680286406 10.619068813323974 5665141 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009531801650155102 23.34524917602539 0.10279938131570816 11.793920421600342 5875423 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001003964670599089 13.102472972869872 0.18751530945301056 10.892782878875732 6086669 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005926166180870496 17.392809867858887 0.12342515587806702 11.363078594207764 6297622 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007632000316903032 21.190418815612794 0.32440946698188783 10.938731670379639 6506244 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00015201827254713863 41.23008766174316 0.19678758978843688 11.68798894882202 6717300 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000857644627103582 31.711507415771486 0.2866829887032509 11.355465507507324 6925951 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000644180931703886 22.922562217712404 0.16632348001003266 11.122587299346923 7134490 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0023965182597748937 29.541175651550294 0.10901522859930993 11.58109073638916 7347240 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005780737286841031 18.13715305328369 0.06324341967701912 11.870333862304687 7556758 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  9.27553090150468e-05 25.868245315551757 0.2151579201221466 11.258927822113037 7765112 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0010393922639195807 18.117746448516847 0.240696419775486 11.027163887023926 7976717 0


Pure best response payoff estimated to be 80.725 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (8, 0)
Current player 0 and current strategies (8, 0) took 63.45 seconds to finish estimate with resulting utilities: [134.695   3.075]
Estimating current strategies:  (8, 1)
Current player 0 and current strategies (8, 1) took 120.18 seconds to finish estimate with resulting utilities: [101.655  48.395]
Estimating current strategies:  (8, 2)
Current player 0 and current strategies (8, 2) took 132.78 seconds to finish estimate with resulting utilities: [64.855 53.015]
Estimating current strategies:  (8, 3)
Current player 0 and current strategies (8, 3) took 128.25 seconds to finish estimate with resulting utilities: [85.26  58.145]
Estimating current strategies:  (8, 4)
Current player 0 and current strategies (8, 4) took 111.9 seconds to finish estimate with resulting utilities: [71.995 58.845]
Estimating current strategies:  (8, 5)
Current player 0 and current strategies (8, 5) took 115.49 seconds to finish estimate with resulting utilities: [70.925 57.125]
Estimating current strategies:  (8, 6)
Current player 0 and current strategies (8, 6) took 126.05 seconds to finish estimate with resulting utilities: [80.945 65.005]
Estimating current strategies:  (8, 7)
Current player 0 and current strategies (8, 7) took 119.85 seconds to finish estimate with resulting utilities: [73.205 68.415]
Estimating current strategies:  (8, 8)
Current player 0 and current strategies (8, 8) took 117.49 seconds to finish estimate with resulting utilities: [58.685 58.455]
Computing meta_strategies
Exited RRD with total regret 7.047051122810942 that was less than regret lambda 7.083333333333335 after 183 iterations 
REGRET STEPS:  25
NEW LAMBDA 6.666666666666668
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    49.79           4.47           3.48           2.83           2.96           3.24           3.96           3.45           3.08      
    1    191.15          94.68          51.06          46.33          39.33          39.83          45.90          47.77          48.40      
    2    183.41          134.70          47.49          74.90          57.62          50.37          52.01          56.10          53.02      
    3    163.37          124.09          75.70          48.25          63.95          58.38          64.56          65.75          58.15      
    4    130.06          100.13          74.42          78.58          38.94          60.97          54.60          49.58          58.84      
    5    127.35          99.92          69.45          80.64          63.76          60.63          62.77          58.53          57.12      
    6    142.88          103.93          74.28          89.46          63.79          75.12          66.14          67.62          65.00      
    7    130.93          103.64          72.84          90.88          61.55          72.41          82.38          69.62          68.42      
    8    134.69          101.66          64.86          85.26          72.00          70.92          80.94          73.20          58.57      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    49.79          191.15          183.41          163.37          130.06          127.35          142.88          130.93          134.69      
    1     4.47          94.68          134.70          124.09          100.13          99.92          103.93          103.64          101.66      
    2     3.48          51.06          47.49          75.70          74.42          69.45          74.28          72.84          64.86      
    3     2.83          46.33          74.90          48.25          78.58          80.64          89.46          90.88          85.26      
    4     2.96          39.33          57.62          63.95          38.94          63.76          63.79          61.55          72.00      
    5     3.24          39.83          50.37          58.38          60.97          60.63          75.12          72.41          70.92      
    6     3.96          45.90          52.01          64.56          54.60          62.77          66.14          82.38          80.94      
    7     3.45          47.77          56.10          65.75          49.58          58.53          67.62          69.62          73.20      
    8     3.08          48.40          53.02          58.15          58.84          57.12          65.00          68.42          58.57      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8      
    0    99.57          195.62          186.89          166.19          133.02          130.59          146.84          134.38          137.77      
    1    195.62          189.36          185.76          170.42          139.45          139.75          149.83          151.40          150.05      
    2    186.89          185.76          94.98          150.61          132.03          119.82          126.29          128.95          117.87      
    3    166.19          170.42          150.61          96.50          142.53          139.01          154.02          156.63          143.41      
    4    133.02          139.45          132.03          142.53          77.87          124.72          118.39          111.12          130.84      
    5    130.59          139.75          119.82          139.01          124.72          121.27          137.90          130.94          128.05      
    6    146.84          149.83          126.29          154.02          118.39          137.90          132.28          150.00          145.95      
    7    134.38          151.40          128.95          156.63          111.12          130.94          150.00          139.24          141.62      
    8    137.77          150.05          117.87          143.41          130.84          128.05          145.95          141.62          117.14      

 

Metagame probabilities: 
Player #0: 0.0001  0.003  0.0204  0.0574  0.0214  0.0518  0.1979  0.381  0.267  
Player #1: 0.0001  0.003  0.0204  0.0574  0.0214  0.0518  0.1979  0.381  0.267  
Iteration : 8
Time so far: 82108.66501402855
Approximating Best Response
Training best response:  True 0.06
rtg values check:  (?,) (?, 1)
2023-07-21 13:17:17.853361: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_62/weights_2/Adam_1/Assign' id:17669 op device:{requested: '', assigned: ''} def:{{{node mlp_62/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_62/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_62/weights_2/Adam_1, mlp_62/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02277726074680686 53.78776321411133 0.45660771429538727 12.136436271667481 10966 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03479832075536251 14.492342853546143 0.7291946470737457 9.938570976257324 221771 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026917395554482935 17.2223162651062 0.6238022208213806 10.635900688171386 431271 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03057102169841528 18.786764717102052 0.7003694772720337 10.294635581970216 641887 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02820787951350212 20.79711322784424 0.6984968781471252 9.863204002380371 853156 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025265841744840145 20.161675453186035 0.6565340518951416 9.748182582855225 1061394 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024532531574368476 14.934968948364258 0.6972773134708404 9.37625494003296 1273170 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023463230952620505 25.383966064453126 0.7382200181484222 9.348047828674316 1485693 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022598651237785816 13.600040435791016 0.7473866701126098 9.207571315765382 1697226 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02000425085425377 21.365607261657715 0.6783157229423523 9.642694473266602 1904907 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016893676202744245 15.80960454940796 0.6898095190525055 9.662803268432617 2114189 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01973448656499386 17.89940595626831 0.8891345977783203 8.590858268737794 2320845 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015385079197585583 15.136760997772218 0.789869773387909 8.832273006439209 2529363 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010631975624710322 18.593971824645998 0.6613674819469452 9.412453269958496 2737213 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009074409632012248 33.261763954162596 0.6663778007030488 9.987816047668456 2946844 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007205005455762148 18.75608310699463 0.6651906311511994 9.440028095245362 3156459 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006160041410475969 17.260478591918947 0.7037185490131378 9.289424705505372 3365175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0040241720154881476 32.03857250213623 0.6683585166931152 9.725891494750977 3575554 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022148209041915833 15.93298044204712 0.6426278352737427 9.93289155960083 3785052 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006506125871965196 13.239165115356446 0.7425217270851135 9.109167289733886 3994120 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015852556010941044 20.619189834594728 0.5378261417150497 9.622390270233154 4203504 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00017107536841649563 20.39157905578613 0.5684182703495025 9.684147453308105 4412842 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007732215686701238 17.536581230163574 0.5620868504047394 9.786007595062255 4624036 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005961016951914644 18.469525718688963 0.28132962733507155 10.266203784942627 4834161 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007944490294903517 16.525196170806886 0.3105654150247574 9.856489372253417 5043254 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008055718564719428 13.33058042526245 0.30225282311439516 10.847726535797118 5253023 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001438865181989968 15.526661205291749 0.39327695667743684 10.271379566192627 5464025 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007595667091663926 21.712916755676268 0.41536924839019773 10.413405990600586 5673731 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -9.094496563193389e-05 14.027980327606201 0.4299883902072906 10.283955574035645 5884160 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003132983671821421 19.523846435546876 0.3066507488489151 10.822010612487793 6092115 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000540971528243972 23.825849723815917 0.20722174346446992 10.831852054595947 6303969 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0035823299156618306 20.53356800079346 0.33060906231403353 10.333381462097169 6514221 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001582279140711762 23.457218742370607 0.24514650255441667 10.990569400787354 6724984 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0001391493817209266 28.857321739196777 0.15996783524751662 11.239924430847168 6934191 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008352384345926111 27.999047470092773 0.12379934191703797 11.77016315460205 7144389 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00013830969874106813 12.179735565185547 0.18219843208789827 10.76278715133667 7355732 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0020324722048826517 25.739319610595704 0.2368027910590172 11.389029121398925 7561070 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0007653752370970323 20.0183967590332 0.24813401252031325 11.236513137817383 7770922 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012353080353932455 13.806927585601807 0.31336189806461334 11.085982036590575 7980833 0


Pure best response payoff estimated to be 79.225 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (9, 0)
Current player 0 and current strategies (9, 0) took 65.61 seconds to finish estimate with resulting utilities: [134.37    3.405]
Estimating current strategies:  (9, 1)
Current player 0 and current strategies (9, 1) took 116.82 seconds to finish estimate with resulting utilities: [101.28   47.285]
Estimating current strategies:  (9, 2)
Current player 0 and current strategies (9, 2) took 136.09 seconds to finish estimate with resulting utilities: [73.915 56.09 ]
Estimating current strategies:  (9, 3)
Current player 0 and current strategies (9, 3) took 128.61 seconds to finish estimate with resulting utilities: [90.04  63.575]
Estimating current strategies:  (9, 4)
Current player 0 and current strategies (9, 4) took 118.76 seconds to finish estimate with resulting utilities: [72.35 60.79]
Estimating current strategies:  (9, 5)
Current player 0 and current strategies (9, 5) took 116.42 seconds to finish estimate with resulting utilities: [77.265 63.735]
Estimating current strategies:  (9, 6)
Current player 0 and current strategies (9, 6) took 120.9 seconds to finish estimate with resulting utilities: [80.81 66.97]
Estimating current strategies:  (9, 7)
Current player 0 and current strategies (9, 7) took 122.2 seconds to finish estimate with resulting utilities: [77.02 76.17]
Estimating current strategies:  (9, 8)
Current player 0 and current strategies (9, 8) took 118.4 seconds to finish estimate with resulting utilities: [71.375 72.605]
Estimating current strategies:  (9, 9)
Current player 0 and current strategies (9, 9) took 121.61 seconds to finish estimate with resulting utilities: [77.785 78.265]
Computing meta_strategies
Exited RRD with total regret 6.6298630917588355 that was less than regret lambda 6.666666666666668 after 220 iterations 
REGRET STEPS:  25
NEW LAMBDA 6.250000000000001
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    49.79           4.47           3.48           2.83           2.96           3.24           3.96           3.45           3.08           3.40      
    1    191.15          94.68          51.06          46.33          39.33          39.83          45.90          47.77          48.40          47.28      
    2    183.41          134.70          47.49          74.90          57.62          50.37          52.01          56.10          53.02          56.09      
    3    163.37          124.09          75.70          48.25          63.95          58.38          64.56          65.75          58.15          63.58      
    4    130.06          100.13          74.42          78.58          38.94          60.97          54.60          49.58          58.84          60.79      
    5    127.35          99.92          69.45          80.64          63.76          60.63          62.77          58.53          57.12          63.73      
    6    142.88          103.93          74.28          89.46          63.79          75.12          66.14          67.62          65.00          66.97      
    7    130.93          103.64          72.84          90.88          61.55          72.41          82.38          69.62          68.42          76.17      
    8    134.69          101.66          64.86          85.26          72.00          70.92          80.94          73.20          58.57          72.61      
    9    134.37          101.28          73.92          90.04          72.35          77.27          80.81          77.02          71.38          78.03      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    49.79          191.15          183.41          163.37          130.06          127.35          142.88          130.93          134.69          134.37      
    1     4.47          94.68          134.70          124.09          100.13          99.92          103.93          103.64          101.66          101.28      
    2     3.48          51.06          47.49          75.70          74.42          69.45          74.28          72.84          64.86          73.92      
    3     2.83          46.33          74.90          48.25          78.58          80.64          89.46          90.88          85.26          90.04      
    4     2.96          39.33          57.62          63.95          38.94          63.76          63.79          61.55          72.00          72.35      
    5     3.24          39.83          50.37          58.38          60.97          60.63          75.12          72.41          70.92          77.27      
    6     3.96          45.90          52.01          64.56          54.60          62.77          66.14          82.38          80.94          80.81      
    7     3.45          47.77          56.10          65.75          49.58          58.53          67.62          69.62          73.20          77.02      
    8     3.08          48.40          53.02          58.15          58.84          57.12          65.00          68.42          58.57          71.38      
    9     3.40          47.28          56.09          63.58          60.79          63.73          66.97          76.17          72.61          78.03      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    99.57          195.62          186.89          166.19          133.02          130.59          146.84          134.38          137.77          137.78      
    1    195.62          189.36          185.76          170.42          139.45          139.75          149.83          151.40          150.05          148.56      
    2    186.89          185.76          94.98          150.61          132.03          119.82          126.29          128.95          117.87          130.00      
    3    166.19          170.42          150.61          96.50          142.53          139.01          154.02          156.63          143.41          153.62      
    4    133.02          139.45          132.03          142.53          77.87          124.72          118.39          111.12          130.84          133.14      
    5    130.59          139.75          119.82          139.01          124.72          121.27          137.90          130.94          128.05          141.00      
    6    146.84          149.83          126.29          154.02          118.39          137.90          132.28          150.00          145.95          147.78      
    7    134.38          151.40          128.95          156.63          111.12          130.94          150.00          139.24          141.62          153.19      
    8    137.77          150.05          117.87          143.41          130.84          128.05          145.95          141.62          117.14          143.98      
    9    137.78          148.56          130.00          153.62          133.14          141.00          147.78          153.19          143.98          156.05      

 

Metagame probabilities: 
Player #0: 0.0001  0.0006  0.0058  0.0227  0.0077  0.0201  0.0801  0.2447  0.1466  0.4717  
Player #1: 0.0001  0.0006  0.0058  0.0227  0.0077  0.0201  0.0801  0.2447  0.1466  0.4717  
Iteration : 9
Time so far: 92005.4256196022
Approximating Best Response
Training best response:  True 0.05500000000000001
rtg values check:  (?,) (?, 1)
2023-07-21 16:02:14.850349: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_69/weights_2/Adam_1/Assign' id:19647 op device:{requested: '', assigned: ''} def:{{{node mlp_69/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_69/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_69/weights_2/Adam_1, mlp_69/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03067497629672289 59.00579986572266 0.6054286867380142 9.02676706314087 10486 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03459578640758991 17.818412685394286 0.7059598803520203 9.144167423248291 221118 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032648087665438655 18.816898918151857 0.6835455596446991 9.507111072540283 428712 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031348079815506934 19.2034273147583 0.7357990384101868 9.09947853088379 637696 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029649432003498077 13.688772678375244 0.7259849011898041 8.912448072433472 846713 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024884925037622452 13.859404563903809 0.6705646216869354 8.977275848388672 1057845 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025572008267045022 14.833088779449463 0.7522749602794647 8.222587633132935 1266968 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020200509019196032 38.926927947998045 0.6156372189521789 9.78459939956665 1477842 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022069568186998366 23.039820671081543 0.7400609016418457 8.999025630950928 1685054 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02191690132021904 12.33107852935791 0.8322414457798004 7.850538158416748 1893872 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019672395661473274 15.51205062866211 0.7787160634994507 8.419301462173461 2105655 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018531348928809165 17.675233840942383 0.837161123752594 8.161010503768921 2314634 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01539732338860631 16.766630363464355 0.7981752276420593 8.101981496810913 2522903 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01335914907976985 15.463737201690673 0.7614934206008911 8.337219858169556 2734198 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012096399813890458 14.011736488342285 0.8073769748210907 8.123442077636719 2945344 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009162025013938545 13.307419967651366 0.7374328553676606 8.28294324874878 3156110 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007155549619346857 17.884896278381348 0.8484628558158874 7.898001480102539 3366469 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0052324277348816395 14.995275592803955 0.8037508845329284 8.070659732818603 3577164 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003269209386780858 14.60409755706787 0.714504498243332 8.299602794647218 3787622 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007133686798624694 16.381731510162354 0.7555551946163177 8.283631515502929 3995558 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012232071656399057 18.517345237731934 0.6043854773044586 8.819219207763672 4203458 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021283224767103094 20.914478874206544 0.47808338701725006 8.934059333801269 4409902 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007347585022216662 14.634152889251709 0.6808363974094391 8.349165344238282 4622395 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006066367350285873 12.676942443847656 0.3690325736999512 8.808953094482423 4828704 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00036956699659640435 11.255597591400146 0.35844538509845736 8.679524993896484 5039048 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005312949460858363 17.475606441497803 0.4289608597755432 9.433667659759521 5249190 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007156755775213242 14.667091846466064 0.18872618973255156 9.047704124450684 5457189 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006237044231966138 13.012093925476075 0.4044960141181946 9.144347858428954 5668576 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004266991527401842 22.32495536804199 0.4646061211824417 9.431202602386474 5881235 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008773283982009162 15.408419704437256 0.3905727744102478 9.868156433105469 6091043 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004267769974831026 16.874999904632567 0.4192160278558731 10.033963394165038 6303287 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001650552213732226 21.294189834594725 0.28690389096736907 10.23701286315918 6510760 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011184642009538948 13.544206714630127 0.4103993684053421 9.22325029373169 6720547 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005680222500814125 15.42022247314453 0.2796831429004669 9.34457187652588 6930970 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00090479145583231 16.413663578033447 0.2877516269683838 9.595253276824952 7138349 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005598422263574321 20.213540840148926 0.370508474111557 9.550539684295654 7348539 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00024907943879952653 15.052407264709473 0.10185913220047951 9.992582321166992 7559138 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007217494057840668 17.185254859924317 0.10683062747120857 9.778642654418945 7769262 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00027908915435546076 20.30762138366699 0.23336633890867234 9.941788578033448 7979213 0
Recovering previous policy with expected return of 73.56218905472637. Long term value was 73.034 and short term was 68.63.


Pure best response payoff estimated to be 80.045 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (10, 0)
Current player 0 and current strategies (10, 0) took 62.69 seconds to finish estimate with resulting utilities: [131.87    3.225]
Estimating current strategies:  (10, 1)
Current player 0 and current strategies (10, 1) took 117.07 seconds to finish estimate with resulting utilities: [102.34   49.095]
Estimating current strategies:  (10, 2)
Current player 0 and current strategies (10, 2) took 134.44 seconds to finish estimate with resulting utilities: [76.13 55.19]
Estimating current strategies:  (10, 3)
Current player 0 and current strategies (10, 3) took 132.79 seconds to finish estimate with resulting utilities: [93.22 66.21]
Estimating current strategies:  (10, 4)
Current player 0 and current strategies (10, 4) took 116.08 seconds to finish estimate with resulting utilities: [69.87 57.73]
Estimating current strategies:  (10, 5)
Current player 0 and current strategies (10, 5) took 119.91 seconds to finish estimate with resulting utilities: [76.64 64.72]
Estimating current strategies:  (10, 6)
Current player 0 and current strategies (10, 6) took 115.95 seconds to finish estimate with resulting utilities: [78.765 65.77 ]
Estimating current strategies:  (10, 7)
Current player 0 and current strategies (10, 7) took 119.09 seconds to finish estimate with resulting utilities: [75.39 74.95]
Estimating current strategies:  (10, 8)
Current player 0 and current strategies (10, 8) took 116.99 seconds to finish estimate with resulting utilities: [70.995 73.355]
Estimating current strategies:  (10, 9)
Current player 0 and current strategies (10, 9) took 117.25 seconds to finish estimate with resulting utilities: [74.305 77.275]
Estimating current strategies:  (10, 10)
Current player 0 and current strategies (10, 10) took 116.34 seconds to finish estimate with resulting utilities: [73.495 74.43 ]
Computing meta_strategies
Exited RRD with total regret 6.237485817623536 that was less than regret lambda 6.250000000000001 after 215 iterations 
REGRET STEPS:  25
NEW LAMBDA 5.833333333333334
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0    49.79           4.47           3.48           2.83           2.96           3.24           3.96           3.45           3.08           3.40           3.23      
    1    191.15          94.68          51.06          46.33          39.33          39.83          45.90          47.77          48.40          47.28          49.09      
    2    183.41          134.70          47.49          74.90          57.62          50.37          52.01          56.10          53.02          56.09          55.19      
    3    163.37          124.09          75.70          48.25          63.95          58.38          64.56          65.75          58.15          63.58          66.21      
    4    130.06          100.13          74.42          78.58          38.94          60.97          54.60          49.58          58.84          60.79          57.73      
    5    127.35          99.92          69.45          80.64          63.76          60.63          62.77          58.53          57.12          63.73          64.72      
    6    142.88          103.93          74.28          89.46          63.79          75.12          66.14          67.62          65.00          66.97          65.77      
    7    130.93          103.64          72.84          90.88          61.55          72.41          82.38          69.62          68.42          76.17          74.95      
    8    134.69          101.66          64.86          85.26          72.00          70.92          80.94          73.20          58.57          72.61          73.36      
    9    134.37          101.28          73.92          90.04          72.35          77.27          80.81          77.02          71.38          78.03          77.28      
   10    131.87          102.34          76.13          93.22          69.87          76.64          78.77          75.39          71.00          74.31          73.96      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0    49.79          191.15          183.41          163.37          130.06          127.35          142.88          130.93          134.69          134.37          131.87      
    1     4.47          94.68          134.70          124.09          100.13          99.92          103.93          103.64          101.66          101.28          102.34      
    2     3.48          51.06          47.49          75.70          74.42          69.45          74.28          72.84          64.86          73.92          76.13      
    3     2.83          46.33          74.90          48.25          78.58          80.64          89.46          90.88          85.26          90.04          93.22      
    4     2.96          39.33          57.62          63.95          38.94          63.76          63.79          61.55          72.00          72.35          69.87      
    5     3.24          39.83          50.37          58.38          60.97          60.63          75.12          72.41          70.92          77.27          76.64      
    6     3.96          45.90          52.01          64.56          54.60          62.77          66.14          82.38          80.94          80.81          78.77      
    7     3.45          47.77          56.10          65.75          49.58          58.53          67.62          69.62          73.20          77.02          75.39      
    8     3.08          48.40          53.02          58.15          58.84          57.12          65.00          68.42          58.57          71.38          71.00      
    9     3.40          47.28          56.09          63.58          60.79          63.73          66.97          76.17          72.61          78.03          74.31      
   10     3.23          49.09          55.19          66.21          57.73          64.72          65.77          74.95          73.36          77.28          73.96      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0    99.57          195.62          186.89          166.19          133.02          130.59          146.84          134.38          137.77          137.78          135.09      
    1    195.62          189.36          185.76          170.42          139.45          139.75          149.83          151.40          150.05          148.56          151.44      
    2    186.89          185.76          94.98          150.61          132.03          119.82          126.29          128.95          117.87          130.00          131.32      
    3    166.19          170.42          150.61          96.50          142.53          139.01          154.02          156.63          143.41          153.62          159.43      
    4    133.02          139.45          132.03          142.53          77.87          124.72          118.39          111.12          130.84          133.14          127.60      
    5    130.59          139.75          119.82          139.01          124.72          121.27          137.90          130.94          128.05          141.00          141.36      
    6    146.84          149.83          126.29          154.02          118.39          137.90          132.28          150.00          145.95          147.78          144.53      
    7    134.38          151.40          128.95          156.63          111.12          130.94          150.00          139.24          141.62          153.19          150.34      
    8    137.77          150.05          117.87          143.41          130.84          128.05          145.95          141.62          117.14          143.98          144.35      
    9    137.78          148.56          130.00          153.62          133.14          141.00          147.78          153.19          143.98          156.05          151.58      
   10    135.09          151.44          131.32          159.43          127.60          141.36          144.53          150.34          144.35          151.58          147.93      

 

Metagame probabilities: 
Player #0: 0.0001  0.0006  0.0045  0.0201  0.0062  0.0175  0.0555  0.1886  0.1174  0.3504  0.2391  
Player #1: 0.0001  0.0006  0.0045  0.0201  0.0062  0.0175  0.0555  0.1886  0.1174  0.3504  0.2391  
Iteration : 10
Time so far: 102168.06183409691
Approximating Best Response
Training best response:  True 0.05
rtg values check:  (?,) (?, 1)
2023-07-21 18:51:37.876913: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_76/weights_2/Adam_1/Assign' id:21625 op device:{requested: '', assigned: ''} def:{{{node mlp_76/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_76/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_76/weights_2/Adam_1, mlp_76/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020784376258961856 67.77180938720703 0.3802644141018391 10.320339679718018 10201 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.035705081187188625 15.792278003692626 0.7522275030612946 8.58178243637085 220149 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03367206659168005 15.645686817169189 0.7434604406356812 8.627553415298461 431401 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030273594707250596 13.816117763519287 0.7491032958030701 8.678192806243896 641598 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03047458752989769 38.69186973571777 0.6662095785140991 9.682750797271728 847946 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026798144914209843 16.831964778900147 0.7079016923904419 8.915349674224853 1056470 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028277159482240678 16.58304328918457 0.8097445607185364 8.261218690872193 1264313 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02243433091789484 29.007448387145995 0.7046932578086853 8.970215320587158 1476078 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022256277687847614 14.466159439086914 0.774250966310501 8.279373168945312 1687312 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020375492610037328 21.629474258422853 0.7444825589656829 8.670754146575927 1896783 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018887600488960744 16.631758785247804 0.7565488934516906 8.542095613479614 2105108 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016200060211122035 33.935819435119626 0.7013201773166656 8.944491481781006 2316140 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014895887579768896 13.573346996307373 0.7333635985851288 8.405921125411988 2525951 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012865151278674603 16.32096138000488 0.7445790350437165 8.465588188171386 2736065 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01029503894969821 13.033527660369874 0.798346471786499 8.028238201141358 2946487 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008190115028992296 17.35228853225708 0.7502924799919128 8.289512491226196 3156977 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006817127903923392 18.502971267700197 0.8213976323604584 8.022690391540527 3365509 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004230324411764741 17.033171939849854 0.7039240598678589 8.449358558654785 3573682 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0028282110462896524 14.733298587799073 0.7548805117607117 8.415396642684936 3782819 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000472481845645234 15.074207019805907 0.6208727598190308 9.307343482971191 3992458 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005344302902813069 18.324172592163087 0.6082146465778351 8.928698348999024 4202564 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009829657072259578 23.30530548095703 0.6053594142198563 9.199104022979736 4410454 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -2.3958345991559327e-05 23.594208335876466 0.6091046810150147 9.4764479637146 4618588 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001630000042496249 15.478414630889892 0.4726705729961395 8.815832996368409 4826758 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005175576778128743 15.6314453125 0.4568712621927261 9.668741607666016 5036637 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003285588845028542 21.711661529541015 0.30704074800014497 9.44165334701538 5245339 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00031322572394856254 21.87197914123535 0.45505473017692566 9.287382698059082 5454158 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00045462560083251444 14.939066505432129 0.46553308665752413 9.141398048400879 5663797 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0002983559621497989 12.638157939910888 0.4463899463415146 9.204717445373536 5871342 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00046557802124880254 17.50847625732422 0.3076987981796265 9.847922039031982 6080639 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006593444159079809 19.196287155151367 0.3223136067390442 9.874314785003662 6291104 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007048815950838616 12.517411231994629 0.20219179540872573 9.55258903503418 6501924 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013491527450241847 14.56154375076294 0.3079580307006836 9.394521141052246 6712625 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000975362853569095 14.540424156188966 0.3426231056451797 9.372135162353516 6925074 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0036292099684942514 14.120736122131348 0.4142313927412033 9.20790729522705 7135574 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00011245329387747916 16.760277271270752 0.3479414314031601 9.979468631744385 7346004 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  5.694371357094497e-05 17.381291484832765 0.3856661647558212 9.777893447875977 7556696 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00015472489194507945 14.406054019927979 0.3015124946832657 9.557164859771728 7768389 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004235356958815828 17.889288902282715 0.1233472153544426 10.833820056915282 7977048 0


Pure best response payoff estimated to be 80.16 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (11, 0)
Current player 0 and current strategies (11, 0) took 60.01 seconds to finish estimate with resulting utilities: [117.99    2.555]
Estimating current strategies:  (11, 1)
Current player 0 and current strategies (11, 1) took 121.05 seconds to finish estimate with resulting utilities: [97.51 43.19]
Estimating current strategies:  (11, 2)
Current player 0 and current strategies (11, 2) took 134.02 seconds to finish estimate with resulting utilities: [65.28  50.495]
Estimating current strategies:  (11, 3)
Current player 0 and current strategies (11, 3) took 123.32 seconds to finish estimate with resulting utilities: [75.905 53.12 ]
Estimating current strategies:  (11, 4)
Current player 0 and current strategies (11, 4) took 111.04 seconds to finish estimate with resulting utilities: [62.38 47.44]
Estimating current strategies:  (11, 5)
Current player 0 and current strategies (11, 5) took 121.22 seconds to finish estimate with resulting utilities: [66.49 55.07]
Estimating current strategies:  (11, 6)
Current player 0 and current strategies (11, 6) took 122.4 seconds to finish estimate with resulting utilities: [74.24  59.205]
Estimating current strategies:  (11, 7)
Current player 0 and current strategies (11, 7) took 118.23 seconds to finish estimate with resulting utilities: [71.555 63.295]
Estimating current strategies:  (11, 8)
Current player 0 and current strategies (11, 8) took 116.03 seconds to finish estimate with resulting utilities: [66.405 63.265]
Estimating current strategies:  (11, 9)
Current player 0 and current strategies (11, 9) took 112.44 seconds to finish estimate with resulting utilities: [71.935 63.985]
Estimating current strategies:  (11, 10)
Current player 0 and current strategies (11, 10) took 118.13 seconds to finish estimate with resulting utilities: [75.975 68.13 ]
Estimating current strategies:  (11, 11)
Current player 0 and current strategies (11, 11) took 101.61 seconds to finish estimate with resulting utilities: [59.115 54.85 ]
Computing meta_strategies
Exited RRD with total regret 5.812306777525592 that was less than regret lambda 5.833333333333334 after 233 iterations 
REGRET STEPS:  25
NEW LAMBDA 5.416666666666667
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11      
    0    49.79           4.47           3.48           2.83           2.96           3.24           3.96           3.45           3.08           3.40           3.23           2.56      
    1    191.15          94.68          51.06          46.33          39.33          39.83          45.90          47.77          48.40          47.28          49.09          43.19      
    2    183.41          134.70          47.49          74.90          57.62          50.37          52.01          56.10          53.02          56.09          55.19          50.49      
    3    163.37          124.09          75.70          48.25          63.95          58.38          64.56          65.75          58.15          63.58          66.21          53.12      
    4    130.06          100.13          74.42          78.58          38.94          60.97          54.60          49.58          58.84          60.79          57.73          47.44      
    5    127.35          99.92          69.45          80.64          63.76          60.63          62.77          58.53          57.12          63.73          64.72          55.07      
    6    142.88          103.93          74.28          89.46          63.79          75.12          66.14          67.62          65.00          66.97          65.77          59.20      
    7    130.93          103.64          72.84          90.88          61.55          72.41          82.38          69.62          68.42          76.17          74.95          63.30      
    8    134.69          101.66          64.86          85.26          72.00          70.92          80.94          73.20          58.57          72.61          73.36          63.27      
    9    134.37          101.28          73.92          90.04          72.35          77.27          80.81          77.02          71.38          78.03          77.28          63.98      
   10    131.87          102.34          76.13          93.22          69.87          76.64          78.77          75.39          71.00          74.31          73.96          68.13      
   11    117.99          97.51          65.28          75.91          62.38          66.49          74.24          71.56          66.41          71.94          75.97          56.98      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11      
    0    49.79          191.15          183.41          163.37          130.06          127.35          142.88          130.93          134.69          134.37          131.87          117.99      
    1     4.47          94.68          134.70          124.09          100.13          99.92          103.93          103.64          101.66          101.28          102.34          97.51      
    2     3.48          51.06          47.49          75.70          74.42          69.45          74.28          72.84          64.86          73.92          76.13          65.28      
    3     2.83          46.33          74.90          48.25          78.58          80.64          89.46          90.88          85.26          90.04          93.22          75.91      
    4     2.96          39.33          57.62          63.95          38.94          63.76          63.79          61.55          72.00          72.35          69.87          62.38      
    5     3.24          39.83          50.37          58.38          60.97          60.63          75.12          72.41          70.92          77.27          76.64          66.49      
    6     3.96          45.90          52.01          64.56          54.60          62.77          66.14          82.38          80.94          80.81          78.77          74.24      
    7     3.45          47.77          56.10          65.75          49.58          58.53          67.62          69.62          73.20          77.02          75.39          71.56      
    8     3.08          48.40          53.02          58.15          58.84          57.12          65.00          68.42          58.57          71.38          71.00          66.41      
    9     3.40          47.28          56.09          63.58          60.79          63.73          66.97          76.17          72.61          78.03          74.31          71.94      
   10     3.23          49.09          55.19          66.21          57.73          64.72          65.77          74.95          73.36          77.28          73.96          75.97      
   11     2.56          43.19          50.49          53.12          47.44          55.07          59.20          63.30          63.27          63.98          68.13          56.98      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10             11      
    0    99.57          195.62          186.89          166.19          133.02          130.59          146.84          134.38          137.77          137.78          135.09          120.55      
    1    195.62          189.36          185.76          170.42          139.45          139.75          149.83          151.40          150.05          148.56          151.44          140.70      
    2    186.89          185.76          94.98          150.61          132.03          119.82          126.29          128.95          117.87          130.00          131.32          115.78      
    3    166.19          170.42          150.61          96.50          142.53          139.01          154.02          156.63          143.41          153.62          159.43          129.03      
    4    133.02          139.45          132.03          142.53          77.87          124.72          118.39          111.12          130.84          133.14          127.60          109.82      
    5    130.59          139.75          119.82          139.01          124.72          121.27          137.90          130.94          128.05          141.00          141.36          121.56      
    6    146.84          149.83          126.29          154.02          118.39          137.90          132.28          150.00          145.95          147.78          144.53          133.44      
    7    134.38          151.40          128.95          156.63          111.12          130.94          150.00          139.24          141.62          153.19          150.34          134.85      
    8    137.77          150.05          117.87          143.41          130.84          128.05          145.95          141.62          117.14          143.98          144.35          129.67      
    9    137.78          148.56          130.00          153.62          133.14          141.00          147.78          153.19          143.98          156.05          151.58          135.92      
   10    135.09          151.44          131.32          159.43          127.60          141.36          144.53          150.34          144.35          151.58          147.93          144.10      
   11    120.55          140.70          115.78          129.03          109.82          121.56          133.44          134.85          129.67          135.92          144.10          113.97      

 

Metagame probabilities: 
Player #0: 0.0001  0.0004  0.0032  0.0154  0.0042  0.0137  0.0462  0.1717  0.1072  0.3211  0.238  0.079  
Player #1: 0.0001  0.0004  0.0032  0.0154  0.0042  0.0137  0.0462  0.1717  0.1072  0.3211  0.238  0.079  
Iteration : 11
Time so far: 112496.37470650673
Approximating Best Response
Training best response:  True 0.045
rtg values check:  (?,) (?, 1)
2023-07-21 21:43:46.286594: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_83/weights_2/Adam_1/Assign' id:23603 op device:{requested: '', assigned: ''} def:{{{node mlp_83/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_83/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_83/weights_2/Adam_1, mlp_83/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025487760454416274 61.65079536437988 0.49688189327716825 11.077869510650634 10640 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0359172660857439 12.80119514465332 0.753767591714859 10.170828533172607 223210 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029103235714137553 19.520728492736815 0.6329963028430938 10.715041732788086 434352 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028148380853235722 19.75187129974365 0.6651698052883148 10.314430713653564 641898 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03250097874552012 14.897738647460937 0.7841874301433563 9.763166236877442 850436 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027224805019795895 15.143005561828613 0.7081572651863098 10.050546550750733 1060940 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02439721394330263 18.700214004516603 0.6907866597175598 10.095103645324707 1270885 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023258426040410996 20.19450454711914 0.7134012460708619 10.020338249206542 1478821 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01977189164608717 16.370761489868165 0.653902119398117 9.99972906112671 1687818 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01826513819396496 23.100306701660156 0.6777352154254913 10.139087581634522 1896310 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01916657146066427 19.98529396057129 0.7540198504924774 9.64630947113037 2104798 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01615410940721631 18.331992626190186 0.723737770318985 9.663159942626953 2315044 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013809516094624996 16.162106418609618 0.7225946247577667 9.4403715133667 2524386 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01346914479508996 15.19312334060669 0.7231450319290161 9.966421031951905 2734830 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010946827288717032 15.46701364517212 0.7533597230911255 9.676746463775634 2945899 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00874749426729977 15.788200664520264 0.7096266746520996 9.832959270477295 3152558 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007063193013891578 14.24279499053955 0.8071855902671814 9.192563247680663 3362379 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0047710049664601685 22.02116928100586 0.7586702823638916 9.894905471801758 3570606 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002637888805475086 13.130040645599365 0.7382440567016602 9.589733600616455 3778721 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001127142148470739 16.59867181777954 0.7434133768081665 9.651844215393066 3987824 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003845343162538484 16.875826835632324 0.6063612222671508 9.971563529968261 4196774 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00174899622797966 16.380311679840087 0.4588769465684891 9.90601749420166 4405792 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006837931707423195 15.449477291107177 0.5455778896808624 9.838456535339356 4615738 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00136680507712299 23.069014167785646 0.49032303094863894 10.394250965118408 4825700 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00012063161702826619 15.419006538391113 0.2127862423658371 10.551018238067627 5035886 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00014491520560113713 14.001942157745361 0.08973336666822433 10.862986087799072 5245498 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002441958995518689 17.358305263519288 0.0648991908878088 11.026461696624756 5455502 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010427815985167399 30.878251838684083 0.07869057357311249 11.32987289428711 5665369 0
Fatal Python error: Segmentation fault

Current thread 0x000014947c8f3b80 (most recent call first):
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/rl_environment.py", line 330 in step
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/rl_oracle_cooperative.py", line 481 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/rl_oracle.py", line 223 in _rollout
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/rl_oracle_cooperative.py", line 190 in __call__
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/psro_v2.py", line 413 in update_agents
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 201 in iteration
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 403 in gpsro_looper
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 482 in main
  File "/home/anrigu/.local/lib/python3.10/site-packages/absl/app.py", line 254 in _run_main
  File "/home/anrigu/.local/lib/python3.10/site-packages/absl/app.py", line 308 in run
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 485 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, google._upb._message, tensorflow.python.framework.fast_tensor_util, charset_normalizer.md, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, PIL._imaging, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label, cvxopt.base, cvxopt.blas, cvxopt.lapack (total: 117)
/var/spool/slurmd.spool/job56105536/slurm_script: line 34: 2886355 Segmentation fault      (core dumped) python3 -u psro_v2_simultaneous.py --game_name=harvest --save_folder_path="../examples/data/harvest/vanilla_ppo_third/($trial)_$parameter_id" --meta_strategy_method=prd_collab --gpsro_iterations=30 --consensus_imitation=False --regret_lambda_init=$curr_lambda --regret_steps=$curr_steps --policy_constraint_steps=20 --joint_action=False --rewards_joint=False --fine_tune=True --clear_trajectories=False --consensus_hidden_layer_size=50 --consensus_n_hidden_layers=2 --min_buffer_size_fine_tune=10000 --epochs_ppo=10 --minibatches_ppo=50 --number_training_steps=8000000 --symmetric_game=True --sims_per_entry=200 --eps_clip=.1 --eps_clip_value=.5 --entropy_decay_duration=.5 --ppo_entropy=.05 --fine_tune_policy_lr=.00003 --fine_tune_value_lr=.0003 --transfer_policy_minimum_entropy=0 --recovery_window=200 --hidden_layer_size=50 --n_hidden_layers=2 --batch_size=64 --dqn_learning_rate=3e-4 --update_target_network_every=500 --learn_every=1 --max_buffer_size=100000 --epsilon_decay_duration=100000 --min_buffer_size_to_learn=20000 --regret_calculation_steps=1500000
