Job Id listed below:
55931189

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

2023-07-16 19:45:42.459963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-16 19:45:43.408566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0716 19:45:44.944533 22564343139200 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x148562682d40>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x148562682d40>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-07-16 19:45:45.241358: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-07-16 19:45:45.508093: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.97 seconds to finish estimate with resulting utilities: [49.1  49.29]
Exited RRD with total regret 0.0 that was less than regret lambda 5.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    49.20      

 

Player 1 Payoff matrix: 

           0      
    0    49.20      

 

Social Welfare Sum Matrix: 

           0      
    0    98.39      

 

Iteration : 0
Time so far: 0.00018525123596191406
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-07-16 19:46:05.400555: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.11835819259285926 29.84329471588135 2.045169544219971 0.0007208694092696533 10495 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09829152449965477 14.727508926391602 1.874443519115448 0.21002095192670822 216013 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09563563689589501 13.570951557159423 1.8671366453170777 0.282430100440979 418034 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08732339963316918 14.246808433532715 1.8404020190238952 0.36328206956386566 619208 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08495502471923828 17.03046598434448 1.8132863998413087 0.43700083196163175 821497 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07520176768302918 12.683909130096435 1.7782894372940063 0.5371490061283112 1023012 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06809745356440544 16.868883037567137 1.7436988949775696 0.6056786894798278 1224446 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06062019281089306 18.65489559173584 1.6677297711372376 0.7561391770839692 1426892 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.051062051206827164 18.86944799423218 1.6379019379615785 0.8517224073410035 1629395 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0492649968713522 20.57308464050293 1.631157100200653 0.9900839686393738 1835482 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03900448866188526 19.812776565551758 1.5270074248313903 1.1631862998008728 2042261 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03633476756513119 19.089024353027344 1.4946035861968994 1.2601822018623352 2249404 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031170346215367316 17.822844886779784 1.4413818717002869 1.4065842390060426 2457694 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025333876349031927 23.339150619506835 1.4005420684814454 1.5528525471687318 2665138 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02092144303023815 25.454773330688475 1.3518404603004455 1.7490283131599427 2873658 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01820804560557008 25.809986305236816 1.2786385059356689 1.9824342608451844 3085073 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011988470051437616 22.407372856140135 1.219433343410492 2.0781954646110536 3293766 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009293217258527876 22.739814376831056 1.1866225123405456 2.240293526649475 3504061 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005954871769063175 21.04635696411133 1.0991610288619995 2.6372990131378176 3718464 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017755931796273217 23.58875904083252 1.04767245054245 2.7833319664001466 3931612 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009488269650319125 25.05322322845459 0.9483938097953797 2.9833491563797 4142683 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015674057241994888 27.645151901245118 0.8741189360618591 3.397067975997925 4355577 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018875093781389296 23.9293550491333 0.777858567237854 3.929148459434509 4570179 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002829832583665848 26.284857940673827 0.6948759257793427 4.171826100349426 4784326 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012256114277988672 20.879752159118652 0.6718334436416626 4.360706567764282 4998257 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010111748822964727 24.24535675048828 0.6018753409385681 4.861345434188843 5215455 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002753741078777239 28.37041244506836 0.5827105879783631 5.194183731079102 5433717 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018959671368065755 23.816978454589844 0.5540370166301727 5.465718793869018 5651811 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010756950447103008 21.5745698928833 0.5562587797641754 5.687834358215332 5867679 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002668873406946659 22.39067134857178 0.5263020008802414 5.7216721534729 6085842 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002218809834448621 28.78005886077881 0.4693593502044678 6.123159313201905 6304250 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015652048139600083 23.51007194519043 0.4688346028327942 6.10955662727356 6523771 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002410041884286329 23.20471248626709 0.44056383669376376 6.316883373260498 6741229 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015635942536846414 25.217570495605468 0.4170794188976288 6.473914623260498 6958222 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00024136655410984532 25.20352210998535 0.38233667612075806 6.811922788619995 7176222 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002543019552831538 25.654405784606933 0.39353897273540495 6.873409175872803 7393419 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009839965030550956 26.0314661026001 0.40868988037109377 6.982199478149414 7610907 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0008362369117094204 23.331932830810548 0.3695452779531479 7.134856653213501 7828918 0


Pure best response payoff estimated to be 195.18 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 81.65 seconds to finish estimate with resulting utilities: [192.18   4.01]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 134.53 seconds to finish estimate with resulting utilities: [90.275 95.305]
Computing meta_strategies
Exited RRD with total regret 4.966696283048066 that was less than regret lambda 5.0 after 34 iterations 
REGRET STEPS:  25
NEW LAMBDA 4.791666666666667
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    49.20           4.01      
    1    192.18          92.79      

 

Player 1 Payoff matrix: 

           0              1      
    0    49.20          192.18      
    1     4.01          92.79      

 

Social Welfare Sum Matrix: 

           0              1      
    0    98.39          196.19      
    1    196.19          185.58      

 

Metagame probabilities: 
Player #0: 0.0275  0.9725  
Player #1: 0.0275  0.9725  
Iteration : 1
Time so far: 6394.153852462769
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-07-16 21:32:39.666506: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018375779315829278 71.74368820190429 0.3502775639295578 8.335529994964599 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.037906257808208464 13.980339527130127 0.783091014623642 6.265381956100464 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03517300933599472 16.036685752868653 0.7602465093135834 6.297782611846924 450396 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03363015204668045 16.95442399978638 0.7949029564857483 5.868715906143189 669711 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.041952185332775116 15.134348297119141 1.0138791084289551 5.119626522064209 888512 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03335932940244675 19.60942897796631 0.8817848265171051 5.255342960357666 1107763 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03299049139022827 15.65339412689209 0.9506084680557251 5.066504669189453 1326861 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030436408706009388 20.679219436645507 0.9539812266826629 5.177886009216309 1544963 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030358271673321724 16.412321376800538 0.9928449511528015 4.753262519836426 1761788 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0257465111091733 13.777037239074707 0.9633689105510712 4.993943214416504 1978763 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02234758697450161 18.541817283630373 0.939405369758606 5.124153709411621 2196855 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01938230786472559 17.32284231185913 0.9266431927680969 5.0611162185668945 2415723 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016285486333072186 19.610984992980956 0.901147472858429 5.119205951690674 2632820 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013633320201188326 16.304968452453615 0.8953338861465454 5.356755161285401 2849666 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012860809918493032 18.48402671813965 0.9402096450328827 5.072546863555909 3066746 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00949376616626978 16.23221311569214 0.9220011174678803 5.315141868591309 3282377 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004967941297218203 16.248516082763672 0.8287424087524414 5.721958160400391 3500294 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004441636265255511 19.940269660949706 0.8927351474761963 5.308926105499268 3712175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016899758775252849 18.890251064300536 0.7951902866363525 5.293325281143188 3927696 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013441208051517607 18.648665046691896 0.7124569535255432 5.775233316421509 4145164 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007363685705058743 21.72684516906738 0.6658632278442382 5.787158060073852 4363884 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00023354118020506575 22.16352310180664 0.580841463804245 6.33326473236084 4581895 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005755752557888627 22.131840705871582 0.46405264735221863 7.169572877883911 4801618 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0024637076538056137 20.42592658996582 0.48673045039176943 6.425974798202515 5020091 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001845119886274915 26.090899085998537 0.4117458134889603 7.159318971633911 5239387 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008015122846700251 22.09297103881836 0.3372157573699951 7.8123644351959225 5455935 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006745576920366148 23.796953392028808 0.27910923808813093 7.660185766220093 5674070 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011164119307068177 18.66222229003906 0.2627422362565994 7.624842548370362 5892870 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017996107519138605 18.5619873046875 0.344822558760643 7.404701614379883 6111712 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012073051737388596 18.170647621154785 0.30660745203495027 7.5095068454742435 6325481 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009558492165524513 14.995160961151123 0.451251482963562 7.567525434494018 6542660 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001006732522364473 23.877804374694826 0.3278143316507339 8.258229637145996 6758651 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005635066627291963 23.501021575927734 0.1647277683019638 8.58728494644165 6975940 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000138738745226874 23.55347137451172 0.1749746397137642 8.492716884613037 7192814 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004480392555706203 26.37867088317871 0.20601760298013688 8.834023475646973 7407581 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012226167134940624 24.06405372619629 0.2085457131266594 9.007167720794678 7619195 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00032841787760844453 21.10274257659912 0.18957214653491974 9.050879287719727 7835821 0


Pure best response payoff estimated to be 126.83 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 82.96 seconds to finish estimate with resulting utilities: [178.115   2.785]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 135.58 seconds to finish estimate with resulting utilities: [125.68   53.645]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 136.29 seconds to finish estimate with resulting utilities: [35.51 35.77]
Computing meta_strategies
Exited RRD with total regret 4.691111560518664 that was less than regret lambda 4.791666666666667 after 54 iterations 
REGRET STEPS:  25
NEW LAMBDA 4.583333333333334
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    49.20           4.01           2.79      
    1    192.18          92.79          53.65      
    2    178.12          125.68          35.64      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    49.20          192.18          178.12      
    1     4.01          92.79          125.68      
    2     2.79          53.65          35.64      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    98.39          196.19          180.90      
    1    196.19          185.58          179.33      
    2    180.90          179.33          71.28      

 

Metagame probabilities: 
Player #0: 0.0061  0.4386  0.5553  
Player #1: 0.0061  0.4386  0.5553  
Iteration : 2
Time so far: 15043.80603981018
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-07-16 23:56:49.423530: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015341640450060368 76.49608535766602 0.2980923533439636 10.385295200347901 10333 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022585817240178586 16.4588604927063 0.45706227123737336 9.21583890914917 224954 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029735344648361205 19.36371955871582 0.6376467108726501 8.108202409744262 442813 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024656938388943672 19.279296493530275 0.5754505664110183 7.7874072074890135 659388 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026945763640105723 15.594295120239257 0.6685414254665375 7.617786645889282 878472 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031791409477591515 12.574733257293701 0.8711583852767945 7.003877830505371 1091865 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02638822514563799 20.938015747070313 0.7543934464454651 7.425078248977661 1304870 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019907598756253718 25.449207878112794 0.6150005161762238 7.594053173065186 1517786 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022184888646006585 21.753135681152344 0.7820476651191711 6.8412128448486325 1732804 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021681343950331212 12.330562591552734 0.8235681831836701 6.905436944961548 1947224 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02205000389367342 17.90426559448242 0.8756312668323517 6.59478645324707 2159104 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01874720919877291 15.696003532409668 0.8686040103435516 6.318047046661377 2371242 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016693940199911595 12.388071823120118 0.8660654664039612 6.430481815338135 2586106 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013121697306632995 13.837837409973144 0.7818837106227875 6.185086965560913 2798641 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009753192868083715 20.287043380737305 0.7285791754722595 6.987342357635498 3016344 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009514959622174501 14.123730754852295 0.8169174313545227 6.4089111328125 3231258 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004911169549450279 17.36901111602783 0.6533395886421204 7.1844642639160154 3447820 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0037865740829147398 18.90489912033081 0.6565241277217865 6.971827411651612 3664569 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0025137252116110174 24.596027183532716 0.5565983027219772 7.44496169090271 3879013 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006066725734854117 16.014654636383057 0.6665948927402496 7.215021228790283 4096495 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008097293262835592 18.483532905578613 0.5750414550304412 7.484344530105591 4313576 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000761294444964733 38.74530220031738 0.3214237868785858 8.217968654632568 4531724 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012775032970239407 23.484901237487794 0.22008862048387529 7.922088479995727 4749358 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00018137475854018703 19.166073989868163 0.2805532798171043 8.192409992218018 4966355 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014530543405271601 18.584210681915284 0.20639356672763826 8.197319269180298 5182678 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007399879526928998 17.30590190887451 0.1273549899458885 8.76194257736206 5400593 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001901675764202082 17.261230659484863 0.1638545110821724 8.186958837509156 5617747 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013207276595494477 21.0754301071167 0.14788282364606858 8.865065383911134 5835827 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008449900764389895 20.97002582550049 0.22596917897462845 8.619309997558593 6053065 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012501517327109467 18.37339382171631 0.3321792006492615 8.535237121582032 6271975 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00025848015793599187 20.673033142089842 0.35362316370010377 8.62549467086792 6491627 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000620073288155254 17.551888179779052 0.23697174489498138 8.755213451385497 6711132 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006742717581801116 34.24445018768311 0.19874443113803864 9.280228424072266 6929704 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0001692334615654545 26.317380714416505 0.14846919924020768 9.308927059173584 7148511 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005462312066811137 19.270941734313965 0.20758395940065383 9.133963775634765 7368494 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006539285473991186 19.25062084197998 0.17486625611782075 9.190327072143555 7588494 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014769551402423532 17.726932430267333 0.14988188967108726 9.365947151184082 7808321 0


Pure best response payoff estimated to be 97.26 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 84.28 seconds to finish estimate with resulting utilities: [170.7   2.5]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 137.61 seconds to finish estimate with resulting utilities: [117.55   55.145]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 137.39 seconds to finish estimate with resulting utilities: [74.86  90.245]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 137.17 seconds to finish estimate with resulting utilities: [74.445 69.24 ]
Computing meta_strategies
Exited RRD with total regret 4.549370109172671 that was less than regret lambda 4.583333333333334 after 119 iterations 
REGRET STEPS:  25
NEW LAMBDA 4.375000000000001
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0    49.20           4.01           2.79           2.50      
    1    192.18          92.79          53.65          55.15      
    2    178.12          125.68          35.64          90.25      
    3    170.70          117.55          74.86          71.84      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0    49.20          192.18          178.12          170.70      
    1     4.01          92.79          125.68          117.55      
    2     2.79          53.65          35.64          74.86      
    3     2.50          55.15          90.25          71.84      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0    98.39          196.19          180.90          173.20      
    1    196.19          185.58          179.33          172.69      
    2    180.90          179.33          71.28          165.11      
    3    173.20          172.69          165.11          143.69      

 

Metagame probabilities: 
Player #0: 0.0001  0.062  0.3612  0.5767  
Player #1: 0.0001  0.062  0.3612  0.5767  
Iteration : 3
Time so far: 23961.273766994476
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-07-17 02:25:27.025454: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7779 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02433969471603632 54.04458999633789 0.4736402630805969 9.51203966140747 10427 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03672505877912045 14.758665466308594 0.7542090475559234 8.22863187789917 230427 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030804674513638018 19.3234393119812 0.6702342391014099 8.089456129074097 448514 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02458473723381758 23.981024169921874 0.5801992654800415 8.322288513183594 667049 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026542661339044572 16.008725070953368 0.6951589584350586 7.839278793334961 883694 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027054025791585445 13.69289789199829 0.7566245913505554 7.571509790420532 1102655 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026440974697470664 13.334968280792236 0.752396535873413 7.548223161697388 1320452 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02703481465578079 13.177029991149903 0.8108669877052307 7.4483623027801515 1539558 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022945595905184746 11.174973487854004 0.828788036108017 7.4473114013671875 1757431 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023190833255648614 13.464130878448486 0.8774873912334442 7.339865827560425 1976088 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01760263917967677 20.111538124084472 0.729105430841446 7.655666732788086 2194667 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016263436619192363 17.2368896484375 0.7960883200168609 7.212226152420044 2410934 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01380472406744957 17.877759838104247 0.7649649500846862 7.536461305618286 2627582 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010722478665411473 18.5679874420166 0.6737238645553589 7.751020765304565 2844631 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010441306140273809 17.715085220336913 0.6950550198554992 7.655264234542846 3063530 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00720466529019177 16.63813066482544 0.7509519040584565 7.51431360244751 3282923 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0043947386089712385 18.485986709594727 0.6676923334598541 7.841486501693725 3501089 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004063202068209648 20.139471817016602 0.6969126403331757 8.023252487182617 3719669 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001391148430411704 19.450301551818846 0.6618961274623871 8.394979476928711 3939665 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002611669056022947 23.28537712097168 0.5702701687812806 8.3219633102417 4158222 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010224695899523795 27.808325386047365 0.42297051250934603 8.919928741455077 4377492 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  9.104175551328808e-05 18.411300659179688 0.5272062212228775 8.561289882659912 4595597 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013832724493113347 18.453746795654297 0.4248450696468353 8.785959053039551 4814241 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002094931621104479 13.183740234375 0.4496748328208923 9.33019199371338 5033662 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006514302083814982 17.997573852539062 0.38550310730934145 9.401821899414063 5253662 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009715707783470862 23.20140552520752 0.35061562061309814 10.028842544555664 5472502 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00037579399468086195 23.10759105682373 0.32592487037181855 9.71477460861206 5691074 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009567516462993808 19.548808097839355 0.34258908927440646 9.99629602432251 5910270 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00022016258517396637 20.266053485870362 0.2776727795600891 9.932898235321044 6129753 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -6.967106455704197e-05 14.663169765472412 0.18902118504047394 10.277494430541992 6347948 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007541807019151748 28.337083625793458 0.31205044984817504 10.311023712158203 6566754 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005719899607356637 17.308192825317384 0.13550435602664948 10.424838638305664 6785500 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008157689517247491 17.577168464660645 0.09506867825984955 11.01347246170044 7004436 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00019631919221865246 20.56557216644287 0.10835901275277138 10.900998783111572 7222238 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005966539873043075 23.66295795440674 0.09813674241304397 11.350069141387939 7441887 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010214430396445096 14.079751873016358 0.09738243073225021 11.430183506011963 7661576 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006322151862150349 20.55202465057373 0.08657907322049141 11.712759113311767 7881400 0


Pure best response payoff estimated to be 89.43 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 81.78 seconds to finish estimate with resulting utilities: [149.375   2.99 ]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 136.12 seconds to finish estimate with resulting utilities: [113.685  44.685]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 136.85 seconds to finish estimate with resulting utilities: [60.925 77.4  ]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 138.05 seconds to finish estimate with resulting utilities: [99.575 66.27 ]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 137.31 seconds to finish estimate with resulting utilities: [72.825 72.395]
Computing meta_strategies
Exited RRD with total regret 4.355709372084419 that was less than regret lambda 4.375000000000001 after 198 iterations 
REGRET STEPS:  25
NEW LAMBDA 4.166666666666668
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0    49.20           4.01           2.79           2.50           2.99      
    1    192.18          92.79          53.65          55.15          44.69      
    2    178.12          125.68          35.64          90.25          77.40      
    3    170.70          117.55          74.86          71.84          66.27      
    4    149.38          113.69          60.92          99.58          72.61      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0    49.20          192.18          178.12          170.70          149.38      
    1     4.01          92.79          125.68          117.55          113.69      
    2     2.79          53.65          35.64          74.86          60.92      
    3     2.50          55.15          90.25          71.84          99.58      
    4     2.99          44.69          77.40          66.27          72.61      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0    98.39          196.19          180.90          173.20          152.37      
    1    196.19          185.58          179.33          172.69          158.37      
    2    180.90          179.33          71.28          165.11          138.32      
    3    173.20          172.69          165.11          143.69          165.84      
    4    152.37          158.37          138.32          165.84          145.22      

 

Metagame probabilities: 
Player #0: 0.0001  0.0035  0.2071  0.1943  0.595  
Player #1: 0.0001  0.0035  0.2071  0.1943  0.595  
Iteration : 4
Time so far: 33040.44374227524
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-07-17 04:56:46.324994: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9757 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01671755372080952 50.87698669433594 0.3302243292331696 11.267154312133789 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01826737876981497 13.958335685729981 0.3615249842405319 10.748065376281739 226023 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023856174573302268 16.469733428955077 0.5150286585092545 10.114355850219727 443176 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022173001989722253 13.267909240722656 0.5017913728952408 9.828110599517823 655765 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018572993483394383 24.431008529663085 0.4672284036874771 10.3040940284729 863145 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022417016699910162 11.48615894317627 0.589531809091568 9.325300979614259 1075512 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015184340998530388 20.91202583312988 0.41680424809455874 10.488720893859863 1287868 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02162672411650419 17.985200119018554 0.6534279406070709 9.199274921417237 1502721 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018793746922165157 15.063188076019287 0.6419556140899658 9.302172565460205 1713694 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015366716962307691 16.392245292663574 0.5881889581680297 9.431029796600342 1923710 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012087407056242228 29.771584701538085 0.44382968842983245 10.687491703033448 2132426 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011586311925202607 10.757117652893067 0.5485014468431473 9.777525997161865 2339359 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009583383798599243 19.83546028137207 0.5074461549520493 10.198925685882568 2546884 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009498592466115952 14.641276168823243 0.5407843381166458 10.054308223724366 2754052 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007502127764746546 15.127557468414306 0.5432277828454971 10.035836219787598 2959004 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0050240576732903715 23.175706100463866 0.42487589716911317 10.732553291320801 3164902 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004218851029872894 10.092317962646485 0.5802605986595154 9.945546627044678 3368119 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002171112154610455 32.25542850494385 0.3141323983669281 12.229767322540283 3573929 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0023040323168970643 15.0667142868042 0.6431419074535369 9.769924354553222 3779040 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007181338296504691 28.705978012084962 0.39056321084499357 11.838955497741699 3985570 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006855768428067677 16.67226324081421 0.39170593321323394 11.025998306274413 4193069 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -5.5938352306839076e-05 26.72162036895752 0.15862738713622093 12.549504470825195 4400947 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008276741005829536 14.998936939239503 0.2882385358214378 11.582987976074218 4605693 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006853958795545623 20.46997814178467 0.3283631235361099 11.902172374725343 4811782 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  6.124211722635664e-06 25.54608287811279 0.3795704901218414 11.585229778289795 5017896 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -1.326166238868609e-05 16.370345497131346 0.23914081007242202 11.882673072814942 5225947 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00019592625185396172 19.325958442687988 0.253464537858963 11.754642009735107 5434602 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -9.332132685813122e-06 9.617436027526855 0.3757690727710724 11.84242115020752 5640461 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00023298304367926903 13.301236724853515 0.2149331673979759 11.772337341308594 5847283 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006360079627484083 9.84552984237671 0.3300513237714767 12.124122142791748 6053000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014860807903460227 16.92565212249756 0.23908500820398332 12.491075611114502 6258090 0
Fatal Python error: Segmentation fault

Current thread 0x00001485aba84b80 (most recent call first):
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/rl_environment.py", line 330 in step
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/rl_oracle_cooperative.py", line 481 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/rl_oracle.py", line 223 in _rollout
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/rl_oracle_cooperative.py", line 190 in __call__
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/psro_v2.py", line 413 in update_agents
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 201 in iteration
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 403 in gpsro_looper
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 482 in main
  File "/home/anrigu/.local/lib/python3.10/site-packages/absl/app.py", line 254 in _run_main
  File "/home/anrigu/.local/lib/python3.10/site-packages/absl/app.py", line 308 in run
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 485 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, google._upb._message, tensorflow.python.framework.fast_tensor_util, charset_normalizer.md, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, PIL._imaging, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label, cvxopt.base, cvxopt.blas, cvxopt.lapack (total: 117)
/var/spool/slurmd.spool/job55931189/slurm_script: line 34: 749192 Segmentation fault      (core dumped) python3 -u psro_v2_simultaneous.py --game_name=harvest --save_folder_path="../examples/data/harvest/vanilla_ppo_full/($trial)_$parameter_id" --meta_strategy_method=prd_collab --gpsro_iterations=30 --consensus_imitation=False --regret_lambda_init=$curr_lambda --regret_steps=$curr_steps --policy_constraint_steps=20 --joint_action=False --rewards_joint=False --fine_tune=True --clear_trajectories=False --consensus_hidden_layer_size=50 --consensus_n_hidden_layers=2 --min_buffer_size_fine_tune=10000 --epochs_ppo=10 --minibatches_ppo=50 --number_training_steps=8000000 --symmetric_game=True --sims_per_entry=200 --eps_clip=.1 --eps_clip_value=.5 --entropy_decay_duration=.5 --ppo_entropy=.05 --fine_tune_policy_lr=.00003 --fine_tune_value_lr=.0003 --transfer_policy_minimum_entropy=0 --recovery_window=200 --hidden_layer_size=50 --n_hidden_layers=2 --batch_size=64 --dqn_learning_rate=3e-4 --update_target_network_every=500 --learn_every=1 --max_buffer_size=100000 --epsilon_decay_duration=100000 --min_buffer_size_to_learn=20000 --regret_calculation_steps=1500000
