Job Id listed below:
55931196

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

2023-07-16 19:45:49.903944: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-16 19:45:51.272573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0716 19:45:56.304053 22540549827456 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x147fcb776d40>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x147fcb776d40>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-07-16 19:45:56.625749: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-07-16 19:45:56.948207: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 17.79 seconds to finish estimate with resulting utilities: [48.51  48.665]
Exited RRD with total regret 0.0 that was less than regret lambda 5.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    48.59      

 

Player 1 Payoff matrix: 

           0      
    0    48.59      

 

Social Welfare Sum Matrix: 

           0      
    0    97.17      

 

Iteration : 0
Time so far: 0.00017952919006347656
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-07-16 19:46:15.628977: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.1244439959526062 17.033975315093993 2.058599090576172 0.0012505934922955931 10050 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09990680888295174 14.027532196044922 1.874650490283966 0.24046221375465393 215276 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09319993406534195 12.505136203765868 1.8418161869049072 0.3514964669942856 418148 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08786218538880348 16.92073612213135 1.8406741857528686 0.4060752928256989 620480 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07992297261953354 18.3000825881958 1.8053658962249757 0.49869667887687685 822429 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07096917256712913 15.560894584655761 1.7721201062202454 0.6076904714107514 1024615 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0691013477742672 15.693562984466553 1.7198615789413452 0.7024153709411621 1227268 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05967100486159325 19.595704650878908 1.6239865899086 0.9382728159427642 1431043 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05067238137125969 19.281828212738038 1.5901809453964233 0.993468028306961 1636330 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.048882638290524486 20.312840080261232 1.5606658458709717 1.1665340065956116 1845795 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03610177263617516 20.460300636291503 1.4728313207626342 1.4165458798408508 2052685 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03291096650063992 25.4868501663208 1.3972755312919616 1.5778613567352295 2262173 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02946874126791954 22.138934707641603 1.382088565826416 1.7061020016670227 2471323 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024227763526141644 18.537803077697752 1.3188639402389526 1.7063986301422118 2681151 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021482597663998605 26.416491508483887 1.2708333015441895 1.971126127243042 2891172 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016023478750139476 19.827080249786377 1.2119218230247497 2.170199251174927 3101219 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011921190284192562 24.192247009277345 1.1666885137557983 2.314847302436829 3313129 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009190640086308122 31.087426948547364 1.1307302236557006 2.4953858137130736 3526061 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004956581653095782 21.272797775268554 1.018585991859436 2.667221474647522 3738002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008891561679774896 25.030278587341307 0.9288899540901184 3.049681377410889 3949529 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001191250306146685 22.26614761352539 0.8533827841281891 3.4605886697769166 4165810 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021559908106610235 23.236241149902344 0.7659425556659698 3.7975438833236694 4379790 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002353622434020508 25.39663944244385 0.7135192632675171 4.136179780960083 4597870 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011285587446764112 23.246567153930663 0.6929863393306732 4.48229603767395 4811856 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0023546276264823974 22.96145133972168 0.6420301616191864 4.761731100082398 5028287 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016964957088930533 24.951608848571777 0.5862239599227905 5.071032094955444 5243187 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005932764383032918 23.853911781311034 0.5476079463958741 5.3995014190673825 5460480 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014607071003410964 27.760778617858886 0.5235268622636795 5.72691388130188 5676125 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013372080749832094 22.362231254577637 0.4842193365097046 6.108104562759399 5891423 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001154321656213142 23.899117088317873 0.47346803843975066 6.184139823913574 6109853 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012849474849645048 24.606318092346193 0.46652105152606965 6.672446918487549 6325605 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0020898586022667586 23.096962547302248 0.4821944922208786 6.857684564590454 6543346 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018194383737863973 23.256641578674316 0.4690283119678497 7.186323785781861 6757577 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013972826549434104 24.31839771270752 0.4081855446100235 7.497291088104248 6974528 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0020813201321288944 27.476638984680175 0.41564962565898894 7.337946653366089 7192770 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008018194872420281 24.392146110534668 0.3910563439130783 7.33214168548584 7410423 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021274696220643817 23.874867820739745 0.37645356059074403 7.2843376159667965 7628465 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012843992531998082 26.146016693115236 0.3663336545228958 7.322984123229981 7846737 0


Pure best response payoff estimated to be 194.48 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 78.16 seconds to finish estimate with resulting utilities: [191.5     4.585]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 128.19 seconds to finish estimate with resulting utilities: [100.09  97.55]
Computing meta_strategies
Exited RRD with total regret 4.963322047322038 that was less than regret lambda 5.0 after 33 iterations 
REGRET STEPS:  15
NEW LAMBDA 4.642857142857143
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    48.59           4.58      
    1    191.50          98.82      

 

Player 1 Payoff matrix: 

           0              1      
    0    48.59          191.50      
    1     4.58          98.82      

 

Social Welfare Sum Matrix: 

           0              1      
    0    97.17          196.09      
    1    196.09          197.64      

 

Metagame probabilities: 
Player #0: 0.026  0.974  
Player #1: 0.026  0.974  
Iteration : 1
Time so far: 6147.494186401367
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-07-16 21:28:43.355741: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025136028602719306 62.75804138183594 0.47097978591918943 9.261840724945069 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02323543969541788 14.280514144897461 0.486679008603096 8.054333019256593 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028093442134559155 22.684318923950194 0.6161323249340057 6.618470239639282 450423 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032438280805945394 20.834342575073244 0.7454948425292969 6.159005641937256 669179 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028491964377462865 20.246016693115234 0.7099986374378204 5.888608121871949 887833 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026784054934978485 19.723242378234865 0.7086069643497467 6.130035781860352 1106254 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029776585660874842 15.752838325500488 0.838248348236084 5.562902307510376 1324266 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026694410666823386 15.509643363952637 0.8302084267139435 5.2100731372833256 1543115 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025038540922105314 18.52085189819336 0.8525668382644653 5.2056389331817625 1761934 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023868972808122633 16.600909519195557 0.8730786561965942 5.0538583278656 1978338 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020561868883669378 21.265909576416014 0.8352034270763398 4.917675924301148 2195389 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019238376710563897 18.92638053894043 0.8447306871414184 5.285901594161987 2412239 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015409958641976118 19.647757244110107 0.8843741714954376 4.690292930603027 2631320 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0130670634098351 22.597655296325684 0.7987527310848236 5.232623958587647 2849979 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011252629198133945 19.53761692047119 0.8507039964199066 4.901065492630005 3065637 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008367267064750194 17.344318294525145 0.8204517662525177 4.999861764907837 3283878 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006669477978721261 18.751443099975585 0.7953326940536499 5.05316858291626 3498272 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0029277226189151407 15.357989883422851 0.7244777143001556 5.245612001419067 3718272 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0026851226924918593 17.26467580795288 0.7968471765518188 5.007548379898071 3937617 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008617781473731157 22.42447872161865 0.6678946018218994 5.5236622333526615 4156166 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005431628436781466 23.01970043182373 0.6947343230247498 5.042335081100464 4376166 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022662542058242254 19.18312339782715 0.6771470606327057 5.376491403579712 4596166 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011864436673931777 21.547243881225587 0.5932230949401855 5.709295225143433 4813844 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008710064648767002 17.943272590637207 0.5204865574836731 5.420063829421997 5033844 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009919782478391426 20.56775817871094 0.6090476155281067 5.531269598007202 5253844 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013965841208118945 27.702939605712892 0.5642694950103759 5.923277187347412 5473225 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006536879503983073 23.67929916381836 0.5045474648475647 6.241914939880371 5693203 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010326663679734339 17.072282314300537 0.5516481071710586 6.120554733276367 5913203 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010856348817469552 22.98609790802002 0.5107197731733322 6.392931365966797 6132815 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009260599879780784 17.71458806991577 0.4041222482919693 7.269464731216431 6351453 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010086001711897552 16.138353538513183 0.4417655527591705 7.391267585754394 6571453 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005898642764077522 34.4793586730957 0.4204899609088898 8.391341209411621 6791453 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  5.6696758838370444e-05 29.18084125518799 0.4049257516860962 8.188989686965943 7011453 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011969526676693931 18.652278709411622 0.38613990545272825 8.283467388153076 7231453 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011359480107785203 19.1231388092041 0.3137131631374359 8.91036901473999 7451453 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011721555289113893 20.34599781036377 0.312863877415657 8.590106964111328 7669984 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009681528365035775 22.996986961364748 0.2871719509363174 9.057313251495362 7888953 0


Pure best response payoff estimated to be 137.98 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 78.32 seconds to finish estimate with resulting utilities: [180.815   2.65 ]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 130.65 seconds to finish estimate with resulting utilities: [134.975  49.25 ]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 130.34 seconds to finish estimate with resulting utilities: [31.52  30.675]
Computing meta_strategies
Exited RRD with total regret 4.6408993157554335 that was less than regret lambda 4.642857142857143 after 57 iterations 
REGRET STEPS:  15
NEW LAMBDA 4.2857142857142865
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    48.59           4.58           2.65      
    1    191.50          98.82          49.25      
    2    180.81          134.97          31.10      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    48.59          191.50          180.81      
    1     4.58          98.82          134.97      
    2     2.65          49.25          31.10      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    97.17          196.09          183.47      
    1    196.09          197.64          184.22      
    2    183.47          184.22          62.20      

 

Metagame probabilities: 
Player #0: 0.0048  0.4203  0.575  
Player #1: 0.0048  0.4203  0.575  
Iteration : 2
Time so far: 14357.565761327744
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-07-16 23:45:33.394906: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017584231030195952 94.93029251098633 0.37410474717617037 9.369909858703613 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04189301393926144 18.02571048736572 0.872187715768814 5.664843654632568 230726 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03369022123515606 20.533937835693358 0.7392102003097534 5.7821879386901855 447968 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03713263478130102 21.47622013092041 0.8738962411880493 5.369473266601562 662810 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03736872635781765 17.8449405670166 0.8967138469219208 5.148414373397827 880342 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03283882699906826 21.613728713989257 0.8803745925426483 5.14608359336853 1095152 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03408600687980652 14.51741590499878 0.9915442407131195 4.762233877182007 1308338 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027952815219759942 21.718587112426757 0.8707336544990539 5.1522407054901125 1520572 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028656135872006418 17.06316556930542 0.9803654730319977 4.59157509803772 1737673 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02315358929336071 26.273943519592287 0.8435576796531677 5.35252103805542 1953564 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02309876251965761 20.273104667663574 0.8852806031703949 5.440173816680908 2171130 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019425328820943832 16.18540735244751 0.8982650518417359 4.887638282775879 2382882 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015834758616983892 22.693321990966798 0.73645498752594 5.968225717544556 2593898 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013879663031548261 20.82459716796875 0.7912822842597962 5.353589296340942 2802220 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010967588517814875 23.387086868286133 0.7885018467903138 5.8332582950592045 3013979 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009719931241124868 26.511788177490235 0.8902775406837463 5.453631973266601 3224745 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007172364043071866 15.01464614868164 0.8608499467372894 5.451204109191894 3435046 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004164098517503589 24.849046516418458 0.6683436453342437 6.597742652893066 3645877 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018307647900655867 21.56359996795654 0.7141699016094207 6.413688039779663 3855859 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008929871313739568 24.173398399353026 0.7471174836158753 6.210856676101685 4064652 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013403378543443978 21.540910148620604 0.6114336311817169 6.390746212005615 4273092 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004341522959293798 21.481049919128417 0.5403006613254547 6.3482677936553955 4482353 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017114750517066568 21.326612091064455 0.2647809937596321 7.5313255310058596 4692965 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009307740205258596 27.652778244018556 0.1950332850217819 8.369103288650512 4902858 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007928411156171933 17.3000919342041 0.2347264215350151 6.792132091522217 5112125 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00020755620207637548 24.44258918762207 0.1292152926325798 8.075981378555298 5321068 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017709240375552327 18.559665298461915 0.13194214850664138 8.29421534538269 5530653 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013610201771371067 22.736708641052246 0.15017435252666472 8.45781660079956 5740215 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000830557213339489 19.744544219970702 0.178187495470047 7.9447966575622555 5950498 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00048479238903382794 21.090005683898926 0.14885633960366249 8.612219047546386 6160418 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011260809013037942 28.498376846313477 0.16010019481182097 9.027320766448975 6370111 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011176894637173972 20.95877170562744 0.14401571303606034 8.434198474884033 6579667 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008821640534733888 21.92914733886719 0.12174700945615768 9.214913654327393 6788955 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016048019300797024 28.68412971496582 0.21795196682214737 8.724594211578369 6996504 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000882770586758852 22.578359413146973 0.1953296482563019 9.55901050567627 7205045 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00011356325085216667 16.906650257110595 0.1564530462026596 9.52338628768921 7413090 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00020784511434612796 20.836957931518555 0.17183489203453065 9.009759330749512 7623089 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003904589142621262 21.829248237609864 0.1545240744948387 9.21868963241577 7832432 0
Recovering previous policy with expected return of 77.03482587064677. Long term value was 64.959 and short term was 68.7.


Pure best response payoff estimated to be 82.975 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 79.23 seconds to finish estimate with resulting utilities: [179.075   2.7  ]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 130.17 seconds to finish estimate with resulting utilities: [134.53   47.685]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 131.56 seconds to finish estimate with resulting utilities: [30.24  30.575]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 131.3 seconds to finish estimate with resulting utilities: [31.69 32.77]
Computing meta_strategies
Exited RRD with total regret 4.089537454862295 that was less than regret lambda 4.2857142857142865 after 32 iterations 
REGRET STEPS:  15
NEW LAMBDA 3.9285714285714293
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0    48.59           4.58           2.65           2.70      
    1    191.50          98.82          49.25          47.69      
    2    180.81          134.97          31.10          30.57      
    3    179.07          134.53          30.24          32.23      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0    48.59          191.50          180.81          179.07      
    1     4.58          98.82          134.97          134.53      
    2     2.65          49.25          31.10          30.24      
    3     2.70          47.69          30.57          32.23      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0    97.17          196.09          183.47          181.77      
    1    196.09          197.64          184.22          182.22      
    2    183.47          184.22          62.20          60.81      
    3    181.77          182.22          60.81          64.46      

 

Metagame probabilities: 
Player #0: 0.0297  0.3285  0.3214  0.3204  
Player #1: 0.0297  0.3285  0.3214  0.3204  
Iteration : 3
Time so far: 22850.717972517014
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-07-17 02:07:06.672720: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7779 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0173626565374434 97.68742980957032 0.3555785208940506 9.556019115447999 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03385985177010298 21.1179292678833 0.7083443820476532 6.215817594528199 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.038513879477977755 20.029342079162596 0.8690936982631683 5.793974304199219 449977 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03909331299364567 16.66498613357544 0.936187207698822 5.763921213150025 667050 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03989208415150643 12.191751098632812 0.9987936437129974 5.284794712066651 876529 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03361787684261799 19.933639144897462 0.911822521686554 5.304015636444092 1086075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.036069486290216446 12.58632164001465 0.9990241050720214 5.251740503311157 1296658 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027468291111290455 15.787811374664306 0.837945282459259 5.371141576766968 1504392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026648163981735706 16.92586498260498 0.9231469094753265 5.330747795104981 1713905 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0248783890157938 25.65283203125 0.9139909029006958 5.311777591705322 1922922 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02452648989856243 13.389389610290527 0.97086141705513 5.398258781433105 2132990 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020757106505334376 11.665358829498292 0.9354635536670685 5.2373284816741945 2342450 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015829710103571416 17.626220512390137 0.8463136553764343 5.201023149490356 2552945 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013902242388576269 18.497152042388915 0.8483318865299225 5.498932552337647 2760755 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01227312944829464 9.116255760192871 0.8263541340827942 5.967125701904297 2967578 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010840535629540682 12.466070556640625 0.9231931984424591 5.1798254489898685 3173897 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00638731881044805 18.457254219055176 0.7468515157699585 6.093490505218506 3383171 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004169504973106086 11.134410190582276 0.7528647720813751 6.066075849533081 3590221 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0031018002657219766 12.905790424346923 0.7647398710250854 6.224890327453613 3797366 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001901999645633623 21.822834396362303 0.6043525993824005 6.910905170440674 4006180 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012511254462879151 10.781887340545655 0.6739800989627838 6.40807614326477 4215037 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0029714060088736004 14.099983406066894 0.4022153943777084 6.79493989944458 4423521 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008546023163944483 17.68909683227539 0.3543950140476227 6.809795999526978 4634073 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016947685740888118 15.916347885131836 0.4508966445922852 6.561566257476807 4842149 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006855980405816809 13.88191614151001 0.2505017265677452 7.043266344070434 5052456 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007283810380613431 19.4974271774292 0.19259250313043594 7.465094995498657 5261129 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007198040537332417 21.849630737304686 0.16272144317626952 7.739012908935547 5470228 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002046797442744719 22.848533630371094 0.16834107488393785 7.525446176528931 5679948 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006003910137224011 12.650565242767334 0.14356942921876908 8.090178632736206 5887001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008582219728850759 24.230609130859374 0.0946007564663887 9.298423576354981 6097279 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006602921814192087 13.969005680084228 0.10471139252185821 9.545735073089599 6307506 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009380417144257081 20.682566356658935 0.12458957359194756 9.205764007568359 6516011 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001230056381518807 16.2044527053833 0.10156959444284439 8.993780040740967 6724411 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006786696685594507 17.221328258514404 0.1335390217602253 8.553025436401366 6933466 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012846424312471072 12.925837898254395 0.12016371488571168 9.093499183654785 7143860 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016350634046830236 15.903336143493652 0.1339036628603935 9.210374641418458 7353219 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003686357544211205 17.777466773986816 0.12078596949577332 9.779603195190429 7560510 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005758974060881883 15.141205024719238 0.1105403371155262 10.00576114654541 7768205 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007154263643315062 18.983348846435547 0.08597215712070465 10.55148630142212 7977398 0
Recovering previous policy with expected return of 71.92537313432835. Long term value was 54.017 and short term was 50.805.


Pure best response payoff estimated to be 83.23 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 79.25 seconds to finish estimate with resulting utilities: [180.97    2.885]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 130.26 seconds to finish estimate with resulting utilities: [134.765  49.48 ]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 132.92 seconds to finish estimate with resulting utilities: [29.905 32.885]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 132.66 seconds to finish estimate with resulting utilities: [30.76 32.11]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 131.47 seconds to finish estimate with resulting utilities: [32.465 30.365]
Computing meta_strategies
Exited RRD with total regret 3.8876012412953216 that was less than regret lambda 3.9285714285714293 after 56 iterations 
REGRET STEPS:  15
NEW LAMBDA 3.571428571428572
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0    48.59           4.58           2.65           2.70           2.88      
    1    191.50          98.82          49.25          47.69          49.48      
    2    180.81          134.97          31.10          30.57          32.88      
    3    179.07          134.53          30.24          32.23          32.11      
    4    180.97          134.76          29.91          30.76          31.41      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0    48.59          191.50          180.81          179.07          180.97      
    1     4.58          98.82          134.97          134.53          134.76      
    2     2.65          49.25          31.10          30.24          29.91      
    3     2.70          47.69          30.57          32.23          30.76      
    4     2.88          49.48          32.88          32.11          31.41      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0    97.17          196.09          183.47          181.77          183.85      
    1    196.09          197.64          184.22          182.22          184.24      
    2    183.47          184.22          62.20          60.81          62.79      
    3    181.77          182.22          60.81          64.46          62.87      
    4    183.85          184.24          62.79          62.87          62.83      

 

Metagame probabilities: 
Player #0: 0.0067  0.2868  0.2392  0.2363  0.231  
Player #1: 0.0067  0.2868  0.2392  0.2363  0.231  
Iteration : 4
Time so far: 31472.552087545395
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-07-17 04:30:48.639034: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9757 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016162033565342425 44.99308319091797 0.29159257411956785 10.878952789306641 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029568099044263364 12.194071578979493 0.6077703893184662 8.583087682723999 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026480866223573686 21.701395988464355 0.5572761178016663 7.465467357635498 448175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03335710186511278 18.87468328475952 0.7635121881961823 7.228781604766846 663777 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03772267587482929 16.92725191116333 0.9209140777587891 6.030240392684936 878128 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03929915726184845 13.68372802734375 1.0511290192604066 5.722829580307007 1088049 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.033433301374316216 13.059499359130859 0.9662677228450776 5.962549638748169 1298150 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031286613270640376 12.72165355682373 0.9409089982509613 5.805244398117066 1507102 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029402191005647182 10.825759410858154 1.0009685337543488 5.581462764739991 1715547 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0265131713822484 13.657477951049804 1.007445353269577 5.755087089538574 1923858 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023256481066346167 14.032579612731933 0.9642330348491669 5.806197738647461 2131902 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019542653299868106 12.556583499908447 0.9652036547660827 5.402124738693237 2341218 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019411404617130756 16.605861282348634 1.0118906199932098 5.580593156814575 2551770 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016290144249796867 13.709237194061279 0.9855165779590607 5.295550394058227 2758052 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010799699276685715 12.169054412841797 0.7886678338050842 7.182726621627808 2966724 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010035842843353748 12.866569328308106 0.9201084673404694 5.876233959197998 3174336 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007329202396795154 20.013528060913085 0.8159871280193329 6.506786346435547 3383641 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00485825992655009 13.306210136413574 0.8358993470668793 6.406701898574829 3594932 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003434119839221239 17.84399061203003 0.874261486530304 5.798598575592041 3803982 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011818933344329707 16.339128017425537 0.7933489143848419 6.735354042053222 4015204 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019056129036471247 17.875196552276613 0.5679830610752106 7.240024042129517 4223529 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006910849886480718 22.055634880065917 0.6495546758174896 7.0619669437408445 4435295 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001010233667329885 14.099284839630126 0.6884953379631042 6.543173742294312 4643547 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014716334582772106 18.43335704803467 0.4759948819875717 7.039216375350952 4852284 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00029764174942101815 16.448041248321534 0.6451554536819458 6.950058794021606 5061140 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006331169290206163 18.09358491897583 0.43212089538574217 7.600714206695557 5270275 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008854951491230167 21.240086555480957 0.3318840801715851 7.79890398979187 5477145 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007697348897636403 14.959139823913574 0.232394939661026 8.037519168853759 5686809 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004435610549990088 11.314379119873047 0.1803448259830475 8.706088256835937 5894981 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005991498837829567 14.824621200561523 0.1677020624279976 9.062509632110595 6104454 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005963729112409055 17.505015659332276 0.15637348890304564 10.121174621582032 6312481 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009666157566243783 15.252675247192382 0.1716259926557541 10.11814479827881 6518782 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0002003537292694091 12.898263740539551 0.1668834865093231 9.70332670211792 6729047 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007686729994020424 12.021528339385986 0.17803361713886262 9.97452392578125 6938126 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008061900764005259 15.805063915252685 0.17938532531261445 9.745691585540772 7146667 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001763392179782386 13.503733158111572 0.12905259504914285 10.099505138397216 7354232 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014250741947762434 16.53344020843506 0.10767484605312347 10.628428554534912 7563608 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009892888192553073 13.899492073059083 0.11873788982629777 10.115555953979491 7772505 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008372338663320989 15.30750551223755 0.09586241394281388 10.803584480285645 7982653 0
Recovering previous policy with expected return of 64.21890547263682. Long term value was 47.306 and short term was 45.855.


Pure best response payoff estimated to be 69.985 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 80.66 seconds to finish estimate with resulting utilities: [177.69   2.39]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 133.93 seconds to finish estimate with resulting utilities: [136.41  47.71]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 134.28 seconds to finish estimate with resulting utilities: [31.975 30.09 ]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 134.43 seconds to finish estimate with resulting utilities: [32.47  31.995]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 135.12 seconds to finish estimate with resulting utilities: [33.365 33.45 ]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 134.2 seconds to finish estimate with resulting utilities: [30.505 29.58 ]
Computing meta_strategies
Exited RRD with total regret 3.5343270047602147 that was less than regret lambda 3.571428571428572 after 95 iterations 
REGRET STEPS:  15
NEW LAMBDA 3.214285714285715
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0    48.59           4.58           2.65           2.70           2.88           2.39      
    1    191.50          98.82          49.25          47.69          49.48          47.71      
    2    180.81          134.97          31.10          30.57          32.88          30.09      
    3    179.07          134.53          30.24          32.23          32.11          32.00      
    4    180.97          134.76          29.91          30.76          31.41          33.45      
    5    177.69          136.41          31.98          32.47          33.37          30.04      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0    48.59          191.50          180.81          179.07          180.97          177.69      
    1     4.58          98.82          134.97          134.53          134.76          136.41      
    2     2.65          49.25          31.10          30.24          29.91          31.98      
    3     2.70          47.69          30.57          32.23          30.76          32.47      
    4     2.88          49.48          32.88          32.11          31.41          33.37      
    5     2.39          47.71          30.09          32.00          33.45          30.04      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0    97.17          196.09          183.47          181.77          183.85          180.08      
    1    196.09          197.64          184.22          182.22          184.24          184.12      
    2    183.47          184.22          62.20          60.81          62.79          62.06      
    3    181.77          182.22          60.81          64.46          62.87          64.47      
    4    183.85          184.24          62.79          62.87          62.83          66.81      
    5    180.08          184.12          62.06          64.47          66.81          60.08      

 

Metagame probabilities: 
Player #0: 0.0007  0.2735  0.1764  0.1799  0.1787  0.1908  
Player #1: 0.0007  0.2735  0.1764  0.1799  0.1787  0.1908  
Iteration : 5
Time so far: 40305.36872148514
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-07-17 06:58:01.921590: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11735 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0180929584428668 99.02961120605468 0.3436299622058868 10.278956031799316 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031526295468211174 20.14341640472412 0.6627492547035218 7.891305160522461 229994 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04294215589761734 18.63819408416748 0.9382425963878631 5.184247589111328 449978 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034263746812939644 23.10578384399414 0.8147133529186249 5.821934795379638 667242 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03957052305340767 17.409039783477784 0.96756631731987 5.008214235305786 879821 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034025021083652976 15.804002475738525 0.8813988566398621 5.698239946365357 1089919 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03536832816898823 14.48953161239624 1.0247061491012572 4.777707624435425 1298971 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03147079776972532 10.827016067504882 0.9950249910354614 5.025751447677612 1510364 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028431962803006174 14.862692546844482 0.9916316151618958 4.887147569656372 1719664 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026045536436140538 17.745033550262452 0.9590475201606751 5.257472562789917 1928358 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018946296907961367 17.273113632202147 0.774524575471878 5.486509943008423 2137357 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0191907674074173 20.194425201416017 0.8330458283424378 5.6010383605957035 2345917 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018147226981818677 9.364709568023681 0.933230072259903 5.319151735305786 2553490 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015728962421417237 12.559630203247071 0.8764277815818786 5.419905471801758 2759656 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012432643584907055 9.222999572753906 0.9284899890422821 4.961663484573364 2965577 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009238035790622235 14.584614658355713 0.8186630070209503 5.839105606079102 3172749 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008152403309941292 11.885600185394287 0.9213352262973785 5.372046327590942 3379744 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0037027776008471845 10.628740406036377 0.7417256116867066 6.626396751403808 3587999 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0032017939141951502 9.034879207611084 0.7258162021636962 6.661845254898071 3795126 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009522541768092196 16.203144359588624 0.7639344811439515 6.737056922912598 4001768 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010136339435121045 9.911826038360596 0.7039406359195709 6.380828762054444 4208679 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009818903035920812 7.9526537418365475 0.6084132492542267 6.566297340393066 4413774 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001770530827343464 11.159969806671143 0.49296209812164304 7.662598037719727 4619435 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014189675450325013 7.5615815162658695 0.37209112048149107 7.6553661823272705 4825370 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010718535108026118 11.120971870422363 0.2057267114520073 8.754586791992187 5031814 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008920448512071743 11.532740783691406 0.24230121374130248 8.619129467010499 5237126 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00019594014934227745 8.958640670776367 0.2949764162302017 8.805935668945313 5442787 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016268838662654161 9.562145471572876 0.2901489853858948 9.159319210052491 5647761 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00013026783017267007 11.733605766296387 0.17635450512170792 9.34580659866333 5853844 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00100751593708992 12.543050384521484 0.08226607851684094 10.92725076675415 6059868 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00042410387291056393 11.473505306243897 0.09693837687373161 10.078356647491455 6265211 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004919583618175239 8.024020004272462 0.14967612773180008 9.928953742980957 6469355 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013067511965346057 9.206555557250976 0.14027779996395112 9.911657238006592 6675187 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0005385455166106113 8.345887184143066 0.12701266407966613 10.08381986618042 6880450 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0001256609335541725 12.903412342071533 0.1448776125907898 10.515305137634277 7086338 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007052018409012817 11.627416229248047 0.22903544157743455 10.165303897857665 7291526 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007608130821608938 10.125201988220216 0.2248977541923523 10.545786476135254 7497651 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015724205673905089 21.00859966278076 0.13796196803450583 11.474157905578613 7702868 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00034053478157147765 12.673893356323243 0.16270403414964676 10.809836864471436 7909110 0
Recovering previous policy with expected return of 64.2089552238806. Long term value was 20.65 and short term was 20.2.


Pure best response payoff estimated to be 73.87 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 83.37 seconds to finish estimate with resulting utilities: [178.945   2.24 ]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 135.73 seconds to finish estimate with resulting utilities: [136.8   48.68]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 136.16 seconds to finish estimate with resulting utilities: [33.045 33.72 ]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 136.28 seconds to finish estimate with resulting utilities: [34.515 33.565]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 136.2 seconds to finish estimate with resulting utilities: [29.305 29.42 ]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 136.0 seconds to finish estimate with resulting utilities: [35.47 35.86]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 134.89 seconds to finish estimate with resulting utilities: [30.805 32.75 ]
Computing meta_strategies
Exited RRD with total regret 3.1798776060688922 that was less than regret lambda 3.214285714285715 after 130 iterations 
REGRET STEPS:  15
NEW LAMBDA 2.8571428571428577
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    48.59           4.58           2.65           2.70           2.88           2.39           2.24      
    1    191.50          98.82          49.25          47.69          49.48          47.71          48.68      
    2    180.81          134.97          31.10          30.57          32.88          30.09          33.72      
    3    179.07          134.53          30.24          32.23          32.11          32.00          33.56      
    4    180.97          134.76          29.91          30.76          31.41          33.45          29.42      
    5    177.69          136.41          31.98          32.47          33.37          30.04          35.86      
    6    178.94          136.80          33.05          34.52          29.30          35.47          31.78      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    48.59          191.50          180.81          179.07          180.97          177.69          178.94      
    1     4.58          98.82          134.97          134.53          134.76          136.41          136.80      
    2     2.65          49.25          31.10          30.24          29.91          31.98          33.05      
    3     2.70          47.69          30.57          32.23          30.76          32.47          34.52      
    4     2.88          49.48          32.88          32.11          31.41          33.37          29.30      
    5     2.39          47.71          30.09          32.00          33.45          30.04          35.47      
    6     2.24          48.68          33.72          33.56          29.42          35.86          31.78      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0    97.17          196.09          183.47          181.77          183.85          180.08          181.19      
    1    196.09          197.64          184.22          182.22          184.24          184.12          185.48      
    2    183.47          184.22          62.20          60.81          62.79          62.06          66.77      
    3    181.77          182.22          60.81          64.46          62.87          64.47          68.08      
    4    183.85          184.24          62.79          62.87          62.83          66.81          58.73      
    5    180.08          184.12          62.06          64.47          66.81          60.08          71.33      
    6    181.19          185.48          66.77          68.08          58.73          71.33          63.55      

 

Metagame probabilities: 
Player #0: 0.0001  0.2675  0.1387  0.1415  0.1289  0.1595  0.1638  
Player #1: 0.0001  0.2675  0.1387  0.1415  0.1289  0.1595  0.1638  
Iteration : 6
Time so far: 49292.14868187904
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-07-17 09:27:48.482145: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13713 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013587257638573647 39.135238838195804 0.25101056694984436 10.869491004943848 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03680346235632896 16.981801509857178 0.757425844669342 7.081914281845092 229621 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04071843810379505 16.788950824737547 0.9142461359500885 6.576644945144653 446142 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.039368749409914014 16.51327018737793 0.9079318463802337 6.5083324909210205 662967 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0399894617497921 17.446990966796875 0.9706480622291564 6.041992998123169 874156 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03447569850832224 15.303500366210937 0.9175767064094543 6.307429456710816 1088516 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03920643478631973 12.774432182312012 1.1040477633476258 5.772784996032715 1299215 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02908394355326891 19.631885242462157 0.9173166394233704 6.183586597442627 1508791 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031183898635208606 12.262387084960938 1.0355438590049744 5.917428827285766 1717310 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027095222286880015 12.851821804046631 1.0160732984542846 6.234985637664795 1926712 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02217849288135767 12.279409217834473 0.8600494980812072 6.588472461700439 2137095 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0210152804851532 15.727742004394532 0.9938044428825379 5.968492841720581 2344704 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01855417024344206 16.64024257659912 0.9571846485137939 6.226192474365234 2554500 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015434784255921841 14.898826313018798 0.9325976848602295 5.931045007705689 2763792 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013254996854811907 15.392958164215088 0.956602257490158 5.947894716262818 2970651 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01146292993798852 11.276560497283935 1.0319251358509063 6.077220821380616 3177637 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007144714938476682 13.714790153503419 0.8036137402057648 6.969293689727783 3384241 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005546515271998942 10.68773193359375 0.8815385401248932 6.61587119102478 3589711 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003267067589331418 11.12375087738037 0.788937908411026 7.016236400604248 3794759 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009987032433855347 12.631922912597656 0.7493835747241974 7.494395780563354 4000665 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009322435536887497 12.334749507904053 0.6499268233776092 8.14874005317688 4205837 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014518856231006793 12.020335578918457 0.5438791811466217 7.701899194717408 4412133 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004363781918073073 9.001621913909911 0.582314532995224 8.431089782714844 4618374 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009951241525413934 9.576271963119506 0.5545071840286255 9.290274810791015 4825663 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00021900389074289706 17.905695629119872 0.4984752148389816 9.590026473999023 5032175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00014691090618725866 9.592447566986085 0.5943328857421875 9.639611148834229 5238527 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000621952471556142 14.86647071838379 0.43068400025367737 10.14929265975952 5444303 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004121175792533904 10.694917488098145 0.3922287255525589 10.302568531036377 5650711 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008392382653255482 10.835294818878173 0.21402897387742997 10.507895946502686 5858291 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005048118331615114 13.449409294128419 0.12785944119095802 10.969834232330323 6065877 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008939232066040859 12.886282539367675 0.117313402146101 10.935130405426026 6271344 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008043600362725555 13.25676097869873 0.07223920226097107 11.661737442016602 6476958 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009242844156688079 9.25874948501587 0.09006017073988914 11.638137245178223 6682240 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004889012554485817 8.327442312240601 0.09101707488298416 11.779250049591065 6890028 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00015459990936506074 13.869418525695801 0.08574265763163566 12.156686687469483 7098472 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016446343834104482 17.25630416870117 0.12105989679694176 12.263637447357178 7303253 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010745001414761645 8.46664981842041 0.14810676425695418 12.342369651794433 7507668 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  8.51087606861256e-05 15.061389350891114 0.15945195257663727 13.223580741882325 7715797 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002681708021555096 11.254985809326172 0.22668198943138124 12.965060138702393 7920742 0
Recovering previous policy with expected return of 58.80099502487562. Long term value was 25.118 and short term was 26.785.


Pure best response payoff estimated to be 70.86 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 81.9 seconds to finish estimate with resulting utilities: [179.03    2.255]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 133.42 seconds to finish estimate with resulting utilities: [135.315  47.975]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 135.7 seconds to finish estimate with resulting utilities: [28.15  28.285]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 135.08 seconds to finish estimate with resulting utilities: [30.435 32.715]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 134.91 seconds to finish estimate with resulting utilities: [31.16 29.25]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 135.09 seconds to finish estimate with resulting utilities: [34.045 33.51 ]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 136.03 seconds to finish estimate with resulting utilities: [33.535 31.065]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 132.79 seconds to finish estimate with resulting utilities: [31.915 33.315]
Computing meta_strategies
Exited RRD with total regret 2.8513182714478376 that was less than regret lambda 2.8571428571428577 after 162 iterations 
REGRET STEPS:  15
NEW LAMBDA 2.5000000000000004
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    48.59           4.58           2.65           2.70           2.88           2.39           2.24           2.25      
    1    191.50          98.82          49.25          47.69          49.48          47.71          48.68          47.98      
    2    180.81          134.97          31.10          30.57          32.88          30.09          33.72          28.29      
    3    179.07          134.53          30.24          32.23          32.11          32.00          33.56          32.72      
    4    180.97          134.76          29.91          30.76          31.41          33.45          29.42          29.25      
    5    177.69          136.41          31.98          32.47          33.37          30.04          35.86          33.51      
    6    178.94          136.80          33.05          34.52          29.30          35.47          31.78          31.07      
    7    179.03          135.31          28.15          30.43          31.16          34.05          33.53          32.61      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    48.59          191.50          180.81          179.07          180.97          177.69          178.94          179.03      
    1     4.58          98.82          134.97          134.53          134.76          136.41          136.80          135.31      
    2     2.65          49.25          31.10          30.24          29.91          31.98          33.05          28.15      
    3     2.70          47.69          30.57          32.23          30.76          32.47          34.52          30.43      
    4     2.88          49.48          32.88          32.11          31.41          33.37          29.30          31.16      
    5     2.39          47.71          30.09          32.00          33.45          30.04          35.47          34.05      
    6     2.24          48.68          33.72          33.56          29.42          35.86          31.78          33.53      
    7     2.25          47.98          28.29          32.72          29.25          33.51          31.07          32.61      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0    97.17          196.09          183.47          181.77          183.85          180.08          181.19          181.28      
    1    196.09          197.64          184.22          182.22          184.24          184.12          185.48          183.29      
    2    183.47          184.22          62.20          60.81          62.79          62.06          66.77          56.44      
    3    181.77          182.22          60.81          64.46          62.87          64.47          68.08          63.15      
    4    183.85          184.24          62.79          62.87          62.83          66.81          58.73          60.41      
    5    180.08          184.12          62.06          64.47          66.81          60.08          71.33          67.56      
    6    181.19          185.48          66.77          68.08          58.73          71.33          63.55          64.60      
    7    181.28          183.29          56.44          63.15          60.41          67.56          64.60          65.23      

 

Metagame probabilities: 
Player #0: 0.0001  0.2722  0.108  0.121  0.1022  0.1405  0.1382  0.1178  
Player #1: 0.0001  0.2722  0.108  0.121  0.1022  0.1405  0.1382  0.1178  
Iteration : 7
Time so far: 58420.83209347725
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-07-17 11:59:57.354562: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15691 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017600863613188265 103.12098846435546 0.3328617662191391 10.025133514404297 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04417047537863254 18.701957988739014 0.905483889579773 5.795947170257568 229646 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04245530031621456 12.773924446105957 0.92550408244133 5.659049654006958 449420 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03674202188849449 16.741698455810546 0.8331968426704407 5.818066740036011 669420 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.042041664198040965 12.558382892608643 1.0433315277099608 5.10258150100708 887840 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04039001315832138 13.821028518676759 1.0984980463981628 5.160099172592163 1107330 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034005196392536165 15.955924701690673 0.9859442412853241 5.374499416351318 1322842 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030388138070702553 16.309277057647705 0.9667492985725403 5.138965177536011 1538196 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027347096428275108 15.507985019683838 0.9410677075386047 5.646727514266968 1752283 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026959424279630183 21.927308082580566 0.7480580329895019 6.513973140716553 1966381 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024837001226842403 12.031034755706788 0.9966243147850037 5.3337317943573 2176693 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02141037378460169 17.76749210357666 0.9035760283470153 5.15708589553833 2388474 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020888268016278744 8.575487995147705 0.8906561195850372 5.614067888259887 2593134 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016081206779927016 12.014641571044923 0.8041079759597778 6.060434627532959 2801465 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009823707956820726 12.953577995300293 0.8463956415653229 5.8108031272888185 3006523 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008773485803976655 15.770042514801025 0.8389693975448609 5.751808309555054 3211491 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010837115347385406 11.531174087524414 0.7640759289264679 6.470795488357544 3417511 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005661774217151106 6.8833705425262455 0.9015113115310669 5.754795694351197 3624156 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002588942495640367 8.026531791687011 0.8360646724700928 6.1646981716156 3829124 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001846494252094999 11.369873523712158 0.7994176030158997 6.6532285690307615 4032554 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004459638599655591 10.211692333221436 0.6773480832576751 7.010302877426147 4238474 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005506149231223389 12.556809425354004 0.5879455864429474 7.276005840301513 4443696 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003728988489456242 8.925381469726563 0.43367269933223723 7.9149411678314205 4648384 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000793627355596982 9.140243530273438 0.21714164614677428 9.077208709716796 4854010 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007141755486372858 12.685222244262695 0.1500823587179184 10.669754886627198 5058643 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000478859060240211 19.139968872070312 0.0690374232828617 11.745898818969726 5262348 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005143288694853254 15.186752605438233 0.06524318642914295 11.580830955505371 5466740 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007108850812073797 14.292554950714111 0.059310685470700265 11.936336135864257 5672524 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -2.3805787350283936e-06 8.847334718704223 0.053974186256527903 12.312432861328125 5875827 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010916681647358927 7.745498609542847 0.07159778214991093 12.28668212890625 6080580 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006787561891542282 7.222766971588134 0.07899749204516411 12.507649040222168 6284828 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00016574013789067977 7.589498615264892 0.06229944080114365 12.469560623168945 6488029 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006970197347982321 10.809595966339112 0.11458050906658172 12.695871067047118 6693941 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00017241655241377883 8.14548225402832 0.06761793941259384 12.752550315856933 6901532 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012190162247861735 8.1309344291687 0.06492385491728783 13.201599884033204 7105307 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007440590212354437 7.828719425201416 0.05358744338154793 13.140985298156739 7310277 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00011258895974606276 13.213426113128662 0.05273320712149143 13.32263536453247 7514641 0
/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/imitation_fine_tune.py:379: RuntimeWarning: invalid value encountered in divide
  legal_probs = legal_probs / np.sum(legal_probs)
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007657789225049783 9.585105037689209 0.043136018142104146 13.935697460174561 7719260 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010902381749474444 9.448789072036742 0.047430599108338356 13.97635612487793 7924168 0
Recovering previous policy with expected return of 60.666666666666664. Long term value was 18.771 and short term was 18.155.


Pure best response payoff estimated to be 72.96 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (8, 0)
Current player 0 and current strategies (8, 0) took 82.41 seconds to finish estimate with resulting utilities: [180.73   2.73]
Estimating current strategies:  (8, 1)
Current player 0 and current strategies (8, 1) took 135.57 seconds to finish estimate with resulting utilities: [134.56   48.975]
Estimating current strategies:  (8, 2)
Current player 0 and current strategies (8, 2) took 135.73 seconds to finish estimate with resulting utilities: [30.63  31.735]
Estimating current strategies:  (8, 3)
Current player 0 and current strategies (8, 3) took 135.99 seconds to finish estimate with resulting utilities: [31.215 28.79 ]
Estimating current strategies:  (8, 4)
Current player 0 and current strategies (8, 4) took 135.15 seconds to finish estimate with resulting utilities: [30.395 30.19 ]
Estimating current strategies:  (8, 5)
Current player 0 and current strategies (8, 5) took 135.9 seconds to finish estimate with resulting utilities: [32.12  30.855]
Estimating current strategies:  (8, 6)
Current player 0 and current strategies (8, 6) took 136.0 seconds to finish estimate with resulting utilities: [31.93  30.905]
Estimating current strategies:  (8, 7)
Current player 0 and current strategies (8, 7) took 135.68 seconds to finish estimate with resulting utilities: [32.885 32.62 ]
Estimating current strategies:  (8, 8)
Current player 0 and current strategies (8, 8) took 134.79 seconds to finish estimate with resulting utilities: [30.78  32.305]
Computing meta_strategies
Exited RRD with total regret 2.4995298392484813 that was less than regret lambda 2.5000000000000004 after 191 iterations 
REGRET STEPS:  15
NEW LAMBDA 2.1428571428571432
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    48.59           4.58           2.65           2.70           2.88           2.39           2.24           2.25           2.73      
    1    191.50          98.82          49.25          47.69          49.48          47.71          48.68          47.98          48.98      
    2    180.81          134.97          31.10          30.57          32.88          30.09          33.72          28.29          31.73      
    3    179.07          134.53          30.24          32.23          32.11          32.00          33.56          32.72          28.79      
    4    180.97          134.76          29.91          30.76          31.41          33.45          29.42          29.25          30.19      
    5    177.69          136.41          31.98          32.47          33.37          30.04          35.86          33.51          30.86      
    6    178.94          136.80          33.05          34.52          29.30          35.47          31.78          31.07          30.91      
    7    179.03          135.31          28.15          30.43          31.16          34.05          33.53          32.61          32.62      
    8    180.73          134.56          30.63          31.21          30.39          32.12          31.93          32.88          31.54      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    48.59          191.50          180.81          179.07          180.97          177.69          178.94          179.03          180.73      
    1     4.58          98.82          134.97          134.53          134.76          136.41          136.80          135.31          134.56      
    2     2.65          49.25          31.10          30.24          29.91          31.98          33.05          28.15          30.63      
    3     2.70          47.69          30.57          32.23          30.76          32.47          34.52          30.43          31.21      
    4     2.88          49.48          32.88          32.11          31.41          33.37          29.30          31.16          30.39      
    5     2.39          47.71          30.09          32.00          33.45          30.04          35.47          34.05          32.12      
    6     2.24          48.68          33.72          33.56          29.42          35.86          31.78          33.53          31.93      
    7     2.25          47.98          28.29          32.72          29.25          33.51          31.07          32.61          32.88      
    8     2.73          48.98          31.73          28.79          30.19          30.86          30.91          32.62          31.54      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8      
    0    97.17          196.09          183.47          181.77          183.85          180.08          181.19          181.28          183.46      
    1    196.09          197.64          184.22          182.22          184.24          184.12          185.48          183.29          183.53      
    2    183.47          184.22          62.20          60.81          62.79          62.06          66.77          56.44          62.36      
    3    181.77          182.22          60.81          64.46          62.87          64.47          68.08          63.15          60.00      
    4    183.85          184.24          62.79          62.87          62.83          66.81          58.73          60.41          60.59      
    5    180.08          184.12          62.06          64.47          66.81          60.08          71.33          67.56          62.97      
    6    181.19          185.48          66.77          68.08          58.73          71.33          63.55          64.60          62.84      
    7    181.28          183.29          56.44          63.15          60.41          67.56          64.60          65.23          65.50      
    8    183.46          183.53          62.36          60.00          60.59          62.97          62.84          65.50          63.09      

 

Metagame probabilities: 
Player #0: 0.0001  0.2805  0.0934  0.0986  0.0854  0.1211  0.1193  0.1044  0.0971  
Player #1: 0.0001  0.2805  0.0934  0.0986  0.0854  0.1211  0.1193  0.1044  0.0971  
Iteration : 8
Time so far: 67718.03892207146
Approximating Best Response
Training best response:  True 0.06
rtg values check:  (?,) (?, 1)
2023-07-17 14:34:55.025594: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_62/weights_2/Adam_1/Assign' id:17669 op device:{requested: '', assigned: ''} def:{{{node mlp_62/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_62/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_62/weights_2/Adam_1, mlp_62/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01233505755662918 77.79062576293946 0.23882679790258407 11.364772605895997 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02866220846772194 18.83239879608154 0.5812034428119659 7.562563276290893 229196 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04560092426836491 19.378242111206056 0.9865853369235993 5.538886213302613 446609 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.043616241216659545 15.058604049682618 1.0274414718151093 5.18638768196106 659579 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03670371025800705 21.276721954345703 0.8862301290035248 5.7702985286712645 870148 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.041070792451500894 10.599770545959473 1.0725166976451874 5.049502801895142 1076999 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03418857455253601 13.601250553131104 0.9801862001419067 5.266127395629883 1284872 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0331711646169424 16.426937770843505 1.0141704082489014 5.12604775428772 1492314 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02718668691813946 21.933737373352052 0.8938244462013245 5.844185733795166 1701301 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025879268161952497 16.6562611579895 0.9384151935577393 5.548708486557007 1908687 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022602234967052937 22.197181510925294 0.9234038889408112 5.390561294555664 2116957 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01767024789005518 15.410137557983399 0.7924552619457245 6.333083200454712 2322954 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018487920891493557 17.22087173461914 0.913266658782959 5.463944292068481 2531141 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016393376421183346 11.065801334381103 0.9538316249847412 5.574131679534912 2738559 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012174152489751577 13.367880630493165 0.863474678993225 6.06038179397583 2947614 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010898546176031232 15.541414737701416 0.9302052319049835 5.766733646392822 3154946 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007317672390490771 20.05502872467041 0.8042682945728302 6.2893181324005125 3363379 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006601060880348087 17.24162836074829 0.7903212010860443 6.276539516448975 3573768 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003172055562026799 20.765745162963867 0.8199892163276672 5.922437715530395 3782717 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011320847639581188 16.220016956329346 0.6059225916862487 7.053123331069946 3992425 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008115859120152891 18.7990385055542 0.7033764123916626 6.690569019317627 4201573 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00128609346866142 17.422175025939943 0.5967322826385498 6.881990814208985 4412812 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009030108696606476 18.189805889129637 0.5970955491065979 6.898013257980347 4622258 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010414564865641295 36.139874649047854 0.4436837464570999 7.996747541427612 4834405 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00018961763635161333 18.729142570495604 0.340839222073555 7.686775350570679 5044363 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005228751957474742 15.652001190185548 0.3470176845788956 7.160657215118408 5254596 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  9.658882991061545e-05 17.744851207733156 0.32030878961086273 8.371507263183593 5462768 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006908301060320809 13.411685943603516 0.3927853018045425 8.399003314971925 5674071 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  9.000197424029466e-05 17.562181186676025 0.3506165772676468 8.445117378234864 5884510 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006023152556736023 12.718272876739501 0.31344920098781587 8.738944816589356 6093231 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013761870766757056 18.113612842559814 0.32300609052181245 9.196394729614259 6301760 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000569243852805812 20.218243217468263 0.23529372960329056 9.377057743072509 6510330 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  8.621753077022731e-05 18.38272361755371 0.17040156871080397 9.101021003723144 6720934 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007621613549417816 20.14881000518799 0.15546550005674362 9.176316833496093 6929076 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007695142570810276 21.947324752807617 0.1263531558215618 9.685079383850098 7139351 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00033315313448838423 15.486666011810303 0.09876827225089073 10.381051445007325 7351158 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009127405792241916 20.891300201416016 0.12405979186296463 9.665226268768311 7564749 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005766657501226291 22.895076942443847 0.10659956187009811 9.80807867050171 7777156 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005214392964262516 17.15170736312866 0.11028266996145249 10.169902992248534 7990019 0


Pure best response payoff estimated to be 73.02 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (9, 0)
Current player 0 and current strategies (9, 0) took 58.02 seconds to finish estimate with resulting utilities: [111.28   1.8 ]
Estimating current strategies:  (9, 1)
Current player 0 and current strategies (9, 1) took 110.43 seconds to finish estimate with resulting utilities: [100.955  37.88 ]
Estimating current strategies:  (9, 2)
Current player 0 and current strategies (9, 2) took 121.98 seconds to finish estimate with resulting utilities: [60.97  67.215]
Estimating current strategies:  (9, 3)
Current player 0 and current strategies (9, 3) took 121.63 seconds to finish estimate with resulting utilities: [56.44  70.325]
Estimating current strategies:  (9, 4)
Current player 0 and current strategies (9, 4) took 123.95 seconds to finish estimate with resulting utilities: [61.485 74.185]
Estimating current strategies:  (9, 5)
Current player 0 and current strategies (9, 5) took 120.44 seconds to finish estimate with resulting utilities: [58.205 69.11 ]
Estimating current strategies:  (9, 6)
Current player 0 and current strategies (9, 6) took 119.47 seconds to finish estimate with resulting utilities: [59.325 67.995]
Estimating current strategies:  (9, 7)
Current player 0 and current strategies (9, 7) took 119.9 seconds to finish estimate with resulting utilities: [57.6   65.595]
Estimating current strategies:  (9, 8)
Current player 0 and current strategies (9, 8) took 124.87 seconds to finish estimate with resulting utilities: [57.81 71.6 ]
Estimating current strategies:  (9, 9)
Current player 0 and current strategies (9, 9) took 109.52 seconds to finish estimate with resulting utilities: [54.95 54.17]
Computing meta_strategies
Exited RRD with total regret 2.1308959195851003 that was less than regret lambda 2.1428571428571432 after 330 iterations 
REGRET STEPS:  15
NEW LAMBDA 1.785714285714286
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    48.59           4.58           2.65           2.70           2.88           2.39           2.24           2.25           2.73           1.80      
    1    191.50          98.82          49.25          47.69          49.48          47.71          48.68          47.98          48.98          37.88      
    2    180.81          134.97          31.10          30.57          32.88          30.09          33.72          28.29          31.73          67.22      
    3    179.07          134.53          30.24          32.23          32.11          32.00          33.56          32.72          28.79          70.33      
    4    180.97          134.76          29.91          30.76          31.41          33.45          29.42          29.25          30.19          74.19      
    5    177.69          136.41          31.98          32.47          33.37          30.04          35.86          33.51          30.86          69.11      
    6    178.94          136.80          33.05          34.52          29.30          35.47          31.78          31.07          30.91          68.00      
    7    179.03          135.31          28.15          30.43          31.16          34.05          33.53          32.61          32.62          65.59      
    8    180.73          134.56          30.63          31.21          30.39          32.12          31.93          32.88          31.54          71.60      
    9    111.28          100.95          60.97          56.44          61.48          58.20          59.33          57.60          57.81          54.56      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    48.59          191.50          180.81          179.07          180.97          177.69          178.94          179.03          180.73          111.28      
    1     4.58          98.82          134.97          134.53          134.76          136.41          136.80          135.31          134.56          100.95      
    2     2.65          49.25          31.10          30.24          29.91          31.98          33.05          28.15          30.63          60.97      
    3     2.70          47.69          30.57          32.23          30.76          32.47          34.52          30.43          31.21          56.44      
    4     2.88          49.48          32.88          32.11          31.41          33.37          29.30          31.16          30.39          61.48      
    5     2.39          47.71          30.09          32.00          33.45          30.04          35.47          34.05          32.12          58.20      
    6     2.24          48.68          33.72          33.56          29.42          35.86          31.78          33.53          31.93          59.33      
    7     2.25          47.98          28.29          32.72          29.25          33.51          31.07          32.61          32.88          57.60      
    8     2.73          48.98          31.73          28.79          30.19          30.86          30.91          32.62          31.54          57.81      
    9     1.80          37.88          67.22          70.33          74.19          69.11          68.00          65.59          71.60          54.56      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    97.17          196.09          183.47          181.77          183.85          180.08          181.19          181.28          183.46          113.08      
    1    196.09          197.64          184.22          182.22          184.24          184.12          185.48          183.29          183.53          138.84      
    2    183.47          184.22          62.20          60.81          62.79          62.06          66.77          56.44          62.36          128.19      
    3    181.77          182.22          60.81          64.46          62.87          64.47          68.08          63.15          60.00          126.77      
    4    183.85          184.24          62.79          62.87          62.83          66.81          58.73          60.41          60.59          135.67      
    5    180.08          184.12          62.06          64.47          66.81          60.08          71.33          67.56          62.97          127.31      
    6    181.19          185.48          66.77          68.08          58.73          71.33          63.55          64.60          62.84          127.32      
    7    181.28          183.29          56.44          63.15          60.41          67.56          64.60          65.23          65.50          123.19      
    8    183.46          183.53          62.36          60.00          60.59          62.97          62.84          65.50          63.09          129.41      
    9    113.08          138.84          128.19          126.77          135.67          127.31          127.32          123.19          129.41          109.12      

 

Metagame probabilities: 
Player #0: 0.0001  0.0128  0.0406  0.0625  0.0837  0.0665  0.0562  0.0376  0.0711  0.569  
Player #1: 0.0001  0.0128  0.0406  0.0625  0.0837  0.0665  0.0562  0.0376  0.0711  0.569  
Iteration : 9
Time so far: 77226.73735952377
Approximating Best Response
Training best response:  True 0.05500000000000001
rtg values check:  (?,) (?, 1)
2023-07-17 17:13:23.560692: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_69/weights_2/Adam_1/Assign' id:19647 op device:{requested: '', assigned: ''} def:{{{node mlp_69/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_69/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_69/weights_2/Adam_1, mlp_69/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02053935807198286 35.34099178314209 0.4208197697997093 9.0993408203125 10224 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03992307223379612 16.261710357666015 0.8102057099342346 7.009795188903809 221998 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0391428591683507 11.294983196258546 0.8518553912639618 6.63987627029419 433245 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034291820228099824 14.643651294708253 0.7859010815620422 6.9856010437011715 644454 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03538808971643448 15.278687763214112 0.8679012596607208 6.314066171646118 856359 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027652044594287873 25.031279182434083 0.7382136404514312 6.953068447113037 1066529 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029542830400168897 15.857719135284423 0.8819823741912842 6.61305193901062 1275899 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029831978119909765 13.915964698791504 0.8745830655097961 6.276542615890503 1486787 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026976910419762135 12.304483413696289 0.8821204543113709 6.21096830368042 1696672 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021636436507105828 11.896423816680908 0.8202700793743134 6.582808208465576 1905802 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018781131692230703 15.582323169708252 0.7823425590991974 7.0423903465271 2116031 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020128404907882213 13.305770206451417 0.9127650141716004 6.198690605163574 2324184 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015350013878196478 17.956537437438964 0.7906820952892304 6.8466438293457035 2534423 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014301108662039042 16.993061828613282 0.8126854538917542 6.347893619537354 2742620 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010830087959766388 15.545300483703613 0.7949908554553986 6.516715574264526 2953913 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009057160187512635 16.58218584060669 0.7783883154392243 6.703867340087891 3162264 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006397955724969507 19.215831756591797 0.7868519365787506 6.880558586120605 3374237 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004236181173473597 22.156698608398436 0.6399666786193847 7.507364892959595 3582477 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017434115230571478 24.19682331085205 0.500235503911972 8.235346269607543 3793982 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00029202673977124505 17.973466682434083 0.6488356947898865 7.492369890213013 4002720 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013323279417818412 15.284388637542724 0.6232069492340088 7.195238828659058 4211109 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003620115647208877 14.620366859436036 0.5238963693380356 8.406514596939086 4419483 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  9.897140407701954e-05 20.50759353637695 0.32411983907222747 8.824269008636474 4629422 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005648636724799871 15.238188362121582 0.428643473982811 8.086111354827882 4839655 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002773378000711091 18.117125511169434 0.36235890090465545 9.140214920043945 5050769 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0001933737934450619 14.131398105621338 0.3288384109735489 8.594190120697021 5259825 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010679702798370271 17.786597347259523 0.42597653567790983 9.074745178222656 5469550 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005326496862835484 17.40838203430176 0.37764954566955566 8.784463024139404 5680765 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007494327175663785 17.444242095947267 0.3393825262784958 9.181487655639648 5892269 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004900028867268702 18.824991989135743 0.26946801543235777 9.72316541671753 6103747 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005144431255757808 19.901529693603514 0.23823682516813277 10.467645072937012 6315628 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00010950919986498774 20.129987144470213 0.26967418342828753 10.41326208114624 6527557 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00044992251132498496 19.76625919342041 0.38005450963973997 10.384850215911865 6736895 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008834704916807823 19.516218280792238 0.3081066370010376 10.756181716918945 6948535 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006621814151003491 18.344882678985595 0.16020628064870834 11.213141536712646 7158719 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005136633260917733 31.10990676879883 0.12274853959679603 11.668693161010742 7367930 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0024871171417544247 23.420043182373046 0.12384703382849693 11.154063701629639 7574850 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  3.093336054007523e-05 16.578330707550048 0.09698568806052207 11.529080581665038 7783291 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00046201794393709863 29.754974174499512 0.11556513383984565 12.404823493957519 7994785 0


Pure best response payoff estimated to be 72.38 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (10, 0)
Current player 0 and current strategies (10, 0) took 64.33 seconds to finish estimate with resulting utilities: [117.38    2.405]
Estimating current strategies:  (10, 1)
Current player 0 and current strategies (10, 1) took 110.43 seconds to finish estimate with resulting utilities: [100.615  35.815]
Estimating current strategies:  (10, 2)
Current player 0 and current strategies (10, 2) took 127.3 seconds to finish estimate with resulting utilities: [51.525 72.595]
Estimating current strategies:  (10, 3)
Current player 0 and current strategies (10, 3) took 124.69 seconds to finish estimate with resulting utilities: [50.055 69.62 ]
Estimating current strategies:  (10, 4)
Current player 0 and current strategies (10, 4) took 123.52 seconds to finish estimate with resulting utilities: [53.295 66.695]
Estimating current strategies:  (10, 5)
Current player 0 and current strategies (10, 5) took 128.79 seconds to finish estimate with resulting utilities: [52.215 68.415]
Estimating current strategies:  (10, 6)
Current player 0 and current strategies (10, 6) took 122.6 seconds to finish estimate with resulting utilities: [49.935 68.615]
Estimating current strategies:  (10, 7)
Current player 0 and current strategies (10, 7) took 125.58 seconds to finish estimate with resulting utilities: [50.75 68.9 ]
Estimating current strategies:  (10, 8)
Current player 0 and current strategies (10, 8) took 126.5 seconds to finish estimate with resulting utilities: [51.035 69.205]
Estimating current strategies:  (10, 9)
Current player 0 and current strategies (10, 9) took 111.58 seconds to finish estimate with resulting utilities: [75.175 51.91 ]
Estimating current strategies:  (10, 10)
Current player 0 and current strategies (10, 10) took 118.51 seconds to finish estimate with resulting utilities: [60.91  61.885]
Computing meta_strategies
Exited RRD with total regret 1.7853212313020919 that was less than regret lambda 1.785714285714286 after 1435 iterations 
REGRET STEPS:  15
NEW LAMBDA 1.4285714285714288
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0    48.59           4.58           2.65           2.70           2.88           2.39           2.24           2.25           2.73           1.80           2.40      
    1    191.50          98.82          49.25          47.69          49.48          47.71          48.68          47.98          48.98          37.88          35.81      
    2    180.81          134.97          31.10          30.57          32.88          30.09          33.72          28.29          31.73          67.22          72.59      
    3    179.07          134.53          30.24          32.23          32.11          32.00          33.56          32.72          28.79          70.33          69.62      
    4    180.97          134.76          29.91          30.76          31.41          33.45          29.42          29.25          30.19          74.19          66.69      
    5    177.69          136.41          31.98          32.47          33.37          30.04          35.86          33.51          30.86          69.11          68.42      
    6    178.94          136.80          33.05          34.52          29.30          35.47          31.78          31.07          30.91          68.00          68.61      
    7    179.03          135.31          28.15          30.43          31.16          34.05          33.53          32.61          32.62          65.59          68.90      
    8    180.73          134.56          30.63          31.21          30.39          32.12          31.93          32.88          31.54          71.60          69.20      
    9    111.28          100.95          60.97          56.44          61.48          58.20          59.33          57.60          57.81          54.56          51.91      
   10    117.38          100.61          51.52          50.05          53.30          52.22          49.94          50.75          51.03          75.17          61.40      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0    48.59          191.50          180.81          179.07          180.97          177.69          178.94          179.03          180.73          111.28          117.38      
    1     4.58          98.82          134.97          134.53          134.76          136.41          136.80          135.31          134.56          100.95          100.61      
    2     2.65          49.25          31.10          30.24          29.91          31.98          33.05          28.15          30.63          60.97          51.52      
    3     2.70          47.69          30.57          32.23          30.76          32.47          34.52          30.43          31.21          56.44          50.05      
    4     2.88          49.48          32.88          32.11          31.41          33.37          29.30          31.16          30.39          61.48          53.30      
    5     2.39          47.71          30.09          32.00          33.45          30.04          35.47          34.05          32.12          58.20          52.22      
    6     2.24          48.68          33.72          33.56          29.42          35.86          31.78          33.53          31.93          59.33          49.94      
    7     2.25          47.98          28.29          32.72          29.25          33.51          31.07          32.61          32.88          57.60          50.75      
    8     2.73          48.98          31.73          28.79          30.19          30.86          30.91          32.62          31.54          57.81          51.03      
    9     1.80          37.88          67.22          70.33          74.19          69.11          68.00          65.59          71.60          54.56          75.17      
   10     2.40          35.81          72.59          69.62          66.69          68.42          68.61          68.90          69.20          51.91          61.40      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0    97.17          196.09          183.47          181.77          183.85          180.08          181.19          181.28          183.46          113.08          119.78      
    1    196.09          197.64          184.22          182.22          184.24          184.12          185.48          183.29          183.53          138.84          136.43      
    2    183.47          184.22          62.20          60.81          62.79          62.06          66.77          56.44          62.36          128.19          124.12      
    3    181.77          182.22          60.81          64.46          62.87          64.47          68.08          63.15          60.00          126.77          119.68      
    4    183.85          184.24          62.79          62.87          62.83          66.81          58.73          60.41          60.59          135.67          119.99      
    5    180.08          184.12          62.06          64.47          66.81          60.08          71.33          67.56          62.97          127.31          120.63      
    6    181.19          185.48          66.77          68.08          58.73          71.33          63.55          64.60          62.84          127.32          118.55      
    7    181.28          183.29          56.44          63.15          60.41          67.56          64.60          65.23          65.50          123.19          119.65      
    8    183.46          183.53          62.36          60.00          60.59          62.97          62.84          65.50          63.09          129.41          120.24      
    9    113.08          138.84          128.19          126.77          135.67          127.31          127.32          123.19          129.41          109.12          127.08      
   10    119.78          136.43          124.12          119.68          119.99          120.63          118.55          119.65          120.24          127.08          122.79      

 

Metagame probabilities: 
Player #0: 0.0001  0.0001  0.2256  0.0289  0.0025  0.0165  0.0197  0.0085  0.0244  0.0014  0.6721  
Player #1: 0.0001  0.0001  0.2256  0.0289  0.0025  0.0165  0.0197  0.0085  0.0244  0.0014  0.6721  
Iteration : 10
Time so far: 86943.14267611504
Approximating Best Response
Training best response:  True 0.05
rtg values check:  (?,) (?, 1)
2023-07-17 19:55:20.457090: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_76/weights_2/Adam_1/Assign' id:21625 op device:{requested: '', assigned: ''} def:{{{node mlp_76/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_76/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_76/weights_2/Adam_1, mlp_76/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019131178315728904 47.31875724792481 0.37594995200634 11.129369640350342 10895 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028940366022288798 20.009086418151856 0.6172028303146362 9.66708812713623 222047 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034219628944993016 15.196119785308838 0.7352148652076721 9.11072587966919 432628 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02837557215243578 13.845411586761475 0.6875521838665009 9.558238220214843 641047 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027413246780633928 20.61134624481201 0.6755087494850158 9.665544033050537 851043 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02243884839117527 31.66204128265381 0.5961144030094147 9.439642906188965 1060103 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022797050513327123 23.94237289428711 0.6727865695953369 9.136215877532958 1270063 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02170505952090025 14.3930495262146 0.6658923923969269 8.78157606124878 1478648 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018876973539590836 22.471477890014647 0.6558857858181 9.385617542266846 1688877 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01973246317356825 12.331251335144042 0.689991933107376 8.730965900421143 1896293 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015799872018396853 16.98685779571533 0.6322342336177826 9.641653728485107 2106172 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014814313407987357 22.561982917785645 0.6925289630889893 9.0976806640625 2314963 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01529177213087678 14.486880683898926 0.797689950466156 8.392129278182983 2524274 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011428129673004151 16.772667694091798 0.6903882205486298 9.030920314788819 2735231 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0075297804549336435 17.992697525024415 0.48263923823833466 10.021079921722412 2944053 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007290951907634735 13.94402904510498 0.6723586678504944 8.965473842620849 3153909 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004524423507973552 25.254750633239745 0.47755190134048464 10.09328031539917 3364844 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004142371518537402 12.454290199279786 0.6420715868473053 9.026919174194337 3571886 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013440520328003914 23.652802658081054 0.4554719507694244 10.215293598175048 3781657 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013658086754730903 22.920713233947755 0.47369795441627505 10.41106357574463 3992679 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000549842623877339 13.41634397506714 0.38872108459472654 10.996159172058105 4201683 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010008673283664394 21.533287239074706 0.4355496853590012 10.516199588775635 4410769 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010660757252480835 15.898189735412597 0.35939126908779145 10.454504680633544 4621260 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008488545754516963 21.02262897491455 0.2510703235864639 11.023974418640137 4831743 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003153549987473525 13.345212841033936 0.1341370865702629 11.670843982696534 5041362 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005954054591711611 15.267245006561279 0.06659017130732536 12.578570461273193 5252558 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00047625976367271505 18.551160049438476 0.10293548777699471 11.935840320587157 5463348 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00030121669169602683 20.19893608093262 0.06181524507701397 12.79537582397461 5673238 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005626307174679823 14.095688438415527 0.036351284198462966 12.368147659301759 5883619 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005782031599665061 19.428931427001952 0.031148004345595837 12.963528919219971 6092965 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000506177721399581 21.917782974243163 0.03422559555619955 12.616290855407716 6301643 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00017013115784720868 23.93555030822754 0.028929652646183968 13.087842178344726 6511123 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003965860506286845 26.374614334106447 0.047455941513180736 13.365745353698731 6720004 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.000156396583042806 14.136502742767334 0.022057898715138434 13.839748859405518 6931622 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004503278118136222 18.007868576049805 0.028208483569324018 13.902610111236573 7140883 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007080879673594609 21.512453269958495 0.020651791244745255 13.977725410461426 7349015 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000598048503161408 19.315899085998534 0.030458175763487815 14.28304443359375 7556548 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008905485505238175 17.547755432128906 0.027777862921357154 14.209787178039551 7770492 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003768647755350685 16.567834758758544 0.02947406191378832 14.238672065734864 7977558 0


Pure best response payoff estimated to be 65.52 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (11, 0)
Current player 0 and current strategies (11, 0) took 49.43 seconds to finish estimate with resulting utilities: [90.25   1.435]
Estimating current strategies:  (11, 1)
Current player 0 and current strategies (11, 1) took 90.71 seconds to finish estimate with resulting utilities: [79.555 29.955]
Estimating current strategies:  (11, 2)
Current player 0 and current strategies (11, 2) took 122.17 seconds to finish estimate with resulting utilities: [44.215 44.78 ]
Estimating current strategies:  (11, 3)
Current player 0 and current strategies (11, 3) took 121.06 seconds to finish estimate with resulting utilities: [44.165 43.275]
Estimating current strategies:  (11, 4)
Current player 0 and current strategies (11, 4) took 123.74 seconds to finish estimate with resulting utilities: [42.01 42.45]
Estimating current strategies:  (11, 5)
Current player 0 and current strategies (11, 5) took 121.81 seconds to finish estimate with resulting utilities: [43.06 41.66]
Estimating current strategies:  (11, 6)
Current player 0 and current strategies (11, 6) took 121.65 seconds to finish estimate with resulting utilities: [41.205 39.46 ]
Estimating current strategies:  (11, 7)
Current player 0 and current strategies (11, 7) took 122.11 seconds to finish estimate with resulting utilities: [44.34  47.015]
Estimating current strategies:  (11, 8)
Current player 0 and current strategies (11, 8) took 115.54 seconds to finish estimate with resulting utilities: [41.43  45.525]
Estimating current strategies:  (11, 9)
Current player 0 and current strategies (11, 9) took 99.95 seconds to finish estimate with resulting utilities: [52.545 39.46 ]
Estimating current strategies:  (11, 10)
OpenSpiel exception: /home/anrigu/srg_research/open_spiel/open_spiel/games/harvest.cc:242 CHECK_TRUE(grid_[loc.first][loc.second] == kempty)
Traceback (most recent call last):
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 485, in <module>
    app.run(main)
  File "/home/anrigu/.local/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/anrigu/.local/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 482, in main
    gpsro_looper(env, oracle, agents)
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 403, in gpsro_looper
    g_psro_solver.iteration()
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 203, in iteration
    self.update_empirical_gamestate(seed=seed)  # Update gamestate matrix.
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/psro_v2.py", line 510, in update_empirical_gamestate
    utility_estimates, trajectories, action_trajectories, all_returns = self.sample_episodes(estimated_policies,
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 240, in sample_episodes
    rets, _, _ = sample_episode(self._game.new_initial_state(),
pyspiel.SpielError: /home/anrigu/srg_research/open_spiel/open_spiel/games/harvest.cc:242 CHECK_TRUE(grid_[loc.first][loc.second] == kempty)
