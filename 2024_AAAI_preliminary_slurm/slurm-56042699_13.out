Job Id listed below:
56042727

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

2023-07-18 13:25:32.330980: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 13:25:33.319275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0718 13:25:35.000079 22975916325760 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x14e536156d40>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x14e536156d40>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-07-18 13:25:35.320613: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-07-18 13:25:35.602071: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 19.87 seconds to finish estimate with resulting utilities: [50.56  49.545]
Exited RRD with total regret 0.0 that was less than regret lambda 5.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    50.05      

 

Player 1 Payoff matrix: 

           0      
    0    50.05      

 

Social Welfare Sum Matrix: 

           0      
    0    100.11      

 

Iteration : 0
Time so far: 0.00019073486328125
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-07-18 13:25:56.485928: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.12168851941823959 26.40475082397461 2.0684993267059326 0.0008535214728908613 10556 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09798632934689522 15.010655784606934 1.8807043075561523 0.21476288437843322 217624 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09222995936870575 13.738638591766357 1.8266358375549316 0.3274244010448456 419520 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08490479290485382 14.697171783447265 1.8168969631195069 0.395502433180809 621822 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07970619946718216 15.042162132263183 1.7833744049072267 0.4865875542163849 824124 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07693430781364441 16.165279960632326 1.7756318688392638 0.5813432276248932 1026804 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06799741461873055 16.242972660064698 1.7369054079055786 0.6285842061042786 1229107 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06036000773310661 15.715854930877686 1.7014310479164123 0.7638908684253692 1433613 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05954837203025818 17.22781229019165 1.6786107182502747 0.8607563138008117 1637401 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.047118204459547994 18.044522190093993 1.5913714647293091 1.0920051693916322 1842498 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03914870321750641 19.38845224380493 1.5155560731887818 1.1409554123878478 2049149 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03306454345583916 22.972858428955078 1.4424165606498718 1.3341265320777893 2258174 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028372285328805445 21.478181076049804 1.4260942935943604 1.3747917771339417 2467643 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02347915694117546 16.802272033691406 1.3744081616401673 1.5149412870407104 2679209 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01905305599793792 19.61811943054199 1.2948009967803955 1.8047236800193787 2887113 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014312037080526353 23.150619506835938 1.2214966058731078 2.026078164577484 3099123 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011104226391762496 22.425677680969237 1.1281317949295044 2.297516107559204 3309319 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0071846432983875275 21.767107582092287 1.0786696672439575 2.5358553647994997 3520160 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005396409030072391 21.404764556884764 1.0162882328033447 2.7639328718185423 3733674 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002146499784430489 21.20704574584961 0.9539408206939697 2.940684771537781 3945719 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013471769343595952 24.112191009521485 0.9237307071685791 3.195455551147461 4156338 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001945546580827795 21.216045951843263 0.8139951646327972 3.6240137338638307 4374210 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0027290829399134963 22.409400177001952 0.7292858779430389 3.7849281787872315 4588584 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001331706886412576 22.4720100402832 0.6670488476753235 4.395722484588623 4804330 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021007301955251025 31.40541801452637 0.6071649849414825 4.810734605789184 5019797 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008297015374409966 23.13570957183838 0.6197222948074341 4.907496595382691 5235570 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010184144892264157 23.759282493591307 0.5918397605419159 5.048623895645141 5451096 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010914308950304985 24.787212944030763 0.5566758334636688 5.272549343109131 5666854 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001295649075473193 28.328359031677245 0.5195866495370864 5.555216646194458 5886258 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0023942023632116615 23.129970359802247 0.5005711555480957 6.2468057632446286 6104935 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00012588192876137328 21.906635665893553 0.45680427849292754 6.757648849487305 6322853 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016200082958675922 21.10146827697754 0.43023535907268523 7.070128726959228 6541075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010956592093862127 26.281174087524413 0.4376706093549728 7.060391092300415 6759410 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013954108726466075 24.561069869995116 0.42355875968933104 7.271265363693237 6977495 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001512649421056267 25.734978675842285 0.4209659039974213 7.496031761169434 7196011 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002179944422096014 24.49732551574707 0.3594271630048752 7.658968544006347 7411045 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00123635615746025 24.81671142578125 0.325653538107872 7.833596420288086 7629274 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003516702898195945 22.57057418823242 0.3403711050748825 7.790737342834473 7847894 0


Pure best response payoff estimated to be 193.13 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 85.11 seconds to finish estimate with resulting utilities: [191.225   4.245]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 139.7 seconds to finish estimate with resulting utilities: [96.365 95.455]
Computing meta_strategies
Exited RRD with total regret 4.763542868016344 that was less than regret lambda 5.0 after 34 iterations 
REGRET STEPS:  20
NEW LAMBDA 4.7368421052631575
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    50.05           4.25      
    1    191.22          95.91      

 

Player 1 Payoff matrix: 

           0              1      
    0    50.05          191.22      
    1     4.25          95.91      

 

Social Welfare Sum Matrix: 

           0              1      
    0    100.11          195.47      
    1    195.47          191.82      

 

Metagame probabilities: 
Player #0: 0.0256  0.9744  
Player #1: 0.0256  0.9744  
Iteration : 1
Time so far: 6817.209115028381
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-07-18 15:19:34.133707: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01686038915067911 74.22980041503907 0.3310197815299034 9.716502952575684 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03677832931280136 17.18904314041138 0.7493281900882721 6.958598566055298 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03479913827031851 18.969964218139648 0.7710619568824768 5.934853982925415 449279 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.039479165151715276 17.068657302856444 0.8950088739395141 5.371893405914307 666077 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0360107347369194 19.324499320983886 0.8979673862457276 5.2606555938720705 880517 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03365583010017872 22.841297912597657 0.8736181437969208 5.646736669540405 1095738 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03225778341293335 14.299778175354003 0.925388640165329 5.124854660034179 1311748 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03158257119357586 16.90117998123169 1.0298747897148133 4.533262491226196 1523006 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02947460226714611 29.93910369873047 0.9920219898223877 4.476890707015992 1735996 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02583379689604044 19.0409215927124 1.0126308143138885 4.679388475418091 1949589 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020929525420069695 15.986327838897704 0.9005493104457856 4.806357097625733 2162685 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02093861550092697 15.479081058502198 0.997020810842514 4.648909330368042 2377772 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01847549332305789 17.29696731567383 0.9565672636032104 4.536005115509033 2590590 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014045386295765638 17.705908012390136 0.8983691930770874 4.884329986572266 2804959 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01290320698171854 16.946944332122804 0.9102485954761506 4.748116064071655 3021331 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008193862764164806 16.72402982711792 0.8484255969524384 4.833813619613648 3237304 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006583993788808584 18.054687404632567 0.832525885105133 5.145058870315552 3452714 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004146747337654233 21.072653388977052 0.8145975172519684 5.2917084217071535 3671543 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0033672059536911545 19.81960573196411 0.8009775400161743 5.718540191650391 3887630 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005920867435634137 24.150805473327637 0.6894915461540222 5.946336650848389 4104824 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010458752803970128 22.196443557739258 0.6913745760917663 5.741421365737915 4323610 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007297003587154905 18.389697456359862 0.6248704671859742 5.632526063919068 4541916 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017924055428011343 19.79742250442505 0.5146177619695663 6.159853982925415 4761344 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010087207425385713 20.87681179046631 0.5415518194437027 6.319968175888062 4979290 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002657235589140328 25.943161010742188 0.5040958195924758 6.916053724288941 5198591 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008598703127063346 24.266708183288575 0.520388388633728 6.73138165473938 5416244 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000300593539577676 24.054968070983886 0.551241111755371 6.647045373916626 5635896 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004037140679429285 22.777926445007324 0.4479882836341858 7.056792974472046 5855086 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  3.78572754925699e-05 28.660841369628905 0.4337725341320038 6.961823463439941 6074353 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010381872998550534 24.539192771911623 0.45010256469249726 7.245187664031983 6294083 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001058869627013337 20.268607139587402 0.4288079380989075 7.270638132095337 6512223 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013513880840037018 24.000256156921388 0.4612819582223892 7.325666379928589 6729915 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010644521826179698 21.04014472961426 0.36974747478961945 8.038020706176757 6947109 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  6.218172056833282e-05 29.84897651672363 0.3583686649799347 7.5124907970428465 7167029 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00012430351052898913 18.50910167694092 0.3542143225669861 7.987228298187256 7386280 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00016523275662621016 23.84530372619629 0.34169278740882875 8.139538288116455 7605693 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014716827368829398 17.491649436950684 0.40339353680610657 7.904755687713623 7825693 0


Pure best response payoff estimated to be 136.425 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 79.82 seconds to finish estimate with resulting utilities: [179.47    3.015]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 132.53 seconds to finish estimate with resulting utilities: [135.3    43.725]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 134.17 seconds to finish estimate with resulting utilities: [18.725 18.89 ]
Computing meta_strategies
Exited RRD with total regret 4.61965023506454 that was less than regret lambda 4.7368421052631575 after 51 iterations 
REGRET STEPS:  20
NEW LAMBDA 4.473684210526315
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    50.05           4.25           3.02      
    1    191.22          95.91          43.73      
    2    179.47          135.30          18.81      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    50.05          191.22          179.47      
    1     4.25          95.91          135.30      
    2     3.02          43.73          18.81      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    100.11          195.47          182.48      
    1    195.47          191.82          179.03      
    2    182.48          179.03          37.62      

 

Metagame probabilities: 
Player #0: 0.0093  0.4444  0.5464  
Player #1: 0.0093  0.4444  0.5464  
Iteration : 2
Time so far: 15519.922978401184
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-07-18 17:44:36.518221: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018474491033703088 79.84397354125977 0.36563487350940704 9.71806755065918 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03690749797970057 23.42409896850586 0.7856729745864868 6.562195920944214 226336 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03720736689865589 22.615680694580078 0.8234859049320221 6.437413692474365 440280 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034493419155478475 21.58125705718994 0.7769835889339447 6.296739387512207 656210 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02971234228461981 25.270268440246582 0.7299658119678497 6.235485219955445 874607 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029508870281279086 23.32849464416504 0.7888913571834564 5.678022813796997 1090738 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030307097360491753 23.823558807373047 0.8768731355667114 5.2848835468292235 1303200 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0269315155223012 21.7246732711792 0.8577815175056458 5.700813961029053 1513749 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022823935374617577 24.78094310760498 0.7684321403503418 5.987099647521973 1722771 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023061869479715825 26.399064445495604 0.8110337257385254 6.304841470718384 1931862 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024961124919354916 14.855288314819337 1.006977927684784 4.668713235855103 2141283 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01701769568026066 19.72090349197388 0.8257628560066224 5.3201171398162845 2349176 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017169025912880897 10.983483123779298 0.8727921187877655 5.404956293106079 2554201 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012525359075516463 16.671281719207762 0.7642307937145233 6.151816415786743 2760219 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012253592815250158 11.016477966308594 0.8061849415302277 5.9593933582305905 2966076 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010003726510331035 10.885009860992431 0.7707815706729889 5.962230253219604 3169567 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006930829701013863 10.515841674804687 0.797307950258255 6.4259542465209964 3375128 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00485958915669471 9.424487209320068 0.8124209225177765 6.2043397426605225 3580092 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003945582662709057 12.873153591156006 0.8775201618671418 5.943643522262573 3783773 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001400630445277784 6.539666414260864 0.7873011291027069 6.629731369018555 3989624 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003149428837787127 10.279095029830932 0.6169596910476685 7.2251382827758786 4193970 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009219184015819337 11.85973663330078 0.6099970281124115 6.904765462875366 4396877 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00011258077138336376 13.75692138671875 0.36702582240104675 8.033961963653564 4602855 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000545815299210517 7.288055181503296 0.3541906803846359 7.230518579483032 4806403 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007273186929523945 5.917900514602661 0.41513762474060056 7.656033945083618 5009163 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0003645762801170349 10.319914245605469 0.41043487191200256 8.593248748779297 5212780 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.000583838639431633 10.840935802459716 0.22376251220703125 9.921601486206054 5417115 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001056799214484272 6.6902917385101315 0.3451653212308884 8.625390338897706 5621589 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0008634586236439645 9.100113534927369 0.37728374302387235 8.799266815185547 5825255 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004095383148523979 7.509042263031006 0.32391427755355834 9.581095123291016 6028917 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  4.691971607826417e-05 8.230048990249633 0.34969484210014345 9.362521362304687 6232884 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0028612210342544133 6.26823582649231 0.30571592748165133 9.690533924102784 6437110 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -6.602638895856217e-05 15.762381839752198 0.25590228736400605 10.433840084075928 6640967 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013472949727656668 10.215616607666016 0.3112536668777466 9.73569622039795 6845542 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014140344399493187 8.409262943267823 0.33371893763542176 9.93954963684082 7050582 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00034404483158141376 7.644239616394043 0.33528572916984556 9.512506198883056 7255214 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -6.780499206797686e-05 11.426755428314209 0.23122138828039168 10.071037864685058 7458651 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001096257218887331 12.134883403778076 0.08233236111700534 10.974337482452393 7661646 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -8.121793944155797e-05 11.007280349731445 0.12959715276956557 10.33104486465454 7865749 0
Recovering previous policy with expected return of 76.2636815920398. Long term value was 16.7 and short term was 16.655.


Pure best response payoff estimated to be 88.54 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 82.0 seconds to finish estimate with resulting utilities: [181.77   2.83]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 133.57 seconds to finish estimate with resulting utilities: [134.06   45.795]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 135.77 seconds to finish estimate with resulting utilities: [18.865 19.54 ]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 135.35 seconds to finish estimate with resulting utilities: [20.025 17.19 ]
Computing meta_strategies
Exited RRD with total regret 4.437207357551117 that was less than regret lambda 4.473684210526315 after 45 iterations 
REGRET STEPS:  20
NEW LAMBDA 4.210526315789473
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0    50.05           4.25           3.02           2.83      
    1    191.22          95.91          43.73          45.80      
    2    179.47          135.30          18.81          19.54      
    3    181.77          134.06          18.86          18.61      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0    50.05          191.22          179.47          181.77      
    1     4.25          95.91          135.30          134.06      
    2     3.02          43.73          18.81          18.86      
    3     2.83          45.80          19.54          18.61      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0    100.11          195.47          182.48          184.60      
    1    195.47          191.82          179.03          179.86      
    2    182.48          179.03          37.62          38.41      
    3    184.60          179.86          38.41          37.22      

 

Metagame probabilities: 
Player #0: 0.0156  0.3644  0.3132  0.3067  
Player #1: 0.0156  0.3644  0.3132  0.3067  
Iteration : 3
Time so far: 24053.915066242218
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-07-18 20:06:50.645229: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7779 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01244053542613983 56.99150695800781 0.2167817771434784 11.577881622314454 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030642551928758623 20.798628997802734 0.6433533549308776 7.927539920806884 230186 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023492289520800114 20.924757194519042 0.5286057502031326 8.862585926055909 446642 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03194536827504635 20.933968162536623 0.7422221779823304 6.765458297729492 665448 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03188469596207142 21.226333236694337 0.7943514108657836 6.049727296829223 881815 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027753850817680357 21.149303245544434 0.7601960837841034 6.317601013183594 1095469 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02614875640720129 21.967679977416992 0.7468775510787964 6.66524486541748 1309952 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02949705980718136 19.07523384094238 0.9029941082000732 5.908722162246704 1522856 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01903126761317253 21.441305732727052 0.6674505293369293 7.474784088134766 1736087 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02535272128880024 22.15266227722168 0.9192051768302918 5.3997802734375 1946942 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030713628232479095 15.42413396835327 0.8233212947845459 6.554490375518799 2158969 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021975594386458398 24.673126220703125 0.8093085408210754 5.931950998306275 2371761 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01683652577921748 18.92917137145996 0.8649864912033081 5.563372182846069 2581294 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010296218981966376 16.01823196411133 0.5850776314735413 7.998075342178344 2796637 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012087277602404356 22.259309196472167 0.8809307634830474 5.164069700241089 3008380 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011970356106758118 19.80680408477783 0.8111444354057312 5.858682346343994 3219105 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00983967836946249 16.271440982818604 0.7853251755237579 6.117522954940796 3433838 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005240643606521189 23.284863662719726 0.862331235408783 4.946842050552368 3644873 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011504373280331492 15.900119304656982 0.8584398031234741 5.678304004669189 3856861 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00291148682590574 20.14817523956299 0.8097179174423218 5.230726146697998 4065771 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004834392253542319 14.878883361816406 0.7524121522903442 5.768673372268677 4274001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017873685574159026 13.211758899688721 0.7924822628498077 5.823827743530273 4481481 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008123321272432805 13.668610286712646 0.6298395872116089 6.835508489608765 4688602 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00106345183448866 10.19139986038208 0.8280009210109711 5.203359746932984 4895607 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00560238187899813 11.423243522644043 0.7954041481018066 5.694739294052124 5102094 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00022136889747343959 16.92491397857666 0.620880526304245 6.7560498237609865 5309041 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002640930056804791 10.262614727020264 0.6434062719345093 6.687853622436523 5514898 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008215433161240071 13.128761577606202 0.7412043035030365 6.196779584884643 5721036 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002685084962286055 7.867710399627685 0.7462619364261627 6.11096158027649 5927498 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001413452043561847 14.923273372650147 0.7211202800273895 6.525479984283447 6131568 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001019399099459406 10.158616256713866 0.7019877314567566 6.532283449172974 6337714 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008321831095963716 9.370927429199218 0.7987211644649506 6.705064344406128 6544441 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007394463894888759 9.300782394409179 0.7797807157039642 6.752067041397095 6749916 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010934392696071882 10.16207571029663 0.7159795165061951 7.279767179489136 6956736 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021829704579431563 12.06921558380127 0.623606413602829 8.039262819290162 7164422 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00025854499835986644 8.457091903686523 0.6612769603729248 7.694304132461548 7371423 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007238021977536846 10.176453876495362 0.6612604796886444 7.840962266921997 7576598 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005172520807718683 7.330458307266236 0.6646883010864257 8.297506999969482 7782327 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007921472773887217 10.852984046936035 0.5435415506362915 8.357700300216674 7988179 0
Recovering previous policy with expected return of 65.9950248756219. Long term value was 23.741 and short term was 23.835.


Pure best response payoff estimated to be 79.925 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 82.3 seconds to finish estimate with resulting utilities: [181.095   2.595]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 135.38 seconds to finish estimate with resulting utilities: [134.515  44.78 ]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 136.76 seconds to finish estimate with resulting utilities: [17.84 19.22]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 136.69 seconds to finish estimate with resulting utilities: [18.355 18.785]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 136.04 seconds to finish estimate with resulting utilities: [16.77  16.395]
Computing meta_strategies
Exited RRD with total regret 4.203366653199396 that was less than regret lambda 4.210526315789473 after 82 iterations 
REGRET STEPS:  20
NEW LAMBDA 3.9473684210526305
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0    50.05           4.25           3.02           2.83           2.60      
    1    191.22          95.91          43.73          45.80          44.78      
    2    179.47          135.30          18.81          19.54          19.22      
    3    181.77          134.06          18.86          18.61          18.79      
    4    181.09          134.51          17.84          18.36          16.58      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0    50.05          191.22          179.47          181.77          181.09      
    1     4.25          95.91          135.30          134.06          134.51      
    2     3.02          43.73          18.81          18.86          17.84      
    3     2.83          45.80          19.54          18.61          18.36      
    4     2.60          44.78          19.22          18.79          16.58      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0    100.11          195.47          182.48          184.60          183.69      
    1    195.47          191.82          179.03          179.86          179.29      
    2    182.48          179.03          37.62          38.41          37.06      
    3    184.60          179.86          38.41          37.22          37.14      
    4    183.69          179.29          37.06          37.14          33.16      

 

Metagame probabilities: 
Player #0: 0.0019  0.3555  0.2246  0.2146  0.2033  
Player #1: 0.0019  0.3555  0.2246  0.2146  0.2033  
Iteration : 4
Time so far: 32933.681864500046
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-07-18 22:34:50.644511: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9757 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017987573333084584 76.47140731811524 0.28903587758541105 10.638915920257569 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.040682234615087506 13.95189266204834 0.8318738818168641 7.400447082519531 229616 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03603997267782688 21.203375816345215 0.7793226718902588 7.243317079544068 443912 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03228710629045963 20.795479011535644 0.7219452500343323 6.86260576248169 659893 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03709438927471638 15.153385162353516 0.8964120090007782 6.489961957931518 871420 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0311142161488533 15.14615077972412 0.8335738956928254 6.452546453475952 1080192 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03222975470125675 20.9537296295166 0.9059375405311585 6.250512933731079 1289046 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029766296967864037 18.541548919677734 0.87843456864357 5.979395389556885 1499149 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02866126913577318 16.730969142913818 0.9675093173980713 5.900159311294556 1708534 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026231757178902627 19.864974212646484 0.904662036895752 5.688180160522461 1915606 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021159956231713295 21.157511901855468 0.8662796139717102 6.171917390823364 2123085 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021388532407581807 15.924341106414795 0.93091299533844 5.882113742828369 2332185 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018199389241635798 13.634865188598633 0.9132837355136871 6.012171363830566 2538094 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020127793215215205 18.16065921783447 0.9136899650096894 5.5965948581695555 2747518 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016663037426769733 12.652190017700196 0.942723560333252 5.735461235046387 2955447 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013911877665668726 18.30952091217041 0.8198665499687194 6.5926354885101315 3166755 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013755130115896463 10.747907638549805 0.9949223458766937 5.724061393737793 3372837 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009788531810045242 8.826493263244629 0.8927652180194855 5.939315462112427 3579711 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006413759780116379 8.481011629104614 0.7521030962467193 7.203659915924073 3782934 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0030436628963798284 7.824258279800415 0.7060500860214234 7.269760465621948 3988439 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003350406552635832 8.833530378341674 0.7042325019836426 7.27412428855896 4193286 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005232933041406795 8.514132833480835 0.8525800764560699 6.5989936828613285 4397641 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001994486548937857 7.953247928619385 0.6679418444633484 7.6180338859558105 4603158 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005453483638120816 10.615953826904297 0.6753726720809936 7.786376571655273 4806442 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008367965197976446 9.720801162719727 0.6509738862514496 7.9030131816864015 5011522 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00035947090655099603 13.503975200653077 0.4971689432859421 8.597913265228271 5216750 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001647880848031491 8.640634202957154 0.4932209372520447 8.395980262756348 5421731 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003390420970390551 6.560197448730468 0.4722271621227264 9.141647624969483 5624141 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010895362705923618 6.032396078109741 0.4180609047412872 9.699504280090332 5827147 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005216064280830323 7.998565053939819 0.49521125555038453 8.896854877471924 6030649 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019446948357654038 11.595841407775879 0.3175184905529022 10.22506399154663 6233406 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  8.816725348879118e-05 10.850569534301759 0.19535958170890808 10.295874118804932 6437174 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011301244609057902 6.300834369659424 0.17599200308322907 11.03774585723877 6640422 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00028212847537361084 10.873080444335937 0.1692897155880928 11.299294185638427 6843810 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007941692339954898 14.614268779754639 0.09966224208474159 11.49974184036255 7048024 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010303512040991335 5.706401634216308 0.10435690358281136 11.335693550109863 7251843 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00019571929087760508 10.301091384887695 0.051846858114004135 12.498271465301514 7454604 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000782882038038224 11.039872741699218 0.09651896357536316 12.319390487670898 7657496 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00036702108718600357 8.336643362045288 0.10743595138192177 12.224838066101075 7861571 0
Recovering previous policy with expected return of 75.30845771144278. Long term value was 14.731 and short term was 14.54.


Pure best response payoff estimated to be 80.845 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 84.28 seconds to finish estimate with resulting utilities: [180.015   2.855]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 138.11 seconds to finish estimate with resulting utilities: [133.845  45.515]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 138.73 seconds to finish estimate with resulting utilities: [17.73  20.545]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 137.43 seconds to finish estimate with resulting utilities: [19.52  21.335]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 139.04 seconds to finish estimate with resulting utilities: [18.84  18.275]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 137.47 seconds to finish estimate with resulting utilities: [18.795 19.035]
Computing meta_strategies
Exited RRD with total regret 3.898606186477906 that was less than regret lambda 3.9473684210526305 after 110 iterations 
REGRET STEPS:  20
NEW LAMBDA 3.6842105263157885
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0    50.05           4.25           3.02           2.83           2.60           2.85      
    1    191.22          95.91          43.73          45.80          44.78          45.52      
    2    179.47          135.30          18.81          19.54          19.22          20.55      
    3    181.77          134.06          18.86          18.61          18.79          21.34      
    4    181.09          134.51          17.84          18.36          16.58          18.27      
    5    180.01          133.84          17.73          19.52          18.84          18.91      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0    50.05          191.22          179.47          181.77          181.09          180.01      
    1     4.25          95.91          135.30          134.06          134.51          133.84      
    2     3.02          43.73          18.81          18.86          17.84          17.73      
    3     2.83          45.80          19.54          18.61          18.36          19.52      
    4     2.60          44.78          19.22          18.79          16.58          18.84      
    5     2.85          45.52          20.55          21.34          18.27          18.91      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0    100.11          195.47          182.48          184.60          183.69          182.87      
    1    195.47          191.82          179.03          179.86          179.29          179.36      
    2    182.48          179.03          37.62          38.41          37.06          38.28      
    3    184.60          179.86          38.41          37.22          37.14          40.86      
    4    183.69          179.29          37.06          37.14          33.16          37.11      
    5    182.87          179.36          38.28          40.86          37.11          37.83      

 

Metagame probabilities: 
Player #0: 0.0004  0.3568  0.1728  0.1658  0.1484  0.1558  
Player #1: 0.0004  0.3568  0.1728  0.1658  0.1484  0.1558  
Iteration : 5
Time so far: 41979.69151043892
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-07-19 01:05:36.789591: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11735 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010036892583593726 55.51226272583008 0.19456166923046112 12.174688339233398 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03486160580068827 24.579128074645997 0.7315989911556244 7.169400644302368 229001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034805893898010254 16.962795639038085 0.7740413665771484 7.180881690979004 446570 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03613045960664749 18.222830772399902 0.8414890944957734 6.376984977722168 659625 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03440794982016086 18.659024047851563 0.832203620672226 6.048629713058472 870275 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030702247843146326 9.110823249816894 0.8396230220794678 7.050937747955322 1081933 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026362641341984273 19.267991065979004 0.7399482607841492 6.755222129821777 1289435 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02984461896121502 14.85438060760498 0.954344516992569 6.0716992855072025 1498317 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027041028812527658 13.531983661651612 0.8977439284324646 6.468930578231811 1707822 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023890981264412404 16.428183555603027 0.8663565814495087 6.838543796539307 1918243 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019179368019104005 23.87456512451172 0.7895368039608002 6.50728178024292 2126176 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018137561157345772 12.001612281799316 0.7850717544555664 6.8112109184265135 2334886 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016571140196174384 14.833701133728027 0.8591600060462952 6.127200317382813 2542732 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01218542717397213 6.948004817962646 0.695929205417633 7.337007617950439 2751136 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01034475713968277 10.459037685394287 0.7009354174137116 7.023433971405029 2961439 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010211761016398669 13.202002620697021 0.837830263376236 6.345298957824707 3170292 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0075385372154414656 12.14875316619873 0.7597701489925385 6.8213859558105465 3379503 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005063819023780525 11.132096481323241 0.8594512283802033 6.065338373184204 3588787 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001915489975363016 14.480644798278808 0.6420199632644653 6.981881189346313 3800354 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  4.453734582057223e-05 18.44111557006836 0.8174167692661285 6.207318735122681 4009051 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001108985740575008 11.713693428039551 0.6635591149330139 6.384117794036865 4217171 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00022032113483874127 10.073165941238404 0.5929657518863678 6.765411281585694 4423902 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016956640814896674 10.080933475494385 0.45910867154598234 8.359922313690186 4630744 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00022374719501385698 9.337405061721801 0.4898218154907227 8.344587707519532 4836503 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003869967142236419 7.6180439472198485 0.5186931133270264 7.982616662979126 5042122 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009160114175756461 8.374574422836304 0.49478363394737246 7.88391146659851 5247293 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0032624330400722102 7.825162029266357 0.3511169821023941 9.24488639831543 5452262 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001168640395917464 9.214743423461915 0.39596510529518125 8.668400192260743 5656115 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010162730919546447 8.953943538665772 0.37514130473136903 8.389329433441162 5859763 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011087308340393064 12.692413139343262 0.35134888291358946 9.101577377319336 6064208 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000276193943045655 15.13906946182251 0.18891411721706391 10.818072891235351 6267572 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002721256776567316 10.526993656158448 0.21128828525543214 10.52424201965332 6470895 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005775500565505353 7.444131660461426 0.15087354853749274 9.62558650970459 6673783 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006932659314770717 7.984770107269287 0.09195243790745736 10.64661111831665 6878709 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00028361716831568626 8.728657388687134 0.09118383303284645 10.51594066619873 7082674 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00018958183354698122 8.97723684310913 0.06513332203030586 11.654905033111572 7286147 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007139243942219764 10.727140712738038 0.059736885502934454 11.572470474243165 7488919 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006727616631252431 8.39199275970459 0.06133524626493454 11.551352405548096 7692527 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006588874297449365 8.56322979927063 0.06894957907497883 11.366519546508789 7895883 0
Recovering previous policy with expected return of 65.34825870646766. Long term value was 15.675 and short term was 15.33.


Pure best response payoff estimated to be 76.07 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 84.01 seconds to finish estimate with resulting utilities: [179.31    3.025]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 137.73 seconds to finish estimate with resulting utilities: [135.235  44.915]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 138.0 seconds to finish estimate with resulting utilities: [19.505 17.81 ]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 138.14 seconds to finish estimate with resulting utilities: [20.845 18.715]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 138.14 seconds to finish estimate with resulting utilities: [16.405 17.8  ]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 138.42 seconds to finish estimate with resulting utilities: [17.47  18.185]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 137.57 seconds to finish estimate with resulting utilities: [17.51  17.385]
Computing meta_strategies
Exited RRD with total regret 3.6732540507256104 that was less than regret lambda 3.6842105263157885 after 129 iterations 
REGRET STEPS:  20
NEW LAMBDA 3.4210526315789465
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    50.05           4.25           3.02           2.83           2.60           2.85           3.02      
    1    191.22          95.91          43.73          45.80          44.78          45.52          44.91      
    2    179.47          135.30          18.81          19.54          19.22          20.55          17.81      
    3    181.77          134.06          18.86          18.61          18.79          21.34          18.71      
    4    181.09          134.51          17.84          18.36          16.58          18.27          17.80      
    5    180.01          133.84          17.73          19.52          18.84          18.91          18.18      
    6    179.31          135.24          19.50          20.84          16.41          17.47          17.45      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    50.05          191.22          179.47          181.77          181.09          180.01          179.31      
    1     4.25          95.91          135.30          134.06          134.51          133.84          135.24      
    2     3.02          43.73          18.81          18.86          17.84          17.73          19.50      
    3     2.83          45.80          19.54          18.61          18.36          19.52          20.84      
    4     2.60          44.78          19.22          18.79          16.58          18.84          16.41      
    5     2.85          45.52          20.55          21.34          18.27          18.91          17.47      
    6     3.02          44.91          17.81          18.71          17.80          18.18          17.45      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0    100.11          195.47          182.48          184.60          183.69          182.87          182.34      
    1    195.47          191.82          179.03          179.86          179.29          179.36          180.15      
    2    182.48          179.03          37.62          38.41          37.06          38.28          37.31      
    3    184.60          179.86          38.41          37.22          37.14          40.86          39.56      
    4    183.69          179.29          37.06          37.14          33.16          37.11          34.20      
    5    182.87          179.36          38.28          40.86          37.11          37.83          35.66      
    6    182.34          180.15          37.31          39.56          34.20          35.66          34.90      

 

Metagame probabilities: 
Player #0: 0.0002  0.3603  0.1372  0.1331  0.118  0.1241  0.1271  
Player #1: 0.0002  0.3603  0.1372  0.1331  0.118  0.1241  0.1271  
Iteration : 6
Time so far: 51162.09288954735
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-07-19 03:38:39.247880: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13713 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012448713462799788 65.20226211547852 0.2396760106086731 11.458234786987305 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03134432937949896 17.13766050338745 0.6500323414802551 8.026076412200927 228809 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03629254475235939 17.578745079040527 0.7957614302635193 6.723591136932373 445441 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034943963214755056 23.17416000366211 0.7965870320796966 6.925106811523437 659701 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034616019763052466 15.391850566864013 0.8669527530670166 6.618214273452759 875220 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03273478671908379 18.909061431884766 0.8827972114086151 6.5061554431915285 1088225 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02681521512567997 22.757859992980958 0.7704515993595124 6.475942850112915 1300222 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02878940515220165 20.27141590118408 0.8931077301502228 5.994221830368042 1511880 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025245814584195614 15.27219295501709 0.8570571482181549 6.10445728302002 1722403 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024652864225208758 15.99011812210083 0.9353968918323516 5.575676965713501 1934921 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02414015866816044 15.42383108139038 0.9869863212108612 5.400103664398193 2144000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01933572832494974 17.852246475219726 0.935690414905548 5.545925664901733 2354803 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017170320451259612 14.545549964904785 0.8791500985622406 6.328785848617554 2564975 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01698488499969244 15.784944629669189 1.0072197914123535 5.710672092437744 2775107 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014114120695739984 12.819343852996827 0.9440103530883789 6.057620811462402 2982931 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010722595918923616 9.76714916229248 0.9402454972267151 5.954068756103515 3190740 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007854923233389854 11.778048419952393 0.8910366237163544 6.239774227142334 3397759 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005639724666252732 9.356292247772217 0.9028254210948944 5.863121604919433 3605412 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002747902390547097 10.052481365203857 0.8981179773807526 5.940553951263428 3810901 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008203670236980543 12.385333919525147 0.7396514534950256 6.605704736709595 4018928 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008351027296157554 10.607574272155762 0.6885905861854553 7.030542612075806 4226087 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008115848853776697 9.074628162384034 0.6775367677211761 7.246204376220703 4430898 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007721003901679068 10.146627426147461 0.6758240938186646 7.329219484329224 4636201 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007734280516160652 9.95685691833496 0.37091310918331144 8.674123668670655 4840948 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009817659549298697 9.253812551498413 0.24981005042791365 8.564638996124268 5043907 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00020314909052103757 7.729005432128906 0.17345011681318284 9.382304763793945 5249213 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005090773905976675 15.914445400238037 0.13437291830778123 10.293931198120116 5453417 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00035492009919835255 8.97508749961853 0.1321776479482651 10.980527973175048 5656263 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005815136770252139 10.471420192718506 0.11118372902274132 11.38421173095703 5860704 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00012795649945473997 12.462813568115234 0.08534647822380066 12.539450645446777 6063411 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -5.5681963863207785e-05 9.157331085205078 0.08166326209902763 12.622926044464112 6266144 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00046227993552747646 11.52895383834839 0.08080298230051994 12.893850326538086 6468502 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00027624331414699554 6.8771360397338865 0.06999038644134999 12.888715171813965 6672601 0
/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/imitation_fine_tune.py:379: RuntimeWarning: invalid value encountered in divide
  legal_probs = legal_probs / np.sum(legal_probs)
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001036008310620673 6.259989309310913 0.06719450205564499 12.99167366027832 6875762 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001799679907708196 9.180024528503418 0.06691716946661472 12.781752872467042 7079017 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004025464044389082 8.437939071655274 0.051793795078992844 13.016064167022705 7283283 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0024543804465793073 9.964933967590332 0.054882655292749404 13.015440464019775 7487054 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00016376629027945456 11.918501472473144 0.04994458332657814 13.038381099700928 7691021 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001627946164808236 8.509425401687622 0.047499557957053185 13.408281230926514 7895081 0
Recovering previous policy with expected return of 67.65671641791045. Long term value was 17.054 and short term was 18.48.


Pure best response payoff estimated to be 79.36 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 83.54 seconds to finish estimate with resulting utilities: [181.48   2.62]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 136.51 seconds to finish estimate with resulting utilities: [136.91   46.005]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 136.87 seconds to finish estimate with resulting utilities: [17.87  17.755]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 136.89 seconds to finish estimate with resulting utilities: [19.375 20.19 ]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 137.41 seconds to finish estimate with resulting utilities: [17.05 17.52]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 137.2 seconds to finish estimate with resulting utilities: [18.32 18.27]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 137.69 seconds to finish estimate with resulting utilities: [19.09  18.575]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 134.91 seconds to finish estimate with resulting utilities: [19.585 19.195]
Computing meta_strategies
Exited RRD with total regret 3.3678148028269703 that was less than regret lambda 3.4210526315789465 after 145 iterations 
REGRET STEPS:  20
NEW LAMBDA 3.1578947368421044
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    50.05           4.25           3.02           2.83           2.60           2.85           3.02           2.62      
    1    191.22          95.91          43.73          45.80          44.78          45.52          44.91          46.01      
    2    179.47          135.30          18.81          19.54          19.22          20.55          17.81          17.75      
    3    181.77          134.06          18.86          18.61          18.79          21.34          18.71          20.19      
    4    181.09          134.51          17.84          18.36          16.58          18.27          17.80          17.52      
    5    180.01          133.84          17.73          19.52          18.84          18.91          18.18          18.27      
    6    179.31          135.24          19.50          20.84          16.41          17.47          17.45          18.57      
    7    181.48          136.91          17.87          19.38          17.05          18.32          19.09          19.39      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0    50.05          191.22          179.47          181.77          181.09          180.01          179.31          181.48      
    1     4.25          95.91          135.30          134.06          134.51          133.84          135.24          136.91      
    2     3.02          43.73          18.81          18.86          17.84          17.73          19.50          17.87      
    3     2.83          45.80          19.54          18.61          18.36          19.52          20.84          19.38      
    4     2.60          44.78          19.22          18.79          16.58          18.84          16.41          17.05      
    5     2.85          45.52          20.55          21.34          18.27          18.91          17.47          18.32      
    6     3.02          44.91          17.81          18.71          17.80          18.18          17.45          19.09      
    7     2.62          46.01          17.75          20.19          17.52          18.27          18.57          19.39      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0    100.11          195.47          182.48          184.60          183.69          182.87          182.34          184.10      
    1    195.47          191.82          179.03          179.86          179.29          179.36          180.15          182.91      
    2    182.48          179.03          37.62          38.41          37.06          38.28          37.31          35.62      
    3    184.60          179.86          38.41          37.22          37.14          40.86          39.56          39.56      
    4    183.69          179.29          37.06          37.14          33.16          37.11          34.20          34.57      
    5    182.87          179.36          38.28          40.86          37.11          37.83          35.66          36.59      
    6    182.34          180.15          37.31          39.56          34.20          35.66          34.90          37.66      
    7    184.10          182.91          35.62          39.56          34.57          36.59          37.66          38.78      

 

Metagame probabilities: 
Player #0: 0.0001  0.3637  0.1106  0.1114  0.0952  0.1008  0.1045  0.1137  
Player #1: 0.0001  0.3637  0.1106  0.1114  0.0952  0.1008  0.1045  0.1137  
Iteration : 7
Time so far: 60462.10791683197
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-07-19 06:13:39.649724: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15691 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016361416876316072 59.32813720703125 0.3193227380514145 11.220166492462159 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.041544176265597345 18.067537784576416 0.8462919354438782 7.32375955581665 230366 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.038654713332653044 15.649853801727295 0.8639157295227051 7.014798593521118 443379 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03400971163064241 23.53979148864746 0.7785368442535401 6.826828908920288 657131 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03614006116986275 15.726298522949218 0.9108810722827911 6.103202390670776 871503 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03463948871940374 18.448228359222412 0.9394188463687897 6.108459949493408 1080027 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03470309637486935 19.779970359802245 0.9911860167980194 5.70852837562561 1288185 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02997365575283766 13.81601972579956 0.9366659462451935 5.86232476234436 1495186 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026821688562631608 16.666517066955567 0.8746222674846649 5.869173765182495 1702835 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026490408182144164 14.329903984069825 0.9565764605998993 5.829765176773071 1910139 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023946450278162956 13.131484031677246 0.970282006263733 5.837583684921265 2118672 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020446959510445596 17.481149196624756 0.9064302802085876 6.051013565063476 2328038 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018537701945751904 12.75641326904297 0.9445516526699066 5.802208757400512 2538058 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015108553599566222 12.040645217895507 0.912896066904068 5.483703517913819 2746511 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013230091612786054 14.198535060882568 0.9008656919002533 5.884971189498901 2954248 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011833431571722031 17.612709522247314 0.8273295104503632 6.219406986236573 3159613 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008818347612395883 15.818852996826172 0.9202068984508515 5.917886018753052 3367254 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006272545037791133 11.35620183944702 1.0711203575134278 5.559210872650146 3573731 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0029305654810741545 8.224674034118653 0.9480682492256165 5.877321434020996 3779417 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011642108314845246 9.357438564300537 0.7798063933849335 7.290101099014282 3983992 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011386836522433441 13.171711540222168 0.7041299283504486 7.726076459884643 4190203 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014629679557401688 7.217205333709717 0.7914600253105164 6.649215745925903 4393035 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010198365926044062 9.331670951843261 0.49875594675540924 7.419903659820557 4598726 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007440558489179239 10.450484657287598 0.41063803136348725 7.9783689975738525 4802218 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -1.2153996794950218e-05 13.73989725112915 0.29851125180721283 8.992977046966553 5005670 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00017491812795924488 6.7290246963500975 0.3712665975093842 8.528640079498292 5211065 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006854173494502902 6.560919332504272 0.2723417982459068 9.31469783782959 5413793 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007978524939971976 9.255070161819457 0.2544624745845795 9.540188312530518 5616999 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  1.3974094326840713e-05 12.522488880157471 0.2851046919822693 10.272133445739746 5819076 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00073503623061697 8.404445552825928 0.34592249393463137 9.380494976043702 6023081 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001389085086339037 6.501149606704712 0.2757404610514641 10.339074516296387 6224884 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0070235247985692695 8.3299476146698 0.24890797287225724 11.194512557983398 6431189 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -9.868088691291632e-05 11.560638523101806 0.20002627670764922 10.603139400482178 6634839 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006477137896581553 11.739203548431396 0.22822287678718567 10.708353042602539 6838684 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00015585336950607598 10.604460620880127 0.1849767029285431 10.943006896972657 7040964 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011208057694602758 9.956963062286377 0.14725139290094375 11.648340320587158 7244161 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00037624611577484756 12.06872272491455 0.20956059396266938 10.800282192230224 7447513 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000921129078415106 13.893117141723632 0.1884774386882782 11.833777713775635 7651559 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011304117913823574 7.305818223953247 0.2909896865487099 11.039164543151855 7855525 0
Recovering previous policy with expected return of 70.7860696517413. Long term value was 14.76 and short term was 15.43.


Pure best response payoff estimated to be 79.25 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (8, 0)
Current player 0 and current strategies (8, 0) took 81.61 seconds to finish estimate with resulting utilities: [180.63    2.835]
Estimating current strategies:  (8, 1)
Current player 0 and current strategies (8, 1) took 135.0 seconds to finish estimate with resulting utilities: [133.865  46.445]
Estimating current strategies:  (8, 2)
Current player 0 and current strategies (8, 2) took 136.81 seconds to finish estimate with resulting utilities: [20.71  19.695]
Estimating current strategies:  (8, 3)
Current player 0 and current strategies (8, 3) took 136.46 seconds to finish estimate with resulting utilities: [18.775 19.08 ]
Estimating current strategies:  (8, 4)
Current player 0 and current strategies (8, 4) took 136.57 seconds to finish estimate with resulting utilities: [16.905 16.73 ]
Estimating current strategies:  (8, 5)
Current player 0 and current strategies (8, 5) took 136.48 seconds to finish estimate with resulting utilities: [19.325 18.83 ]
Estimating current strategies:  (8, 6)
Current player 0 and current strategies (8, 6) took 137.12 seconds to finish estimate with resulting utilities: [19.115 19.885]
Estimating current strategies:  (8, 7)
Current player 0 and current strategies (8, 7) took 136.26 seconds to finish estimate with resulting utilities: [18.69 20.83]
Estimating current strategies:  (8, 8)
Current player 0 and current strategies (8, 8) took 135.77 seconds to finish estimate with resulting utilities: [15.025 17.38 ]
Computing meta_strategies
Exited RRD with total regret 3.13767365207584 that was less than regret lambda 3.1578947368421044 after 158 iterations 
REGRET STEPS:  20
NEW LAMBDA 2.8947368421052624
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    50.05           4.25           3.02           2.83           2.60           2.85           3.02           2.62           2.83      
    1    191.22          95.91          43.73          45.80          44.78          45.52          44.91          46.01          46.45      
    2    179.47          135.30          18.81          19.54          19.22          20.55          17.81          17.75          19.70      
    3    181.77          134.06          18.86          18.61          18.79          21.34          18.71          20.19          19.08      
    4    181.09          134.51          17.84          18.36          16.58          18.27          17.80          17.52          16.73      
    5    180.01          133.84          17.73          19.52          18.84          18.91          18.18          18.27          18.83      
    6    179.31          135.24          19.50          20.84          16.41          17.47          17.45          18.57          19.89      
    7    181.48          136.91          17.87          19.38          17.05          18.32          19.09          19.39          20.83      
    8    180.63          133.87          20.71          18.77          16.91          19.32          19.11          18.69          16.20      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0    50.05          191.22          179.47          181.77          181.09          180.01          179.31          181.48          180.63      
    1     4.25          95.91          135.30          134.06          134.51          133.84          135.24          136.91          133.87      
    2     3.02          43.73          18.81          18.86          17.84          17.73          19.50          17.87          20.71      
    3     2.83          45.80          19.54          18.61          18.36          19.52          20.84          19.38          18.77      
    4     2.60          44.78          19.22          18.79          16.58          18.84          16.41          17.05          16.91      
    5     2.85          45.52          20.55          21.34          18.27          18.91          17.47          18.32          19.32      
    6     3.02          44.91          17.81          18.71          17.80          18.18          17.45          19.09          19.11      
    7     2.62          46.01          17.75          20.19          17.52          18.27          18.57          19.39          18.69      
    8     2.83          46.45          19.70          19.08          16.73          18.83          19.89          20.83          16.20      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8      
    0    100.11          195.47          182.48          184.60          183.69          182.87          182.34          184.10          183.47      
    1    195.47          191.82          179.03          179.86          179.29          179.36          180.15          182.91          180.31      
    2    182.48          179.03          37.62          38.41          37.06          38.28          37.31          35.62          40.41      
    3    184.60          179.86          38.41          37.22          37.14          40.86          39.56          39.56          37.85      
    4    183.69          179.29          37.06          37.14          33.16          37.11          34.20          34.57          33.64      
    5    182.87          179.36          38.28          40.86          37.11          37.83          35.66          36.59          38.16      
    6    182.34          180.15          37.31          39.56          34.20          35.66          34.90          37.66          39.00      
    7    184.10          182.91          35.62          39.56          34.57          36.59          37.66          38.78          39.52      
    8    183.47          180.31          40.41          37.85          33.64          38.16          39.00          39.52          32.41      

 

Metagame probabilities: 
Player #0: 0.0001  0.368  0.0956  0.0947  0.0788  0.0857  0.0908  0.1008  0.0854  
Player #1: 0.0001  0.368  0.0956  0.0947  0.0788  0.0857  0.0908  0.1008  0.0854  
Iteration : 8
Time so far: 69821.9296476841
Approximating Best Response
Training best response:  True 0.06
rtg values check:  (?,) (?, 1)
2023-07-19 08:49:39.284078: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_62/weights_2/Adam_1/Assign' id:17669 op device:{requested: '', assigned: ''} def:{{{node mlp_62/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_62/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_62/weights_2/Adam_1, mlp_62/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017490606848150493 80.48953628540039 0.33658303916454313 10.274033451080323 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03996732980012894 13.577234649658203 0.8055013060569763 7.082257223129273 225379 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03376433420926332 15.698108577728272 0.7256765723228454 6.431088972091675 437628 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03280930183827877 17.19546060562134 0.7752017974853516 6.7000758171081545 650794 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.033847594447433946 20.210724067687988 0.8514915883541108 6.128452444076538 866134 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034790050610899924 19.00803337097168 0.9188712894916534 5.733194780349732 1080582 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03191120568662882 19.43177490234375 0.9025794088840484 5.8378424644470215 1298222 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02871814612299204 17.728739261627197 0.900789350271225 5.688431739807129 1514292 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030045221745967864 16.746475982666016 0.999725753068924 5.32804799079895 1730335 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022796662338078023 16.676235103607176 0.8370916604995727 5.433426141738892 1944202 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021222900412976742 16.899517917633055 0.8683062076568604 5.8423456192016605 2157456 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019595806300640107 16.79296283721924 0.9046484112739563 5.619905090332031 2372499 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01810033628717065 14.290429496765137 0.9651521444320679 5.284568452835083 2585544 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014073008671402931 25.39658393859863 0.8380058467388153 5.713134145736694 2798415 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011977100372314453 24.312500190734863 0.8692120492458344 5.788088369369507 3011484 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008453229488804937 18.59016752243042 0.8236696004867554 6.014701223373413 3222338 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007739494787529111 16.04574308395386 0.90302295088768 5.413529777526856 3432411 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004600897361524403 14.561104583740235 0.9174550592899322 5.740680265426636 3644030 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00233885693596676 18.770314025878907 0.7875397622585296 5.640373706817627 3851535 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007597198185976594 12.3147686958313 0.7962080538272858 6.083752298355103 4061356 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010859488742426039 17.19958715438843 0.7955682516098023 5.795790195465088 4271481 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016660327528370544 12.870734119415284 0.785153615474701 5.915084981918335 4481451 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010497384449990933 12.713720512390136 0.6308368086814881 6.61881742477417 4691315 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010971577488817275 16.616106224060058 0.5879228472709656 6.76203556060791 4901071 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003824469284154475 13.550606060028077 0.6153235137462616 6.615492105484009 5108857 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006626359972869977 10.90874490737915 0.5952660977840424 6.582933807373047 5317883 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006500487303128466 12.796807861328125 0.40997678637504575 7.04009599685669 5524731 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006547130411490798 12.802823257446288 0.5736092567443848 7.151292705535889 5730954 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013568408030550927 10.314437198638917 0.31178800761699677 7.641306829452515 5936596 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008216582413297146 9.528014373779296 0.2274615377187729 8.66523609161377 6141847 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015069004744873381 11.112522602081299 0.3460220694541931 8.77567653656006 6347403 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003698309083119966 8.051875829696655 0.2250712126493454 8.959061241149902 6552643 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006564703535332228 16.05913381576538 0.185018327832222 8.805098915100098 6757391 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004442627847311087 11.454500770568847 0.186279821395874 9.335791206359863 6963073 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005443325091619045 12.336505317687989 0.1605393707752228 10.253849506378174 7169165 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009991863451432436 12.754868507385254 0.3916205495595932 9.130147743225098 7374978 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010559850648860446 11.887825965881348 0.45853952765464784 9.233335399627686 7579524 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00045773747260682287 10.193339633941651 0.3331527650356293 9.875860786437988 7784972 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012435494645615108 10.178991317749023 0.4080530911684036 9.229611015319824 7992289 0
Recovering previous policy with expected return of 64.34328358208955. Long term value was 26.162 and short term was 25.125.


Pure best response payoff estimated to be 76.705 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (9, 0)
Current player 0 and current strategies (9, 0) took 83.8 seconds to finish estimate with resulting utilities: [182.275   2.725]
Estimating current strategies:  (9, 1)
Current player 0 and current strategies (9, 1) took 137.09 seconds to finish estimate with resulting utilities: [135.575  45.05 ]
Estimating current strategies:  (9, 2)
Current player 0 and current strategies (9, 2) took 137.61 seconds to finish estimate with resulting utilities: [18.675 17.68 ]
Estimating current strategies:  (9, 3)
Current player 0 and current strategies (9, 3) took 138.17 seconds to finish estimate with resulting utilities: [20.355 17.96 ]
Estimating current strategies:  (9, 4)
Current player 0 and current strategies (9, 4) took 137.95 seconds to finish estimate with resulting utilities: [20.035 18.61 ]
Estimating current strategies:  (9, 5)
Current player 0 and current strategies (9, 5) took 138.21 seconds to finish estimate with resulting utilities: [20.94 21.14]
Estimating current strategies:  (9, 6)
Current player 0 and current strategies (9, 6) took 137.11 seconds to finish estimate with resulting utilities: [17.91  16.435]
Estimating current strategies:  (9, 7)
Current player 0 and current strategies (9, 7) took 138.16 seconds to finish estimate with resulting utilities: [22.48 19.72]
Estimating current strategies:  (9, 8)
Current player 0 and current strategies (9, 8) took 137.93 seconds to finish estimate with resulting utilities: [18.175 18.22 ]
Estimating current strategies:  (9, 9)
Current player 0 and current strategies (9, 9) took 136.8 seconds to finish estimate with resulting utilities: [16.085 14.925]
Computing meta_strategies
Exited RRD with total regret 2.8560891784087374 that was less than regret lambda 2.8947368421052624 after 171 iterations 
REGRET STEPS:  20
NEW LAMBDA 2.6315789473684204
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    50.05           4.25           3.02           2.83           2.60           2.85           3.02           2.62           2.83           2.73      
    1    191.22          95.91          43.73          45.80          44.78          45.52          44.91          46.01          46.45          45.05      
    2    179.47          135.30          18.81          19.54          19.22          20.55          17.81          17.75          19.70          17.68      
    3    181.77          134.06          18.86          18.61          18.79          21.34          18.71          20.19          19.08          17.96      
    4    181.09          134.51          17.84          18.36          16.58          18.27          17.80          17.52          16.73          18.61      
    5    180.01          133.84          17.73          19.52          18.84          18.91          18.18          18.27          18.83          21.14      
    6    179.31          135.24          19.50          20.84          16.41          17.47          17.45          18.57          19.89          16.43      
    7    181.48          136.91          17.87          19.38          17.05          18.32          19.09          19.39          20.83          19.72      
    8    180.63          133.87          20.71          18.77          16.91          19.32          19.11          18.69          16.20          18.22      
    9    182.28          135.57          18.68          20.36          20.04          20.94          17.91          22.48          18.18          15.51      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    50.05          191.22          179.47          181.77          181.09          180.01          179.31          181.48          180.63          182.28      
    1     4.25          95.91          135.30          134.06          134.51          133.84          135.24          136.91          133.87          135.57      
    2     3.02          43.73          18.81          18.86          17.84          17.73          19.50          17.87          20.71          18.68      
    3     2.83          45.80          19.54          18.61          18.36          19.52          20.84          19.38          18.77          20.36      
    4     2.60          44.78          19.22          18.79          16.58          18.84          16.41          17.05          16.91          20.04      
    5     2.85          45.52          20.55          21.34          18.27          18.91          17.47          18.32          19.32          20.94      
    6     3.02          44.91          17.81          18.71          17.80          18.18          17.45          19.09          19.11          17.91      
    7     2.62          46.01          17.75          20.19          17.52          18.27          18.57          19.39          18.69          22.48      
    8     2.83          46.45          19.70          19.08          16.73          18.83          19.89          20.83          16.20          18.18      
    9     2.73          45.05          17.68          17.96          18.61          21.14          16.43          19.72          18.22          15.51      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0    100.11          195.47          182.48          184.60          183.69          182.87          182.34          184.10          183.47          185.00      
    1    195.47          191.82          179.03          179.86          179.29          179.36          180.15          182.91          180.31          180.62      
    2    182.48          179.03          37.62          38.41          37.06          38.28          37.31          35.62          40.41          36.36      
    3    184.60          179.86          38.41          37.22          37.14          40.86          39.56          39.56          37.85          38.31      
    4    183.69          179.29          37.06          37.14          33.16          37.11          34.20          34.57          33.64          38.64      
    5    182.87          179.36          38.28          40.86          37.11          37.83          35.66          36.59          38.16          42.08      
    6    182.34          180.15          37.31          39.56          34.20          35.66          34.90          37.66          39.00          34.34      
    7    184.10          182.91          35.62          39.56          34.57          36.59          37.66          38.78          39.52          42.20      
    8    183.47          180.31          40.41          37.85          33.64          38.16          39.00          39.52          32.41          36.39      
    9    185.00          180.62          36.36          38.31          38.64          42.08          34.34          42.20          36.39          31.01      

 

Metagame probabilities: 
Player #0: 0.0001  0.3699  0.081  0.0802  0.0682  0.0767  0.0755  0.0887  0.0729  0.0868  
Player #1: 0.0001  0.3699  0.081  0.0802  0.0682  0.0767  0.0755  0.0887  0.0729  0.0868  
Iteration : 9
Time so far: 79335.72699403763
Approximating Best Response
Training best response:  True 0.05500000000000001
rtg values check:  (?,) (?, 1)
2023-07-19 11:28:13.250022: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_69/weights_2/Adam_1/Assign' id:19647 op device:{requested: '', assigned: ''} def:{{{node mlp_69/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_69/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_69/weights_2/Adam_1, mlp_69/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01668929001316428 72.21004981994629 0.32193620800971984 10.85021915435791 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03381739072501659 28.375925064086914 0.709988659620285 7.185967493057251 227526 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03548979368060827 17.362681484222414 0.7661608934402466 6.907972717285157 440500 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03437994159758091 16.65825786590576 0.8016998171806335 6.3099242687225345 653263 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03737365901470184 14.33118143081665 0.9303748190402985 5.9365159034729 863792 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03508874252438545 13.068044948577882 0.9422287046909332 5.65475378036499 1072947 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03173817042261362 13.236820411682128 0.9173939526081085 5.74276032447815 1282576 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0327314829453826 18.64622631072998 1.0033992648124694 5.598718214035034 1489233 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026573150791227816 13.26358413696289 0.9052681207656861 5.848191213607788 1697461 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028184637799859046 11.151746654510498 1.0617741644382477 5.1538169384002686 1906538 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023301393911242484 13.9045090675354 0.9346551418304443 5.596059894561767 2114867 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019707788340747358 13.608018016815185 0.9002614676952362 5.960183191299438 2322597 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016116182412952186 19.462646675109863 0.7806355834007264 6.307864141464234 2531499 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01665610307827592 12.530995941162109 0.9488760590553283 5.449936151504517 2738489 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013860302232205867 13.592778587341309 0.9158834159374237 5.788741254806519 2946089 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010792949423193931 15.019869232177735 0.9154167890548706 5.530359697341919 3155706 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008724071085453033 21.065489196777342 0.8513144433498383 5.734230899810791 3364150 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005737516283988953 16.601433944702148 0.8123637199401855 6.157061576843262 3572232 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003600576485041529 14.134474277496338 0.8298299789428711 5.691268491744995 3778967 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000504736001312267 15.165877437591552 0.6937114655971527 7.032007884979248 3987399 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008201855380320922 13.639022064208984 0.6217449486255646 7.1110704898834225 4194817 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00043036970691900933 9.53934359550476 0.5987777173519134 7.193047761917114 4400441 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001059376756893471 10.114975452423096 0.4956148326396942 7.578039884567261 4605900 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007340487907640636 12.015691471099853 0.400767520070076 7.36556978225708 4811359 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006354115947033278 6.923042249679566 0.3854126572608948 7.999944162368775 5016322 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000341562038192933 7.443282175064087 0.25688680410385134 8.422324657440186 5220458 0
slurmstepd: error: *** JOB 56042727 ON gl3382 CANCELLED AT 2023-07-19T13:00:48 ***
