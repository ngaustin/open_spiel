Job Id listed below:
56105362

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

2023-07-19 16:08:43.351358: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-19 16:08:43.754246: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-19 16:08:45.736327: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0719 16:08:50.060421 22415580064640 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x1462b2b02d10>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x1462b2b02d10>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-07-19 16:08:50.384449: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-07-19 16:08:50.695056: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.54 seconds to finish estimate with resulting utilities: [49.485 48.155]
Exited RRD with total regret 0.0 that was less than regret lambda 10.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    48.82      

 

Player 1 Payoff matrix: 

           0      
    0    48.82      

 

Social Welfare Sum Matrix: 

           0      
    0    97.64      

 

Iteration : 0
Time so far: 0.0001323223114013672
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-07-19 16:09:10.126528: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.12633658051490784 19.20720729827881 2.064476227760315 0.0010108092101290822 10351 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09246207103133201 14.26276216506958 1.8458521485328674 0.28228759765625 215011 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09557841271162033 14.607465553283692 1.8181101441383363 0.4389186084270477 417003 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0859670288860798 17.239600372314452 1.783534872531891 0.5754377186298371 619028 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0795794390141964 14.723380661010742 1.757288646697998 0.6657259643077851 820163 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07962859943509101 14.632774543762206 1.7301746010780334 0.8455030500888825 1021866 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.061903339251875876 15.953991985321045 1.7072655320167542 0.9155314862728119 1223842 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.057933558523654935 16.903697872161864 1.6774355173110962 1.0309804618358611 1426472 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05753586702048778 15.953246593475342 1.6584941744804382 1.1797781825065612 1630018 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.050436945632100105 18.46881809234619 1.638410484790802 1.2665564656257629 1837413 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03988521918654442 16.968946552276613 1.5695276618003846 1.4670427799224854 2045470 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03484294824302196 21.801044845581053 1.519215953350067 1.5602876901626588 2250608 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030110261030495168 22.363871765136718 1.4572543382644654 1.7886115193367005 2459330 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0255135964602232 21.204083824157713 1.4264557719230653 1.8409586310386659 2669379 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021727414429187776 21.20938720703125 1.354109525680542 1.8323955297470094 2877191 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016802508477121593 22.08071937561035 1.2396944522857667 2.218937397003174 3085477 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012169008608907461 27.021415138244627 1.186603832244873 2.2643255472183226 3298284 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009505579806864261 23.941225051879883 1.1479288578033446 2.4781495094299317 3508681 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006868324452079832 22.523485565185545 1.120445430278778 2.6877950191497804 3721744 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0031947138835676015 23.061209678649902 1.0285210072994233 2.967223310470581 3933523 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0024835659307427703 22.858378982543947 0.9431715130805969 3.224559783935547 4146907 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012973629673069809 21.654510498046875 0.8427732408046722 3.451025152206421 4359905 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0022560947414604015 29.78856372833252 0.7486715197563172 3.7223947048187256 4576240 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010209286818280816 22.820408248901366 0.734482616186142 3.9163100481033326 4794156 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008178533229511231 25.53079147338867 0.6631918370723724 4.10851731300354 5012144 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013480928959324957 28.257624435424805 0.6338769793510437 4.33853554725647 5227586 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019445576646830887 22.481509590148924 0.5384348630905151 4.999893426895142 5444309 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016933189224801026 21.061820602416994 0.5148541241884231 5.1899762630462645 5662828 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013815533169690753 27.50660095214844 0.49419461488723754 5.622139120101929 5880460 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015641623278497719 22.978364562988283 0.47546024024486544 5.781846380233764 6099352 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00196058936562622 22.665496253967284 0.4577534139156342 5.951591968536377 6316512 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001866746291398158 26.748496055603027 0.4816253334283829 5.97356424331665 6534640 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004907583352178335 22.783938789367674 0.4635424345731735 6.084367275238037 6753069 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015269998868461699 23.759082221984862 0.43582254350185395 6.406616067886352 6973069 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  7.3702685767784715e-06 27.463606071472167 0.4261012673377991 6.51745285987854 7191699 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014445194625295698 24.86510314941406 0.3757799953222275 6.827926874160767 7409728 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008248441197792999 24.617989540100098 0.3821127027273178 6.6889983177185055 7626950 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016112237353809178 28.350640869140626 0.3690437823534012 6.974125814437866 7844001 0


Pure best response payoff estimated to be 194.185 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 92.49 seconds to finish estimate with resulting utilities: [191.635   4.325]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 151.87 seconds to finish estimate with resulting utilities: [93.27  94.925]
Computing meta_strategies
Exited RRD with total regret 9.456542146806441 that was less than regret lambda 10.0 after 27 iterations 
REGRET STEPS:  15
NEW LAMBDA 9.285714285714286
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    48.82           4.33      
    1    191.63          94.10      

 

Player 1 Payoff matrix: 

           0              1      
    0    48.82          191.63      
    1     4.33          94.10      

 

Social Welfare Sum Matrix: 

           0              1      
    0    97.64          195.96      
    1    195.96          188.19      

 

Metagame probabilities: 
Player #0: 0.0511  0.9489  
Player #1: 0.0511  0.9489  
Iteration : 1
Time so far: 6518.1791479587555
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-07-19 17:57:48.434250: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011986846756190061 115.51176147460937 0.23447301834821702 9.394608688354491 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03315906412899494 22.829256439208983 0.6970369160175324 6.952576684951782 228598 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.033782152086496355 15.19575901031494 0.7417762994766235 6.536589241027832 439609 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029435091465711594 18.883792686462403 0.6950092077255249 6.886455154418945 654428 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030982327274978162 24.95139331817627 0.7998051106929779 6.289169692993164 870211 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0322661392390728 15.602799224853516 0.8293504595756531 6.160978221893311 1086460 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030348853394389152 26.2335506439209 0.8564580380916595 6.129549837112426 1302759 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026369198597967623 22.49716148376465 0.8325341403484344 5.72597827911377 1519013 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026267918385565282 16.995313453674317 0.8845804810523987 5.635882377624512 1736600 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02146766297519207 16.53237705230713 0.8410942792892456 5.73858232498169 1953656 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019910967163741588 17.84104719161987 0.8184248685836792 5.89548134803772 2170812 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019621685706079008 16.675661945343016 0.8520306587219239 5.756355905532837 2388720 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016802745498716833 19.207718086242675 0.8610592305660247 5.405166101455689 2606876 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013954555802047252 16.48916883468628 0.8540397882461548 5.975184535980224 2821824 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011510267574340104 15.80273494720459 0.7976036310195923 5.8374591827392575 3035706 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008448586147278548 16.916154384613037 0.7975383579730988 6.139853525161743 3252590 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007374087302014232 17.560475730895995 0.7876891314983367 6.096485567092896 3468241 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003771280078217387 18.461884212493896 0.7139329314231873 6.714068031311035 3686513 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0025145485531538727 17.550336170196534 0.6367916703224182 6.639696788787842 3903700 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00149410175254161 20.912261962890625 0.633361142873764 6.979865407943725 4117836 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012152789829997345 19.01030731201172 0.5617174327373504 7.374325180053711 4333270 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017716439760988578 26.764316940307616 0.5745686411857605 7.342123317718506 4551157 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006414433140889741 22.129290962219237 0.5796979665756226 7.389017724990845 4767291 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007217500358819961 16.03553352355957 0.568023008108139 7.7000226974487305 4984717 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00024966620694613086 16.52780065536499 0.5510829806327819 7.626784086227417 5203196 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001265956872521201 16.334113788604736 0.4773611217737198 7.973102188110351 5422391 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011135360602565925 22.220423698425293 0.4430183440446854 7.9074536800384525 5641559 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008046609538723715 25.986658096313477 0.3900354981422424 8.674951553344727 5857237 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008389516442548484 20.557935523986817 0.37010484337806704 8.33610782623291 6073783 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015110944557818584 19.224233436584473 0.381011900305748 8.360348415374755 6290278 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002029501338256523 16.142618942260743 0.3204525798559189 8.89580602645874 6507599 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009872951108263806 31.482468795776366 0.23208397924900054 8.842721843719483 6725559 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00032009769056458027 22.307429313659668 0.23925273418426513 8.434663391113281 6944918 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008804741264611948 24.44251937866211 0.22923380136489868 8.937227535247803 7163002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015763114846777172 20.21902847290039 0.20026575773954391 8.755789184570313 7380817 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017216367181390525 16.980848598480225 0.17948183864355088 9.126107597351075 7599739 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004857121617533267 21.79927864074707 0.17184535712003707 9.324870777130126 7818935 0


Pure best response payoff estimated to be 136.55 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 91.51 seconds to finish estimate with resulting utilities: [181.005   2.935]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 154.55 seconds to finish estimate with resulting utilities: [132.245  55.45 ]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 153.41 seconds to finish estimate with resulting utilities: [47.595 46.51 ]
Computing meta_strategies
Exited RRD with total regret 9.045013869179456 that was less than regret lambda 9.285714285714286 after 44 iterations 
REGRET STEPS:  15
NEW LAMBDA 8.571428571428573
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    48.82           4.33           2.94      
    1    191.63          94.10          55.45      
    2    181.00          132.25          47.05      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    48.82          191.63          181.00      
    1     4.33          94.10          132.25      
    2     2.94          55.45          47.05      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    97.64          195.96          183.94      
    1    195.96          188.19          187.69      
    2    183.94          187.69          94.10      

 

Metagame probabilities: 
Player #0: 0.0109  0.3858  0.6033  
Player #1: 0.0109  0.3858  0.6033  
Iteration : 2
Time so far: 15803.408868074417
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-07-19 20:32:33.779358: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014430310344323516 67.95823135375977 0.2792019546031952 11.21110134124756 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025872361660003663 18.53474540710449 0.5279474377632141 9.948448276519775 228353 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0319365119561553 18.84802837371826 0.6818072855472564 8.950006771087647 445549 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028558837622404097 17.270879554748536 0.6589019060134887 8.938506317138671 661781 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02947304006665945 19.486531066894532 0.717258608341217 9.096463584899903 878056 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0283385107293725 14.932264518737792 0.7500222086906433 9.430088806152344 1096660 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02513988371938467 17.390641689300537 0.7176209032535553 8.74855661392212 1312678 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022625542990863324 21.959728240966797 0.6822583258152009 8.52823486328125 1531738 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023162458650767805 20.890718841552733 0.7889576017856598 8.292895221710205 1749963 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017718845698982476 25.7575626373291 0.6351149141788482 8.309618663787841 1968317 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017926152888685466 22.61884117126465 0.7118089139461518 8.205883121490478 2179570 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01485736556351185 19.002077198028566 0.6734527170658111 7.84293212890625 2391652 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013269483484327793 14.399212551116943 0.655261754989624 8.689438533782958 2601215 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009856128040701151 27.78476505279541 0.6007730424404144 8.235157203674316 2816465 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008514670468866826 14.682466888427735 0.6780376672744751 8.795590877532959 3032524 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005727405706420541 18.794979572296143 0.5975484848022461 8.548133277893067 3246567 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004951784224249423 18.419298458099366 0.620256108045578 8.719462585449218 3462306 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0030738090281374753 25.974122047424316 0.5914845287799835 8.868370151519775 3676750 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002533095935359597 10.317379093170166 0.6086275219917298 9.01494836807251 3889963 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015553017990896478 18.49427080154419 0.5792872309684753 8.648605632781983 4103233 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011549817601917312 21.23394603729248 0.5577630668878555 8.429186820983887 4316253 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003101064488873817 15.479179000854492 0.5501992285251618 9.417044353485107 4530774 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009977141926356126 14.83238410949707 0.5423010438680649 9.247799015045166 4744797 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0004058931677718647 17.00804500579834 0.5301692605018615 9.722024059295654 4956954 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006060062703909352 18.99602699279785 0.5500552952289581 9.450914478302002 5168360 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011519332794705406 19.08211612701416 0.470651713013649 9.400575923919678 5381399 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00014545215187808936 20.395597457885742 0.3782424360513687 9.98146619796753 5591850 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008426598767982795 24.76260414123535 0.4154093861579895 9.521230888366699 5799304 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00048417568623904117 21.760754203796388 0.344231578707695 10.404737281799317 6009993 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008349724361323751 21.922218322753906 0.2817607432603836 10.019517612457275 6220288 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012280735303647817 13.773567295074463 0.20224284529685974 10.163480663299561 6430961 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00021650331136697788 19.04353084564209 0.18619873374700546 10.432418251037598 6641415 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012396827281918378 20.84142360687256 0.16231942921876907 10.561603832244874 6851441 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001975450199097395 15.740905284881592 0.1306813456118107 10.787189483642578 7060402 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010152332775760441 17.122103786468507 0.15514373481273652 11.013040542602539 7270795 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004215410925098695 15.460777854919433 0.1351989336311817 11.17604751586914 7478789 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010240584562779987 16.047935581207277 0.14486084282398223 10.329517459869384 7690017 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011077150411438197 18.336526107788085 0.1259130656719208 10.784753513336181 7902632 0


Pure best response payoff estimated to be 92.94 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 52.03 seconds to finish estimate with resulting utilities: [94.125  1.42 ]
Estimating current strategies:  (3, 1)
slurmstepd: error: *** JOB 56105362 ON gl3388 CANCELLED AT 2023-07-19T23:04:42 ***
