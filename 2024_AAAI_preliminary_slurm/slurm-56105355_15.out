Job Id listed below:
56105501

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

2023-07-20 03:18:29.462739: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-20 03:18:37.700724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0720 03:18:55.645663 23337497643904 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x153966022d40>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x153966022d40>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-07-20 03:18:56.200858: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-07-20 03:18:56.833928: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.3 seconds to finish estimate with resulting utilities: [47.33 48.11]
Exited RRD with total regret 0.0 that was less than regret lambda 10.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    47.72      

 

Player 1 Payoff matrix: 

           0      
    0    47.72      

 

Social Welfare Sum Matrix: 

           0      
    0    95.44      

 

Iteration : 0
Time so far: 0.0001823902130126953
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-07-20 03:19:16.097205: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.11677118092775345 22.19205684661865 2.065126967430115 0.000823552964720875 10225 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09918331503868102 15.87167043685913 1.907548975944519 0.1656922161579132 217165 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09535689875483513 17.587508869171142 1.8875294923782349 0.21903396397829056 419428 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08533623367547989 14.861526012420654 1.8384912967681886 0.3020058751106262 621712 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08297120928764343 18.36008472442627 1.8085055589675902 0.37443051636219027 823922 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06999652534723282 16.67080154418945 1.7672550439834596 0.4554473847150803 1026551 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07151206135749817 19.10199317932129 1.7556579470634461 0.5477941155433654 1231075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05947081744670868 18.207569122314453 1.6853622794151306 0.660118556022644 1435160 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.055428322032094 19.182119369506836 1.6679150581359863 0.7782239735126495 1642082 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.051410061120986936 15.898698139190675 1.612567138671875 0.9556554973125457 1847701 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.039437352865934375 20.495766830444335 1.519152081012726 1.0891810297966003 2056774 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03463821373879909 24.494956016540527 1.501608097553253 1.2255267739295959 2265010 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02890576757490635 20.178922367095947 1.4219065308570862 1.3878900051116942 2472837 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023424083180725575 17.789039611816406 1.327304208278656 1.6219637870788575 2681606 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01966508161276579 20.573052406311035 1.2661261677742004 1.8370940446853639 2892546 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01777920499444008 24.135115242004396 1.2179742336273194 2.0408427715301514 3103396 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011270613176748157 23.676362228393554 1.1701523780822753 2.1704060554504396 3314532 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008182357344776393 22.147994804382325 1.0974021315574647 2.458827543258667 3523917 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005074887233786285 25.878209495544432 0.9923476576805115 2.887230730056763 3737694 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002822117178584449 21.633613204956056 0.9663346946239472 2.951637101173401 3952206 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0027115697725093924 24.39430503845215 0.8424080014228821 3.4500391483306885 4165018 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0024081715499050917 23.187549591064453 0.8112946033477784 3.5454113721847533 4379296 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0025433072471059857 25.268379974365235 0.7528275907039642 3.776360201835632 4593777 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019281842993223107 25.798346328735352 0.6924863994121552 4.316032886505127 4808229 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001246989457285963 26.56555004119873 0.666473388671875 4.512859630584717 5023304 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012630322336917744 20.764741134643554 0.6202652096748352 4.912544012069702 5239760 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000296217406867072 25.27724132537842 0.5786911904811859 5.308075332641602 5452783 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005553122347919271 30.004366302490233 0.5579272866249084 5.434690713882446 5668517 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018970935618199292 25.39458484649658 0.5529358685016632 5.795459365844726 5886072 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004249740915838629 20.268451690673828 0.5184614479541778 6.003150177001953 6104460 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013047752567217686 25.546137046813964 0.4882117360830307 6.528438377380371 6321561 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00032050813169917094 27.893917655944826 0.4869822353124619 6.806841087341309 6539295 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012427812034729868 24.616022300720214 0.46044166684150695 7.139293098449707 6757261 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011676854934194124 24.15765953063965 0.4583856999874115 7.419090223312378 6975471 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017490511789219453 25.858979797363283 0.4309320539236069 7.600444650650024 7193915 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005007030864362605 26.309840965270997 0.39054463505744935 7.7237342357635494 7410938 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011410792940296233 24.560542106628418 0.4136681854724884 7.75796594619751 7628358 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008593352657044306 22.138809204101562 0.3741943359375 8.006447696685791 7847513 0


Pure best response payoff estimated to be 194.065 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 82.09 seconds to finish estimate with resulting utilities: [188.74    3.915]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 135.4 seconds to finish estimate with resulting utilities: [95.83  94.395]
Computing meta_strategies
Exited RRD with total regret 9.392131191101043 that was less than regret lambda 10.0 after 27 iterations 
REGRET STEPS:  15
NEW LAMBDA 9.285714285714286
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    47.72           3.92      
    1    188.74          95.11      

 

Player 1 Payoff matrix: 

           0              1      
    0    47.72          188.74      
    1     3.92          95.11      

 

Social Welfare Sum Matrix: 

           0              1      
    0    95.44          192.66      
    1    192.66          190.22      

 

Metagame probabilities: 
Player #0: 0.0501  0.9499  
Player #1: 0.0501  0.9499  
Iteration : 1
Time so far: 6309.335688352585
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-07-20 05:04:25.499002: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03452730365097523 72.54319305419922 0.6773735463619233 7.851988840103149 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04198137260973454 17.881245040893553 0.857051694393158 5.579654121398926 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03720892705023289 18.627396202087404 0.8021922647953034 5.331975078582763 451000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.036475395783782005 17.76319999694824 0.8606445372104645 5.195519256591797 670466 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03306269813328981 22.54883155822754 0.7971634089946746 5.464553451538086 889324 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03560941517353058 18.488459396362305 0.9388980329036712 4.820674133300781 1109324 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03167598340660334 20.574234008789062 0.9074269890785217 4.597181415557861 1329324 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03084185104817152 20.601913833618163 0.9657159328460694 4.451896905899048 1549324 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027527781203389168 17.973773765563966 0.9353962361812591 4.6942695617675785 1769225 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02336462214589119 13.43504524230957 0.9123665928840637 4.611882877349854 1988277 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022729780152440072 24.03155689239502 0.9404890179634094 4.485727071762085 2208277 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01861520791426301 17.78564805984497 0.8908417582511902 4.9234373569488525 2428096 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01872558183968067 17.083262538909914 0.9366961896419526 4.544328260421753 2646163 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014867715071886778 18.094815826416017 0.9563764810562134 4.829420661926269 2864444 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011915974039584398 23.798851203918456 0.9402317821979522 4.479423093795776 3082046 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00904477839358151 16.840988445281983 0.8640607476234436 4.818089437484741 3301049 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0056811771122738716 16.842795658111573 0.8146421015262604 5.057074975967407 3520648 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0037281329394318165 15.806785678863525 0.7781181573867798 5.242901039123535 3739433 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014608528523240238 18.181595611572266 0.7392578303813935 5.3750975131988525 3959060 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006670163085800596 17.759502601623534 0.7340022981166839 5.420351266860962 4179060 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001468911801930517 19.341421127319336 0.6570128858089447 5.713974189758301 4398697 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008566955628339201 22.875714111328126 0.5404493659734726 6.037481260299683 4618184 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009548160247504711 19.632963180541992 0.5490989714860917 6.054322290420532 4838184 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00044939982471987606 16.326334381103514 0.49562338292598723 6.106236457824707 5058184 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016380760818719864 19.819691276550294 0.4473774045705795 6.630567216873169 5277610 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004772088239406003 17.638578796386717 0.36976905167102814 6.72165002822876 5497610 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015011422416137065 18.686065578460692 0.4235652178525925 6.946625423431397 5717610 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011798670602729544 21.62032279968262 0.322411322593689 7.709826803207397 5937001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001222664926899597 19.52532825469971 0.3190072774887085 7.572288560867309 6156986 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001052505028201267 23.568039321899413 0.33597028255462646 7.830760669708252 6376986 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007977559660503175 19.005212783813477 0.42062926292419434 7.934473133087158 6596986 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001816314330790192 15.565466594696044 0.46167523562908175 7.871967077255249 6816986 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011426282195316161 18.25077829360962 0.41426741182804105 8.012199449539185 7036986 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012320998561335728 22.242817687988282 0.4071377694606781 8.058978986740112 7256986 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008973057090770453 14.753656673431397 0.3694376289844513 8.161371421813964 7476986 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006504938341095112 22.744677925109862 0.3451414406299591 8.099602890014648 7696986 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012333638296695426 19.817075347900392 0.36769468784332277 8.235288095474242 7916986 0


Pure best response payoff estimated to be 139.85 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 83.17 seconds to finish estimate with resulting utilities: [184.685   3.005]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 137.02 seconds to finish estimate with resulting utilities: [133.36   54.495]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 136.97 seconds to finish estimate with resulting utilities: [45.295 47.56 ]
Computing meta_strategies
Exited RRD with total regret 9.152231471862649 that was less than regret lambda 9.285714285714286 after 43 iterations 
REGRET STEPS:  15
NEW LAMBDA 8.571428571428573
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    47.72           3.92           3.00      
    1    188.74          95.11          54.49      
    2    184.69          133.36          46.43      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    47.72          188.74          184.69      
    1     3.92          95.11          133.36      
    2     3.00          54.49          46.43      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    95.44          192.66          187.69      
    1    192.66          190.22          187.86      
    2    187.69          187.86          92.86      

 

Metagame probabilities: 
Player #0: 0.0117  0.3811  0.6072  
Player #1: 0.0117  0.3811  0.6072  
Iteration : 2
Time so far: 14654.696285247803
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-07-20 07:23:30.961731: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0189287343993783 97.97502670288085 0.35405596196651457 10.90329351425171 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029036068171262742 15.895193386077882 0.6097915530204773 10.193016052246094 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031191468797624113 26.805859375 0.6855973839759827 6.992736387252807 451000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.033760445564985274 22.71581268310547 0.7867448449134826 6.9213214874267575 671000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027622226998209952 21.77554931640625 0.7126940846443176 6.933125114440918 891000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027500389702618123 24.7399543762207 0.7418796956539154 6.9054749488830565 1111000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031785034388303754 19.7774377822876 0.9329333007335663 5.9051658630371096 1331000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02806351762264967 16.444863986968993 0.855982881784439 6.331609439849854 1551000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027378760650753974 15.980679607391357 0.9391109466552734 5.996891880035401 1771000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02292988244444132 16.60879316329956 0.9084467649459839 6.269049739837646 1990655 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02111358121037483 16.150545406341553 0.8900236845016479 6.202163410186768 2210347 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019114002585411072 17.59249391555786 0.8621688961982727 6.487399435043335 2428876 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017000562604516746 14.764941024780274 0.963488507270813 5.916906404495239 2646452 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013138559088110924 20.883189964294434 0.8333259463310242 6.041187524795532 2865578 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012291401345282792 14.61412534713745 0.9193158328533173 5.652123737335205 3082175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008086677361279725 21.591410446166993 0.8453722894191742 5.932609033584595 3302028 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006034745229408145 17.153089332580567 0.8003575503826141 6.523191165924072 3522028 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0026230084011331202 17.634249687194824 0.799918919801712 6.484267330169677 3741594 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001450031841523014 17.160530471801756 0.8270607471466065 6.101014232635498 3960654 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014300180919235572 19.24199562072754 0.7021822333335876 6.265760803222657 4180654 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -6.265592528507114e-05 15.867442798614501 0.48781571686267855 7.28786883354187 4400483 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005632376974972431 34.12306518554688 0.40035237073898317 7.8377861976623535 4620483 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0004197806985757779 24.74413642883301 0.4385200321674347 7.972925996780395 4840483 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001544238848146051 20.97894115447998 0.49133496582508085 7.815156126022339 5060061 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006446669751312584 19.29148769378662 0.3887871950864792 8.342167139053345 5280061 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006838688408606685 28.73811283111572 0.43695477247238157 7.928939867019653 5500061 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00040799404378049075 16.629210472106934 0.5070696860551834 8.369425964355468 5720061 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009111311354899953 16.11912069320679 0.4182356208562851 8.139449310302734 5939646 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004390938702272251 16.75856761932373 0.49802145957946775 7.945661878585815 6157547 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004940205166349187 15.177803230285644 0.38649033308029174 8.248952102661132 6376220 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014695612786454148 22.276015090942384 0.4101149022579193 7.423578453063965 6595949 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000863377035875601 21.808892250061035 0.36306257247924806 8.80015926361084 6815949 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009952614586836717 21.21159133911133 0.2987202122807503 8.47408151626587 7035949 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012019556044833735 18.61922245025635 0.27234155833721163 8.5204647064209 7255416 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00044441851059673354 17.70445508956909 0.3890493601560593 8.19217300415039 7475416 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006539006164530293 26.520061492919922 0.3578930050134659 8.561462116241454 7694979 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00035543204794521446 27.090998268127443 0.30641508996486666 8.819467449188233 7914628 0


Pure best response payoff estimated to be 98.33 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 83.9 seconds to finish estimate with resulting utilities: [162.675   2.355]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 140.23 seconds to finish estimate with resulting utilities: [124.66  47.4 ]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 139.69 seconds to finish estimate with resulting utilities: [81.31  79.225]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 139.22 seconds to finish estimate with resulting utilities: [52.47  51.465]
Computing meta_strategies
Exited RRD with total regret 8.492978054138206 that was less than regret lambda 8.571428571428573 after 51 iterations 
REGRET STEPS:  15
NEW LAMBDA 7.8571428571428585
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0    47.72           3.92           3.00           2.35      
    1    188.74          95.11          54.49          47.40      
    2    184.69          133.36          46.43          79.22      
    3    162.68          124.66          81.31          51.97      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0    47.72          188.74          184.69          162.68      
    1     3.92          95.11          133.36          124.66      
    2     3.00          54.49          46.43          81.31      
    3     2.35          47.40          79.22          51.97      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0    95.44          192.66          187.69          165.03      
    1    192.66          190.22          187.86          172.06      
    2    187.69          187.86          92.86          160.53      
    3    165.03          172.06          160.53          103.94      

 

Metagame probabilities: 
Player #0: 0.0053  0.174  0.4145  0.4063  
Player #1: 0.0053  0.174  0.4145  0.4063  
Iteration : 3
Time so far: 23477.55575466156
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-07-20 09:50:33.976432: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7779 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020956439152359962 74.81798400878907 0.4090998023748398 8.558326244354248 10627 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04150967970490456 15.346447849273682 0.8431544899940491 6.363237142562866 230627 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.037666996195912364 16.438421535491944 0.81547532081604 6.360690927505493 449972 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03479019422084093 15.714016532897949 0.8078148365020752 6.521762418746948 669411 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03355206660926342 17.77312307357788 0.8486155569553375 6.423409128189087 885128 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030015740543603897 17.355140686035156 0.8276511371135712 6.168346118927002 1101859 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02993921972811222 19.034854125976562 0.8729009211063385 5.9848462581634525 1317144 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03209838327020407 13.981418895721436 0.9690721511840821 5.828610038757324 1530508 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024133829958736898 18.106883811950684 0.82001091837883 6.201461124420166 1743863 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021788361854851244 23.207305908203125 0.8457001626491547 6.080637645721436 1961472 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015485578775405883 20.99235439300537 0.7063715517520904 6.828874778747559 2179005 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01539585441350937 20.655736923217773 0.7237308740615844 6.63035888671875 2393587 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013351294584572316 20.25433769226074 0.6962958157062531 6.900954055786133 2610734 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012680371198803187 19.996213531494142 0.7508513450622558 6.79764347076416 2825432 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013236504420638085 16.42617235183716 0.8479281604290009 6.373281335830688 3043670 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009599939943291247 15.409575462341309 0.8073313236236572 6.436503934860229 3257006 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00756988471839577 23.467977142333986 0.7229728817939758 6.540959405899048 3471556 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006185731361620128 18.67221450805664 0.7919404745101929 6.316172790527344 3687772 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0047257678816094995 14.658587646484374 0.7009430170059204 6.853456449508667 3903645 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0035047386423684656 22.562398529052736 0.683153110742569 7.0050225257873535 4121418 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0035616137785837053 25.2028865814209 0.6093839347362519 7.379765415191651 4337561 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012382121502014342 19.928007507324217 0.6165020942687989 7.134859466552735 4554187 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001073475023440551 19.200119972229004 0.6860331177711487 6.753898096084595 4771580 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003737653495045379 17.129560470581055 0.6177295088768006 7.150116491317749 4986621 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010731697595474543 15.240759372711182 0.6308726131916046 7.17910418510437 5202848 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016686229180777445 21.27703266143799 0.5804420530796051 7.187562561035156 5420230 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003733520698733628 18.30247278213501 0.6133576452732086 7.112566900253296 5636391 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00013819439482176676 17.916168212890625 0.5060304343700409 7.651746797561645 5855206 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00011971659550908953 20.629604148864747 0.5792428612709045 7.044835567474365 6074770 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011527776732691564 19.397010231018065 0.4955785691738129 7.914937829971313 6292978 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0031757742748595775 19.92758560180664 0.5180389106273651 8.048891258239745 6509733 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006562955022673123 22.222359466552735 0.5740655720233917 7.479309749603272 6726325 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0001712016306555597 15.9594557762146 0.5389685600996017 7.771826457977295 6945085 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007939693750813603 23.80626697540283 0.6014951169490814 7.52225694656372 7161703 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00011980613635387272 13.741996002197265 0.5653105616569519 7.2459570407867435 7380357 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004903538431972265 23.4837947845459 0.4995288670063019 7.988601875305176 7597809 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019059360754908993 14.104016590118409 0.5312855064868927 7.646376323699951 7817012 0


Pure best response payoff estimated to be 87.095 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 77.96 seconds to finish estimate with resulting utilities: [140.97    2.555]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 141.51 seconds to finish estimate with resulting utilities: [117.925  44.77 ]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 146.28 seconds to finish estimate with resulting utilities: [75.78  63.805]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 144.48 seconds to finish estimate with resulting utilities: [82.49  71.165]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 139.68 seconds to finish estimate with resulting utilities: [70.17 73.46]
Computing meta_strategies
Exited RRD with total regret 7.828905055671555 that was less than regret lambda 7.8571428571428585 after 137 iterations 
REGRET STEPS:  15
NEW LAMBDA 7.142857142857144
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0    47.72           3.92           3.00           2.35           2.56      
    1    188.74          95.11          54.49          47.40          44.77      
    2    184.69          133.36          46.43          79.22          63.80      
    3    162.68          124.66          81.31          51.97          71.17      
    4    140.97          117.92          75.78          82.49          71.81      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0    47.72          188.74          184.69          162.68          140.97      
    1     3.92          95.11          133.36          124.66          117.92      
    2     3.00          54.49          46.43          81.31          75.78      
    3     2.35          47.40          79.22          51.97          82.49      
    4     2.56          44.77          63.80          71.17          71.81      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0    95.44          192.66          187.69          165.03          143.53      
    1    192.66          190.22          187.86          172.06          162.69      
    2    187.69          187.86          92.86          160.53          139.59      
    3    165.03          172.06          160.53          103.94          153.66      
    4    143.53          162.69          139.59          153.66          143.63      

 

Metagame probabilities: 
Player #0: 0.0001  0.0154  0.1745  0.2476  0.5623  
Player #1: 0.0001  0.0154  0.1745  0.2476  0.5623  
Iteration : 4
Time so far: 32714.483340263367
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-07-20 12:24:31.495059: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9757 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04333615452051163 58.746893310546874 0.594833517074585 7.954033374786377 10772 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03820645101368427 14.74965705871582 0.7962413251399993 6.912595415115357 225375 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04282790906727314 24.55865020751953 0.8308133840560913 6.484308433532715 438023 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03451617155224085 16.47385368347168 0.7416149854660035 7.047269344329834 649969 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03235901556909084 23.48103904724121 0.6839624345302582 7.3527435779571535 859680 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.036271491460502146 13.126380443572998 0.9326522767543792 5.7527184009552 1071596 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03254921678453684 16.39969491958618 0.8401361584663392 6.275939178466797 1280977 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02817139569669962 16.75281286239624 0.8545686483383179 6.357658863067627 1490022 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024922995641827584 15.802313709259034 0.7656093955039978 6.811638212203979 1700159 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023411970771849155 24.525541877746583 0.674411016702652 7.048225784301758 1910482 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01746281236410141 12.706728744506837 0.7261959850788117 6.647402334213257 2118368 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022193479351699352 16.7336443901062 0.8724893748760223 5.895046710968018 2328469 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017897882126271726 12.652534198760986 0.8616823673248291 5.904856538772583 2537422 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014356163702905178 15.701997756958008 0.8464548408985137 6.040147304534912 2747206 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01219546627253294 23.30116424560547 0.7374601900577545 6.972345495223999 2955262 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011111177317798138 13.802578926086426 0.8073844015598297 6.330159044265747 3163884 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007325702393427491 21.780130004882814 0.6219186961650849 7.472952604293823 3371413 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004666013573296368 22.093393898010255 0.7243613719940185 6.950481939315796 3581776 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004015128978062421 19.937079429626465 0.6755157351493836 7.261156415939331 3791307 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0035112572135403753 17.637617206573488 0.6941587150096893 7.1411536693572994 4000867 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00258410659735091 23.790648460388184 0.6733949840068817 7.499729919433594 4210240 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018802799022523686 21.191028594970703 0.5574169158935547 7.944632530212402 4419536 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003420842940977309 22.220353889465333 0.6486171662807465 7.4709906578063965 4630175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013675438705831767 17.102266788482666 0.6478344738483429 7.64222674369812 4841284 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004334955193917267 18.2236216545105 0.6081762611865997 8.198018217086792 5050150 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0027297202090267093 18.4206262588501 0.5681324005126953 8.177907133102417 5258669 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0001266783059691079 25.73027992248535 0.5753474235534668 8.560402584075927 5466967 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001045618000352988 15.344566059112548 0.6683535873889923 7.808216762542725 5678711 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003223851032089442 14.289210033416747 0.3195696771144867 8.453016567230225 5889757 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00028698243404505773 17.02804117202759 0.09457696229219437 9.803550720214844 6097153 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004565561661729589 17.5352424621582 0.23167464584112168 10.029165554046632 6305966 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003655863273888826 16.509815788269044 0.19769034236669542 9.884530925750733 6515018 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010404170941910706 12.710684680938721 0.09291448295116425 10.937384796142577 6722836 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003175339828885626 18.019958305358887 0.08540269657969475 11.469070148468017 6933537 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009339200747490395 20.724463081359865 0.07459038645029067 12.030140018463134 7142531 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00021245120296953246 23.971934127807618 0.05970752388238907 12.197768592834473 7353229 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015447206795215606 13.589791584014893 0.1131265103816986 11.526919937133789 7563402 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009965718272724189 25.968951797485353 0.0563128262758255 12.332762432098388 7773015 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001273902168031782 25.00907783508301 0.06913094483315944 12.340858268737794 7981513 0
Recovering previous policy with expected return of 74.12437810945273. Long term value was 67.986 and short term was 66.53.


Pure best response payoff estimated to be 74.175 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 80.34 seconds to finish estimate with resulting utilities: [141.815   2.965]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 141.29 seconds to finish estimate with resulting utilities: [116.295  45.955]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 144.51 seconds to finish estimate with resulting utilities: [75.835 60.55 ]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 141.83 seconds to finish estimate with resulting utilities: [80.74 69.91]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 138.12 seconds to finish estimate with resulting utilities: [71.85  73.095]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 138.19 seconds to finish estimate with resulting utilities: [72.86 74.52]
Computing meta_strategies
Exited RRD with total regret 7.092869538406916 that was less than regret lambda 7.142857142857144 after 97 iterations 
REGRET STEPS:  15
NEW LAMBDA 6.42857142857143
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0    47.72           3.92           3.00           2.35           2.56           2.96      
    1    188.74          95.11          54.49          47.40          44.77          45.95      
    2    184.69          133.36          46.43          79.22          63.80          60.55      
    3    162.68          124.66          81.31          51.97          71.17          69.91      
    4    140.97          117.92          75.78          82.49          71.81          73.09      
    5    141.81          116.30          75.83          80.74          71.85          73.69      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0    47.72          188.74          184.69          162.68          140.97          141.81      
    1     3.92          95.11          133.36          124.66          117.92          116.30      
    2     3.00          54.49          46.43          81.31          75.78          75.83      
    3     2.35          47.40          79.22          51.97          82.49          80.74      
    4     2.56          44.77          63.80          71.17          71.81          71.85      
    5     2.96          45.95          60.55          69.91          73.09          73.69      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0    95.44          192.66          187.69          165.03          143.53          144.78      
    1    192.66          190.22          187.86          172.06          162.69          162.25      
    2    187.69          187.86          92.86          160.53          139.59          136.38      
    3    165.03          172.06          160.53          103.94          153.66          150.65      
    4    143.53          162.69          139.59          153.66          143.63          144.94      
    5    144.78          162.25          136.38          150.65          144.94          147.38      

 

Metagame probabilities: 
Player #0: 0.0001  0.0262  0.1395  0.1973  0.3233  0.3135  
Player #1: 0.0001  0.0262  0.1395  0.1973  0.3233  0.3135  
Iteration : 5
Time so far: 43010.60375952721
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-07-20 15:16:08.834903: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11735 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.039877574890851974 60.7274974822998 0.5710562527179718 7.720657014846802 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.036338198371231555 15.734660243988037 0.7378948092460632 7.071706914901734 224662 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.036974174156785014 17.28268184661865 0.7653416156768799 6.897173929214477 431586 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032144587859511374 30.942999267578124 0.6470738708972931 7.417613983154297 640480 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03238327726721764 12.452858829498291 0.8115069091320037 7.034541606903076 848423 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030968823470175266 27.652452087402345 0.7076493203639984 7.4813148021698 1057371 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029016929119825362 14.708580112457275 0.8179470121860504 6.218061542510986 1267568 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03239915780723095 25.68963565826416 0.7675994873046875 6.4486504077911375 1476988 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025965096428990364 15.690539169311524 0.6906825125217437 6.903927659988403 1685804 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024226987175643443 15.77178258895874 0.7728663980960846 6.262450408935547 1893998 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02703513391315937 17.14127254486084 0.8118853688240051 6.0858564376831055 2103569 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022805327363312245 18.627131080627443 0.794399094581604 6.41974720954895 2314529 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014309883024543524 16.217417335510255 0.5651319742202758 7.487747240066528 2523879 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018653644435107707 13.356204223632812 0.6690618634223938 6.960719585418701 2733342 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010363916819915175 27.0390474319458 0.6806637465953826 6.81419792175293 2944776 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010392621625214815 12.388579273223877 0.844085419178009 5.788188171386719 3151725 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006180302333086729 21.118362045288087 0.5778868734836579 7.388019943237305 3363038 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009953594440594316 15.850145244598389 0.8040765941143035 5.990005350112915 3571431 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007615906232967973 17.835202598571776 0.6530313789844513 6.776495504379272 3780265 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008152203005738556 16.182460498809814 0.7515279471874237 6.180952692031861 3988795 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016588978091022 13.841186046600342 0.7131204545497895 6.495139217376709 4195810 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007224445440806448 26.339633750915528 0.5406785607337952 7.810995388031006 4403514 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002502838522195816 20.13889961242676 0.6352174580097198 7.016816234588623 4613274 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0008980291735497303 21.929304695129396 0.4731958836317062 7.611797285079956 4822980 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007225073146400973 31.721281242370605 0.12674254849553107 8.291324377059937 5031195 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00023090279719326646 35.649933433532716 0.09439737275242806 9.704744148254395 5241359 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018467078974936158 13.979693698883057 0.18371750116348268 8.450924110412597 5450719 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007102558163751382 15.026436042785644 0.09845606535673142 8.627803421020507 5658641 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008188823657110333 24.7320068359375 0.06280412450432778 10.345571899414063 5867248 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010487567589734681 23.449151229858398 0.07882533371448516 10.019834613800048 6076602 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005647275931551122 29.510038948059083 0.07250147312879562 10.59681577682495 6286206 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00178833647587453 19.911039352416992 0.20119140595197677 10.545627975463868 6497109 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003082550178078236 23.912520790100096 0.061537787690758704 10.968433856964111 6707555 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004517789842793718 23.798786163330078 0.06062088385224342 10.983954524993896 6917628 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010619233784382232 17.737079429626466 0.07069557048380375 10.829911041259766 7126768 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004506233766733203 29.940576553344727 0.04683598093688488 11.670711612701416 7339002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006309984586550854 22.35555057525635 0.05763551406562328 11.286155796051025 7547802 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00013251248292363017 18.631383323669432 0.14913495630025864 10.848278045654297 7756494 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003405373718123883 12.803639316558838 0.22585399597883224 10.89458065032959 7965949 0
Recovering previous policy with expected return of 71.26865671641791. Long term value was 71.03 and short term was 72.78.


Pure best response payoff estimated to be 78.545 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 79.31 seconds to finish estimate with resulting utilities: [145.06    2.925]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 137.67 seconds to finish estimate with resulting utilities: [114.21  46.16]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 142.55 seconds to finish estimate with resulting utilities: [76.735 64.025]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 143.16 seconds to finish estimate with resulting utilities: [81.135 70.87 ]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 139.6 seconds to finish estimate with resulting utilities: [72.88  71.495]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 139.83 seconds to finish estimate with resulting utilities: [73.12  76.105]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 134.43 seconds to finish estimate with resulting utilities: [71.925 68.7  ]
Computing meta_strategies
Exited RRD with total regret 6.406550992208082 that was less than regret lambda 6.42857142857143 after 94 iterations 
REGRET STEPS:  15
NEW LAMBDA 5.714285714285715
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    47.72           3.92           3.00           2.35           2.56           2.96           2.92      
    1    188.74          95.11          54.49          47.40          44.77          45.95          46.16      
    2    184.69          133.36          46.43          79.22          63.80          60.55          64.03      
    3    162.68          124.66          81.31          51.97          71.17          69.91          70.87      
    4    140.97          117.92          75.78          82.49          71.81          73.09          71.50      
    5    141.81          116.30          75.83          80.74          71.85          73.69          76.11      
    6    145.06          114.21          76.73          81.14          72.88          73.12          70.31      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0    47.72          188.74          184.69          162.68          140.97          141.81          145.06      
    1     3.92          95.11          133.36          124.66          117.92          116.30          114.21      
    2     3.00          54.49          46.43          81.31          75.78          75.83          76.73      
    3     2.35          47.40          79.22          51.97          82.49          80.74          81.14      
    4     2.56          44.77          63.80          71.17          71.81          71.85          72.88      
    5     2.96          45.95          60.55          69.91          73.09          73.69          73.12      
    6     2.92          46.16          64.03          70.87          71.50          76.11          70.31      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0    95.44          192.66          187.69          165.03          143.53          144.78          147.99      
    1    192.66          190.22          187.86          172.06          162.69          162.25          160.37      
    2    187.69          187.86          92.86          160.53          139.59          136.38          140.76      
    3    165.03          172.06          160.53          103.94          153.66          150.65          152.00      
    4    143.53          162.69          139.59          153.66          143.63          144.94          144.38      
    5    144.78          162.25          136.38          150.65          144.94          147.38          149.23      
    6    147.99          160.37          140.76          152.00          144.38          149.23          140.62      

 

Metagame probabilities: 
Player #0: 0.0001  0.0208  0.1077  0.1577  0.2351  0.2498  0.2288  
Player #1: 0.0001  0.0208  0.1077  0.1577  0.2351  0.2498  0.2288  
Iteration : 6
Time so far: 53043.82474708557
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-07-20 18:03:21.668372: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13713 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029398589208722115 56.88611335754395 0.45768418312072756 8.976349925994873 10726 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03817278183996677 19.66364803314209 0.7699525952339172 6.7119598388671875 221309 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04207537584006786 16.316959190368653 0.8152005612850189 6.35640139579773 430301 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03186444733291864 18.194252681732177 0.7134761095046998 6.905013799667358 639359 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04001119956374168 13.51108522415161 0.8927479982376099 5.718790578842163 846108 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027111549489200117 16.920703983306886 0.6116662561893463 7.453373670578003 1054266 0
Fatal Python error: Segmentation fault

Current thread 0x00001539af41cb80 (most recent call first):
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/rl_environment.py", line 330 in step
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/rl_oracle_cooperative.py", line 481 in sample_episode
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/rl_oracle.py", line 223 in _rollout
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/rl_oracle_cooperative.py", line 190 in __call__
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/psro_v2.py", line 413 in update_agents
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/algorithms/psro_v2/abstract_meta_trainer.py", line 201 in iteration
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 403 in gpsro_looper
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 482 in main
  File "/home/anrigu/.local/lib/python3.10/site-packages/absl/app.py", line 254 in _run_main
  File "/home/anrigu/.local/lib/python3.10/site-packages/absl/app.py", line 308 in run
  File "/home/anrigu/srg_research/open_spiel/open_spiel/python/examples/psro_v2_simultaneous.py", line 485 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, google._upb._message, tensorflow.python.framework.fast_tensor_util, charset_normalizer.md, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, PIL._imaging, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.tslib, pandas._libs.lib, pandas._libs.hashing, pandas._libs.ops, pandas._libs.arrays, pandas._libs.index, pandas._libs.join, pandas._libs.sparse, pandas._libs.reduction, pandas._libs.indexing, pandas._libs.internals, pandas._libs.writers, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.tslibs.strptime, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.json, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label, cvxopt.base, cvxopt.blas, cvxopt.lapack (total: 117)
/var/spool/slurmd.spool/job56105501/slurm_script: line 34: 2572038 Segmentation fault      (core dumped) python3 -u psro_v2_simultaneous.py --game_name=harvest --save_folder_path="../examples/data/harvest/vanilla_ppo_third/($trial)_$parameter_id" --meta_strategy_method=prd_collab --gpsro_iterations=30 --consensus_imitation=False --regret_lambda_init=$curr_lambda --regret_steps=$curr_steps --policy_constraint_steps=20 --joint_action=False --rewards_joint=False --fine_tune=True --clear_trajectories=False --consensus_hidden_layer_size=50 --consensus_n_hidden_layers=2 --min_buffer_size_fine_tune=10000 --epochs_ppo=10 --minibatches_ppo=50 --number_training_steps=8000000 --symmetric_game=True --sims_per_entry=200 --eps_clip=.1 --eps_clip_value=.5 --entropy_decay_duration=.5 --ppo_entropy=.05 --fine_tune_policy_lr=.00003 --fine_tune_value_lr=.0003 --transfer_policy_minimum_entropy=0 --recovery_window=200 --hidden_layer_size=50 --n_hidden_layers=2 --batch_size=64 --dqn_learning_rate=3e-4 --update_target_network_every=500 --learn_every=1 --max_buffer_size=100000 --epsilon_decay_duration=100000 --min_buffer_size_to_learn=20000 --regret_calculation_steps=1500000
