Job Id listed below:
57062230

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

/var/spool/slurmd.spool/job57062230/slurm_script: line 29: regret_lambda: command not found
/var/spool/slurmd.spool/job57062230/slurm_script: line 30: regret_steps: command not found
2023-08-02 12:24:59.255793: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-02 12:25:01.562577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0802 12:25:06.593178 22647747361664 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x1498cdb0ad70>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x1498cdb0ad70>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-08-02 12:25:06.925293: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-08-02 12:25:07.248801: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.86 seconds to finish estimate with resulting utilities: [49.06 49.3 ]
Exited RRD with total regret 0.0 that was less than regret lambda 2.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    49.18      

 

Player 1 Payoff matrix: 

           0      
    0    49.18      

 

Social Welfare Sum Matrix: 

           0      
    0    98.36      

 

Iteration : 0
Time so far: 0.0001766681671142578
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-08-02 12:25:27.018105: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.11912185996770859 30.34398193359375 2.0552915573120116 0.001350230094976723 10275 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09963979348540306 16.990786361694337 1.8907910108566284 0.2859315872192383 215368 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09297829419374466 14.2937801361084 1.848197305202484 0.46210253834724424 417587 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0879858247935772 14.240877056121827 1.8410989880561828 0.6030185580253601 619361 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07160924300551415 15.80092544555664 1.7993352651596068 0.7770964503288269 821648 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07740032896399499 18.071253967285156 1.7811595916748046 0.8528138816356658 1024217 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07039671912789344 15.809423732757569 1.7407282710075378 1.079287165403366 1226587 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06423694528639316 16.70728998184204 1.7076476573944093 1.0712506949901581 1429176 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05333780087530613 19.191650390625 1.7061096549034118 1.2691639542579651 1634798 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04770447239279747 21.333001708984376 1.665325689315796 1.3388150095939637 1838936 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04277972877025604 20.65264148712158 1.5691734433174134 1.6149190545082093 2042731 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03438153900206089 23.51103267669678 1.5002321600914001 1.7149614095687866 2248436 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029790832102298735 19.967629051208498 1.4565441966056825 1.920735275745392 2455353 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025839623436331748 18.663433265686034 1.398801040649414 1.9929666876792909 2665394 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02217067889869213 18.058028316497804 1.311272633075714 2.162206196784973 2874075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016543182265013456 20.49724359512329 1.2759594321250916 2.253286051750183 3082862 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013114385027438401 22.40265941619873 1.1857999801635741 2.5130589246749877 3291342 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010159308323636651 27.221159362792967 1.1291383981704712 2.6159363746643067 3503191 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005098356655798853 21.236879348754883 1.033366507291794 2.784644675254822 3714644 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003910214506322518 25.561535835266113 0.9546212434768677 3.03862783908844 3927952 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007056597052724101 24.2329309463501 0.9019534230232239 3.284452533721924 4142384 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0005621675794827752 28.08154582977295 0.8345157861709595 3.6056438207626345 4358435 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009719673078507185 29.344717025756836 0.7919636011123657 3.9691240549087525 4576227 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00137739828624035 25.274433898925782 0.7051042556762696 4.25983829498291 4792248 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002006961265578866 23.15996551513672 0.6788132548332214 4.629851245880127 5005634 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009076249552890658 26.51719436645508 0.597974693775177 4.950729465484619 5221171 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015379070915514602 25.41027698516846 0.6038101196289063 5.161555528640747 5437720 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004972093622200191 24.156464195251466 0.5483373820781707 5.580190229415893 5654391 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005944050775724463 29.27304172515869 0.503588604927063 6.1335334300994875 5871782 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010929890180705116 21.527042579650878 0.5296929985284805 6.400787687301635 6090471 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007494737481465563 27.416150283813476 0.5000391572713851 6.666055679321289 6307743 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001601137241232209 22.051604652404784 0.44696937799453734 7.016349267959595 6524365 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00011857937788590789 21.172592639923096 0.44139376282691956 6.89478816986084 6742958 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004713721165899187 20.06074724197388 0.40797916054725647 7.261966562271118 6960465 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016519198936293833 20.509756660461427 0.39771721959114076 7.272894620895386 7176939 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007550375077698846 26.71712875366211 0.3519000202417374 7.705752992630005 7396049 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008777892042417079 30.887086486816408 0.37322755753993986 7.540014457702637 7614170 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010436824493808671 30.721413612365723 0.38749091029167176 7.602483463287354 7830987 0


Pure best response payoff estimated to be 191.62 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 79.7 seconds to finish estimate with resulting utilities: [186.995   4.555]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 132.43 seconds to finish estimate with resulting utilities: [96.165 95.045]
Computing meta_strategies
Exited RRD with total regret 1.8918891161216038 that was less than regret lambda 2.0 after 44 iterations 
NEW LAMBDA 1.9310344827586208
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    49.18           4.55      
    1    187.00          95.61      

 

Player 1 Payoff matrix: 

           0              1      
    0    49.18          187.00      
    1     4.55          95.61      

 

Social Welfare Sum Matrix: 

           0              1      
    0    98.36          191.55      
    1    191.55          191.21      

 

Metagame probabilities: 
Player #0: 0.0103  0.9897  
Player #1: 0.0103  0.9897  
Iteration : 1
Time so far: 6360.53252863884
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-08-02 14:11:27.682531: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027320621535182 88.77639312744141 0.5298903405666351 8.309818363189697 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03803786300122738 14.898905849456787 0.7912994682788849 6.434979677200317 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03336429875344038 16.808877563476564 0.7255551993846894 6.429040622711182 449223 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030801526084542275 18.79292459487915 0.7359154462814331 5.957890558242798 667317 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03459855336695909 21.86829242706299 0.8522801876068116 5.366252088546753 881481 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03299898561090231 17.132743644714356 0.9006285429000854 5.10342173576355 1094495 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03463988602161407 17.768982696533204 0.9733736932277679 4.892981767654419 1306107 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03278116788715124 15.475702381134033 0.9810912847518921 5.014346933364868 1518475 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027896152436733247 18.941311073303222 0.9187030673027039 5.011208200454712 1731340 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02456280551850796 18.633308982849123 0.9432847678661347 4.448036050796508 1942749 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02274666093289852 14.37882595062256 0.9409691989421844 4.716554832458496 2156883 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018206258676946162 24.22312183380127 0.8216596424579621 5.458134889602661 2366933 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01751142889261246 19.276928997039796 0.8896818697452545 4.944707822799683 2578338 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012156633008271456 23.656302642822265 0.7987002909183503 5.890250158309937 2794013 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010576181672513485 19.677278137207033 0.7405943810939789 6.1447070121765135 3006682 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009734961437061429 22.326066780090333 0.8229226648807526 5.360269069671631 3218965 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006279845023527741 17.79178056716919 0.8309541046619415 5.149720621109009 3430235 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005068184877745807 17.934577560424806 0.7753900647163391 5.3940221786499025 3639279 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002348084858385846 23.518047523498534 0.7579556167125702 5.417088556289673 3851722 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008656027897814056 19.890176582336426 0.6431757152080536 6.479348516464233 4062117 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -9.835350065259262e-05 16.949499130249023 0.6952258169651031 6.099859046936035 4273917 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001054254402697552 20.959272575378417 0.6083310604095459 6.232612466812133 4487598 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004889502946753055 22.4570161819458 0.5701565384864807 6.1208329677581785 4702792 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00010878826287807897 19.874374008178712 0.41505524814128875 6.7705726146698 4913977 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012007320176053327 16.778752040863036 0.3880471795797348 6.738151407241821 5128238 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.000401701359442086 21.519394302368163 0.2868741601705551 7.138013410568237 5341886 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0003677720258565387 26.260226440429687 0.3399323016405106 7.087790012359619 5556581 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009627507050936401 16.657569122314452 0.3801549166440964 7.531780338287353 5772175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013133324689988512 20.692269706726073 0.328947988152504 7.81037163734436 5988335 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000947989740234334 19.73683567047119 0.2970470294356346 8.353409481048583 6201542 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015184901210886891 16.142862129211426 0.31983087956905365 8.329315376281738 6418339 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005486044814460911 26.253801918029787 0.2793867811560631 8.80599308013916 6632349 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004647244073566981 23.204926109313966 0.32242544591426847 9.02262086868286 6846947 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00034691446635406467 23.410207748413086 0.34190188348293304 8.680307292938233 7061630 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010345869628508807 17.58961706161499 0.3158226996660233 8.847629642486572 7277272 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001164725660964905 17.178057384490966 0.3065145879983902 9.107921886444093 7493356 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005193534845602699 20.98742618560791 0.2845178246498108 9.40720796585083 7707263 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00035796159645542505 23.19146156311035 0.2994385451078415 9.40186710357666 7922146 0


Pure best response payoff estimated to be 121.135 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 78.71 seconds to finish estimate with resulting utilities: [171.065   2.845]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 132.18 seconds to finish estimate with resulting utilities: [121.28   56.035]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 130.9 seconds to finish estimate with resulting utilities: [53.365 52.17 ]
Computing meta_strategies
Exited RRD with total regret 1.9163101042171036 that was less than regret lambda 1.9310344827586208 after 177 iterations 
NEW LAMBDA 1.8620689655172415
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    49.18           4.55           2.85      
    1    187.00          95.61          56.03      
    2    171.06          121.28          52.77      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    49.18          187.00          171.06      
    1     4.55          95.61          121.28      
    2     2.85          56.03          52.77      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    98.36          191.55          173.91      
    1    191.55          191.21          177.31      
    2    173.91          177.31          105.53      

 

Metagame probabilities: 
Player #0: 0.0001  0.2464  0.7535  
Player #1: 0.0001  0.2464  0.7535  
Iteration : 2
Time so far: 15228.416202068329
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 16:39:15.646879: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012859755754470825 54.05239067077637 0.24366656839847564 11.505526161193847 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025115572847425938 16.48044948577881 0.5235613763332367 9.856452655792236 223063 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028026921302080156 13.124231243133545 0.6174354076385498 9.518386554718017 433360 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03050678987056017 15.428263282775879 0.7207230389118194 6.594524240493774 645590 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03025091774761677 16.628275203704835 0.7241326808929444 6.40508394241333 852635 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026065793819725513 21.28609848022461 0.6777143716812134 6.746068906784058 1061151 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025347560457885266 18.469422912597658 0.710167920589447 5.970240688323974 1273900 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02399636376649141 15.98095178604126 0.7328465104103088 6.1154194355010985 1485536 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02275519985705614 15.017687892913818 0.7654100656509399 5.934123945236206 1696906 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01990816034376621 17.495524215698243 0.7348923742771148 6.313370847702027 1907567 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01702216174453497 20.876231575012206 0.6973992824554444 6.3612096309661865 2119401 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016397357918322086 17.616187000274657 0.7407750010490417 5.7693524837493895 2332317 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01520246807485819 20.369957733154298 0.7576781928539276 5.896895217895508 2541249 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01139033744111657 18.374248886108397 0.6854590713977814 6.419414806365967 2752199 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01078960313461721 17.545874500274657 0.7477183222770691 6.184246397018432 2964290 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008360340725630522 12.071577835083009 0.7388378083705902 6.146940088272094 3174537 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007035129982978105 17.052521896362304 0.8020786464214325 5.7944437026977536 3385992 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004571042209863663 13.333569049835205 0.7205767691135406 6.35503740310669 3598072 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015158874681219458 16.193111991882326 0.621169438958168 6.7391712188720705 3808892 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005326377402525395 22.107142066955568 0.6073502242565155 7.427186346054077 4020127 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002985893639561255 17.869327354431153 0.5591693818569183 7.2977231502532955 4233965 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005353332118829713 15.452808284759522 0.5087127268314362 7.73324556350708 4447720 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004415334229634027 16.532722663879394 0.41229094862937926 8.946791648864746 4655678 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004788137346622534 25.04777297973633 0.36821856200695036 9.953367233276367 4868395 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001028915200731717 11.125403785705567 0.342881578207016 10.32870626449585 5080021 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006748788000550121 16.148786735534667 0.47147197723388673 9.77998390197754 5291010 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  2.4607451086922084e-05 28.23863277435303 0.3567775636911392 10.638542366027831 5503993 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005306705450493609 15.291641139984131 0.4534758448600769 9.854251956939697 5716647 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013235868929768913 17.330385398864745 0.31272167563438413 10.438453483581544 5930242 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017221372210769915 18.905138969421387 0.434744256734848 10.163855457305909 6141285 0
