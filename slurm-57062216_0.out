Job Id listed below:
57062218

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

/var/spool/slurmd.spool/job57062218/slurm_script: line 29: regret_lambda: command not found
/var/spool/slurmd.spool/job57062218/slurm_script: line 30: regret_steps: command not found
2023-08-02 12:24:26.425222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-02 12:24:30.195233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0802 12:24:35.260468 23109935184768 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x15046a3b2d70>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x15046a3b2d70>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-08-02 12:24:35.573076: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-08-02 12:24:35.860706: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.86 seconds to finish estimate with resulting utilities: [49.005 48.625]
Exited RRD with total regret 0.0 that was less than regret lambda 5.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    48.81      

 

Player 1 Payoff matrix: 

           0      
    0    48.81      

 

Social Welfare Sum Matrix: 

           0      
    0    97.63      

 

Iteration : 0
Time so far: 0.00018906593322753906
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-08-02 12:24:55.675643: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.117066989839077 21.98373622894287 2.064823031425476 0.0020060703449416907 10459 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.10013908296823501 15.7943190574646 1.882067823410034 0.26142660081386565 216207 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09197780713438988 15.91637487411499 1.8334620237350463 0.35099012553691866 418566 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0856009803712368 14.609114170074463 1.807456409931183 0.46011169254779816 621094 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07496018707752228 15.24201135635376 1.747441327571869 0.585078901052475 822903 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07514391914010048 17.77857427597046 1.7041005611419677 0.7201933026313782 1025090 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06568529643118382 12.866762351989745 1.679207456111908 0.828800505399704 1227201 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.062399853765964505 15.736966514587403 1.6475680470466614 0.966655147075653 1429880 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.053482086211442945 18.733816814422607 1.5791717648506165 1.12779700756073 1634845 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04745711386203766 20.378823471069335 1.5353400588035584 1.2069349765777588 1840367 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04092523865401745 19.2446569442749 1.5198724150657654 1.4236731052398681 2047873 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03434057626873255 20.06502628326416 1.4711321473121644 1.4053145408630372 2254628 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028831534460186957 17.662299633026123 1.3880620360374452 1.585538423061371 2463213 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.023690139502286912 20.960437202453612 1.333885908126831 1.8170376062393188 2671350 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019182297959923746 21.859119415283203 1.3157634258270263 1.9506685853004455 2881692 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016915304865688086 24.451824188232422 1.2910674214363098 2.0217519998550415 3090972 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011256649903953076 20.184090232849123 1.2176458477973937 2.2112142562866213 3300102 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007328133936971426 22.340667915344238 1.0889097929000855 2.5172189712524413 3510790 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005054440104868263 24.417558670043945 0.9835302472114563 2.8458976984024047 3723424 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005574042093940079 23.95348358154297 0.9446202039718627 3.09360818862915 3931798 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013325010862899945 23.80506076812744 0.8195477783679962 3.6239253520965575 4147421 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015909067878965288 22.850735664367676 0.7230722069740295 3.941967415809631 4365696 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008861005364451558 26.892942810058592 0.6940341591835022 3.9990662336349487 4582000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013048481778241694 29.20028896331787 0.6259544730186463 4.441403436660766 4797428 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012464447354432195 21.69104118347168 0.5737459599971771 4.834430074691772 5015557 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000815935293212533 21.311515045166015 0.5636784613132477 5.049079561233521 5233117 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001239225063181948 22.17310276031494 0.534975665807724 5.335504388809204 5449502 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001920109917409718 18.40571117401123 0.5187863111495972 5.53004035949707 5667840 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011439468362368643 24.070287704467773 0.5253095388412475 5.640914344787598 5885603 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017944282677490263 25.058061408996583 0.49834088385105135 5.966567659378052 6104626 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013865300570614636 21.81334171295166 0.47818570733070376 6.131409549713135 6322878 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010167937172809616 27.023901748657227 0.45218115746974946 6.486998081207275 6541235 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013507732684956864 26.41497669219971 0.45003490149974823 6.455041551589966 6759301 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010026560357800918 20.061325454711913 0.4375389814376831 6.740878248214722 6978305 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00019469996477710082 23.19531078338623 0.44624011516571044 6.966356801986694 7195380 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001889283389755292 20.483646202087403 0.4244940310716629 7.121048831939698 7413532 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007947537000291049 23.890193367004393 0.41816380321979524 7.031223201751709 7629651 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008056352842686465 23.226444053649903 0.38539834320545197 7.669282293319702 7847624 0


Pure best response payoff estimated to be 192.405 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 81.8 seconds to finish estimate with resulting utilities: [184.06    4.535]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 136.66 seconds to finish estimate with resulting utilities: [89.795 88.845]
Computing meta_strategies
Exited RRD with total regret 4.65783502120081 that was less than regret lambda 5.0 after 36 iterations 
NEW LAMBDA 4.827586206896552
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    48.81           4.54      
    1    184.06          89.32      

 

Player 1 Payoff matrix: 

           0              1      
    0    48.81          184.06      
    1     4.54          89.32      

 

Social Welfare Sum Matrix: 

           0              1      
    0    97.63          188.59      
    1    188.59          178.64      

 

Metagame probabilities: 
Player #0: 0.027  0.973  
Player #1: 0.027  0.973  
Iteration : 1
Time so far: 6966.342760324478
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-08-02 14:21:02.188571: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02776289898902178 73.77808837890625 0.5452918529510498 8.679411792755127 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032300396263599394 18.54680347442627 0.6521579623222351 7.432493782043457 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03552438132464886 16.320708656311034 0.7594760358333588 6.458740949630737 450996 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.036035075597465036 19.507460403442384 0.8293938934803009 6.088156700134277 670804 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03452797792851925 19.693812942504884 0.8450668513774872 5.652117490768433 888662 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03352463226765394 15.735860252380371 0.8846091508865357 5.423883390426636 1104505 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02875281199812889 24.224324417114257 0.8375293910503387 5.827743673324585 1322091 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02828271482139826 20.970421600341798 0.9044148683547973 5.248131847381591 1539741 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025135542452335357 30.36119842529297 0.8651116847991943 5.352957820892334 1759663 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.024705403856933116 19.150466918945312 0.9313635110855103 5.114256572723389 1978188 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021294585429131985 19.572125720977784 0.893821781873703 5.686692428588867 2196400 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02013686615973711 21.16777973175049 0.9235940277576447 5.117261219024658 2415009 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017312437202781437 20.98265495300293 0.9195729851722717 5.328553342819214 2631868 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013579074572771788 22.472011375427247 0.8490665495395661 5.141991090774536 2847783 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01200216095894575 19.15031318664551 0.8774593353271485 5.4378577709198 3067783 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008402357855811716 16.10773801803589 0.8335257112979889 5.755160808563232 3287783 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005491382535547018 21.316411399841307 0.731538200378418 6.132219219207764 3506979 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002729071327485144 22.252119636535646 0.6915061116218567 6.1039775848388675 3726979 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017628142843022943 19.696506023406982 0.7269459545612336 6.1377325534820555 3946979 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00020708630181616173 18.954635620117188 0.6670345664024353 6.278966283798217 4165656 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007875776641412813 22.738111305236817 0.6541368901729584 6.402449083328247 4385372 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015077880521857877 20.409664154052734 0.581001752614975 6.459870624542236 4604697 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0001698687730822712 17.94523868560791 0.4723160684108734 6.823065900802613 4823812 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00021467773021868197 19.590011310577392 0.3873060941696167 7.007150554656983 5042020 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011783628948251134 23.494851493835448 0.28483636379241944 7.373043966293335 5261902 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009483618545345962 24.291671752929688 0.3178412765264511 7.646661424636841 5481902 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005997495778046868 18.136104106903076 0.4274886637926102 7.069810199737549 5701580 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005768814553448465 18.41812229156494 0.3811501294374466 7.186906242370606 5921259 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00025820648625085596 18.558645248413086 0.3422541290521622 7.667954826354981 6141259 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001250064047781052 25.926301956176758 0.3554947853088379 7.723732995986938 6361259 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011134118467452936 18.520935249328613 0.3406204730272293 7.721747255325317 6581259 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008382030799111817 19.15213737487793 0.2664108738303185 8.024567413330079 6801259 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00034172932464571204 22.938098526000978 0.32661224603652955 8.008215188980103 7021259 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007882708712713792 15.622758865356445 0.34556865990161895 8.330443096160888 7241259 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007060890307911905 18.940626430511475 0.2860060125589371 8.302656841278075 7461259 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005919728777371347 20.91095886230469 0.2634516268968582 8.719109439849854 7681259 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000559230486760498 22.956223678588866 0.2519085735082626 8.732949542999268 7901259 0


Pure best response payoff estimated to be 135.335 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 83.64 seconds to finish estimate with resulting utilities: [186.21   2.94]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 138.68 seconds to finish estimate with resulting utilities: [134.705  53.81 ]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 138.12 seconds to finish estimate with resulting utilities: [49.875 49.435]
Computing meta_strategies
Exited RRD with total regret 4.826951668369617 that was less than regret lambda 4.827586206896552 after 78 iterations 
NEW LAMBDA 4.655172413793103
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    48.81           4.54           2.94      
    1    184.06          89.32          53.81      
    2    186.21          134.71          49.66      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    48.81          184.06          186.21      
    1     4.54          89.32          134.71      
    2     2.94          53.81          49.66      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    97.63          188.59          189.15      
    1    188.59          178.64          188.52      
    2    189.15          188.52          99.31      

 

Metagame probabilities: 
Player #0: 0.001  0.2633  0.7357  
Player #1: 0.001  0.2633  0.7357  
Iteration : 2
Time so far: 16325.13992190361
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 16:57:01.114519: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.014315489772707224 59.141666412353516 0.2736955866217613 10.064404582977295 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028200345113873482 22.552893257141115 0.5979503989219666 8.347656726837158 231000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027630333974957465 22.208217048645018 0.6110518336296081 7.610005331039429 450868 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03181896172463894 13.611830902099609 0.7360960185527802 7.394097185134887 665599 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030094143562018873 13.924002361297607 0.74956573843956 7.295218896865845 882001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026482231728732587 15.518967723846435 0.7109725117683411 7.079242420196533 1092658 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028777964785695075 19.09552059173584 0.8118252336978913 6.6415855884552 1303343 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02832513488829136 13.683691883087159 0.8877251386642456 6.734084844589233 1511585 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021020515449345113 13.264166069030761 0.7378318786621094 7.132285261154175 1719995 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025243610516190527 8.441232109069825 0.9173535227775573 6.721866130828857 1927438 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019869801588356495 10.97916088104248 0.814331442117691 6.87142767906189 2135201 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018894950859248638 8.62833571434021 0.8500467121601105 7.050566673278809 2343758 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01446324484422803 14.729587364196778 0.7458018958568573 7.4467731475830075 2551320 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012096044421195985 14.365664863586426 0.7619551062583924 7.206560325622559 2760049 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.012442340888082982 11.239637088775634 0.7935948550701142 7.411384963989258 2967768 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008563359547406436 11.022054767608642 0.7146984815597535 7.390387487411499 3175721 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.006142012495547533 8.78220624923706 0.7421865820884704 7.351034498214721 3381793 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.004926978517323733 10.680134963989257 0.8161466956138611 7.384703159332275 3588237 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015930375084280968 9.578810119628907 0.7359354257583618 7.595490455627441 3794793 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004622406064299867 10.734529209136962 0.7428193628787995 7.503760242462159 4000894 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010366754315327853 10.266945457458496 0.7030499935150146 7.599876070022583 4208716 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003651067432656419 9.315406799316406 0.7170334875583648 7.5592933177948 4414843 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00038105621770228025 9.928998756408692 0.4868705183267593 8.886141777038574 4622234 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007429063396557467 7.380705976486206 0.5687687039375305 8.842836856842041 4828244 0
