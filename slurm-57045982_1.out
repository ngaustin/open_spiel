Job Id listed below:
57045985

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

/var/spool/slurmd.spool/job57045985/slurm_script: line 36: regret_lambda: command not found
2023-08-01 23:52:39.362833: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-01 23:52:45.524949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0801 23:52:58.920550 23320058235776 rl_environment.py:187] Using game instance: bargaining
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 32
discount_factor: 0.99
double: True
epsilon_decay_duration: 1000
hidden_layers_sizes: [50, 50]
learn_every: 10
learning_rate: 0.01
min_buffer_size_to_learn: 1000
num_actions: 121
optimizer_str: adam
replay_buffer_capacity: 10000
session: <tensorflow.python.client.session.Session object at 0x15355689ed70>
state_representation_size: 93
symmetric: True
update_target_network_every: 1000


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.8
epochs_ppo: 5
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 20000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 1000
regret_calculation_steps: 200000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x15355689ed70>
sims_per_entry: 5000
state_representation_size: 93
steps_fine_tune: 2000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-08-01 23:52:59.667119: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-08-01 23:53:00.442973: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1079 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 5000 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 6.07 seconds to finish estimate with resulting utilities: [1.8438 1.8324]
Exited RRD with total regret 0.0 that was less than regret lambda 0.5 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0     1.84      

 

Player 1 Payoff matrix: 

           0      
    0     1.84      

 

Social Welfare Sum Matrix: 

           0      
    0     3.68      

 

Iteration : 0
Time so far: 0.0002639293670654297
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-08-01 23:53:08.751205: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1941 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.2460901528596878 7.816886901855469 4.783083438873291 0.0005227870278758928 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.22026897895903813 9.628329122634161 4.667459015619187 0.14440510501472523 420052 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.18439100261141614 8.276272687679384 4.452108590195818 0.3728595614506403 820086 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.15147289601994343 7.422795054951652 4.222569450784902 0.6801988937864825 1220120 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.12277831954214675 6.917421340353695 4.061497607643221 1.0664955969247032 1620157 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.1014201612331637 6.591202301082045 3.913218430245277 1.539761500786454 2020197 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08703095002238416 6.402025455679775 3.7732989114178115 2.0585317446205385 2420232 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07669769040777856 6.274128613911622 3.6396470282940157 2.589174730560048 2820265 0


Pure best response payoff estimated to be 7.4178 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 5.95 seconds to finish estimate with resulting utilities: [7.3656 3.0142]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 7.29 seconds to finish estimate with resulting utilities: [5.4598 5.8164]
Computing meta_strategies
Exited RRD with total regret 0.4989042615718571 that was less than regret lambda 0.5 after 705 iterations 
NEW LAMBDA 0.4827586206896552
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0     1.84           3.01      
    1     7.37           5.64      

 

Player 1 Payoff matrix: 

           0              1      
    0     1.84           7.37      
    1     3.01           5.64      

 

Social Welfare Sum Matrix: 

           0              1      
    0     3.68          10.38      
    1    10.38          11.28      

 

Metagame probabilities: 
Player #0: 0.0867  0.9133  
Player #1: 0.0867  0.9133  
Iteration : 1
Time so far: 6142.557499408722
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-08-02 01:35:30.566078: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3851 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07669151033494725 6.456946483916706 3.6247754732767743 2.660172492930057 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08875794406446469 6.963447639128057 3.6025374764349403 3.0161060967114746 420036 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0924154984020709 6.859024484520373 3.5804363512474557 3.356553589781027 820077 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09085564444471589 6.79397113720576 3.5585002373246586 3.665023871030594 1220117 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08559438816862114 6.738669390124934 3.53440449322973 3.9490724589156865 1620162 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07926641164154777 6.69427069894603 3.5024325124552993 4.2158348517352024 2020212 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07352758488754138 6.652487751751235 3.4601908398397043 4.460644227809308 2420255 0


Pure best response payoff estimated to be 6.889 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 6.01 seconds to finish estimate with resulting utilities: [7.5014 3.1906]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 8.68 seconds to finish estimate with resulting utilities: [6.6154 4.8046]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 12.25 seconds to finish estimate with resulting utilities: [4.0674 4.2618]
Computing meta_strategies
Exited RRD with total regret 0.4819514927026738 that was less than regret lambda 0.4827586206896552 after 660 iterations 
NEW LAMBDA 0.4655172413793104
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0     1.84           3.01           3.19      
    1     7.37           5.64           4.80      
    2     7.50           6.62           4.16      

 

Player 1 Payoff matrix: 

           0              1              2      
    0     1.84           7.37           7.50      
    1     3.01           5.64           6.62      
    2     3.19           4.80           4.16      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0     3.68          10.38          10.69      
    1    10.38          11.28          11.42      
    2    10.69          11.42           8.33      

 

Metagame probabilities: 
Player #0: 0.074  0.4407  0.4852  
Player #1: 0.074  0.4407  0.4852  
Iteration : 2
Time so far: 11474.695628643036
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 03:04:22.559043: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5841 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07210291570852471 6.71696441050838 3.439556535377222 4.551283253642792 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07845120125264046 6.957120356004532 3.43257539125338 4.72558164890601 420056 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08084253982024807 6.933652183489921 3.4269175465290362 4.8906749300477275 820104 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08020870128067605 6.891115567339472 3.4170830202389912 5.057371937798633 1220157 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07728382952171686 6.84074551883069 3.405522076379169 5.214725376801909 1620194 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07364328457487299 6.78576674474183 3.3862267195537528 5.37005952404472 2020225 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07026397509328684 6.719061265064745 3.3480790944731966 5.531260646850027 2420267 0


Pure best response payoff estimated to be 6.2498 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 5.74 seconds to finish estimate with resulting utilities: [7.27   3.1772]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 8.0 seconds to finish estimate with resulting utilities: [6.6574 5.133 ]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 8.95 seconds to finish estimate with resulting utilities: [5.46   5.8408]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 8.46 seconds to finish estimate with resulting utilities: [5.723  5.5518]
Computing meta_strategies
Exited RRD with total regret 0.4654014315964128 that was less than regret lambda 0.4655172413793104 after 1520 iterations 
NEW LAMBDA 0.4482758620689656
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0     1.84           3.01           3.19           3.18      
    1     7.37           5.64           4.80           5.13      
    2     7.50           6.62           4.16           5.84      
    3     7.27           6.66           5.46           5.64      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0     1.84           7.37           7.50           7.27      
    1     3.01           5.64           6.62           6.66      
    2     3.19           4.80           4.16           5.46      
    3     3.18           5.13           5.84           5.64      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0     3.68          10.38          10.69          10.45      
    1    10.38          11.28          11.42          11.79      
    2    10.69          11.42           8.33          11.30      
    3    10.45          11.79          11.30          11.27      

 

Metagame probabilities: 
Player #0: 0.0062  0.1888  0.3092  0.4958  
Player #1: 0.0062  0.1888  0.3092  0.4958  
Iteration : 3
Time so far: 16384.455756664276
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-08-02 04:26:12.658264: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7831 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07046768265920254 6.773005912504123 3.34625305498526 5.5382017858770105 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07379465722160361 6.937189320386467 3.3287498142759677 5.644718314026088 420040 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07474811457613158 6.840678293214935 3.3143556804084335 5.7385383158860295 820078 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0738857235608127 6.724370577266938 3.2983537100271936 5.832714054916741 1220114 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07147428987261724 6.584365847650089 3.272779967467326 5.937480671058627 1620159 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06864896719088896 6.452389789643201 3.242702907771174 6.053640910917197 2020208 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06603330085861452 6.32683829554805 3.2050351745668677 6.1839705623387164 2420243 0


Pure best response payoff estimated to be 6.628 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 5.75 seconds to finish estimate with resulting utilities: [7.3658 3.2396]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 7.26 seconds to finish estimate with resulting utilities: [6.37  5.706]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 7.68 seconds to finish estimate with resulting utilities: [5.89   6.9348]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 6.44 seconds to finish estimate with resulting utilities: [6.6532 6.6386]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 6.36 seconds to finish estimate with resulting utilities: [6.6746 6.9988]
Computing meta_strategies
Exited RRD with total regret 0.4480627662715104 that was less than regret lambda 0.4482758620689656 after 2238 iterations 
NEW LAMBDA 0.4310344827586208
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0     1.84           3.01           3.19           3.18           3.24      
    1     7.37           5.64           4.80           5.13           5.71      
    2     7.50           6.62           4.16           5.84           6.93      
    3     7.27           6.66           5.46           5.64           6.64      
    4     7.37           6.37           5.89           6.65           6.84      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0     1.84           7.37           7.50           7.27           7.37      
    1     3.01           5.64           6.62           6.66           6.37      
    2     3.19           4.80           4.16           5.46           5.89      
    3     3.18           5.13           5.84           5.64           6.65      
    4     3.24           5.71           6.93           6.64           6.84      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0     3.68          10.38          10.69          10.45          10.61      
    1    10.38          11.28          11.42          11.79          12.08      
    2    10.69          11.42           8.33          11.30          12.82      
    3    10.45          11.79          11.30          11.27          13.29      
    4    10.61          12.08          12.82          13.29          13.67      

 

Metagame probabilities: 
Player #0: 0.0003  0.0433  0.18  0.2299  0.5466  
Player #1: 0.0003  0.0433  0.18  0.2299  0.5466  
Iteration : 4
Time so far: 22066.51807308197
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-08-02 06:00:55.080243: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9821 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06589556366509389 6.3760291483051095 3.1987533651119056 6.203901331739489 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06787998822324004 6.451634079039986 3.1788106294265432 6.287501205971552 420056 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06849093927446591 6.327497400868711 3.1622852526551526 6.36002209975619 820095 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06785661811203927 6.208811822409431 3.145508493607243 6.433912056685014 1220133 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06612977132224677 6.098164103735213 3.1293021735328956 6.515169794607986 1620178 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06405799475344899 5.994651224628671 3.109266579576901 6.607001950214922 2020214 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.062091061021150744 5.894364081088852 3.0828357394386385 6.7023432836597285 2420249 0


Pure best response payoff estimated to be 6.8448 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 5.71 seconds to finish estimate with resulting utilities: [7.3392 3.4356]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 7.87 seconds to finish estimate with resulting utilities: [6.5592 5.1712]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 8.56 seconds to finish estimate with resulting utilities: [5.7306 6.7836]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 6.91 seconds to finish estimate with resulting utilities: [6.9254 6.528 ]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 5.58 seconds to finish estimate with resulting utilities: [7.0848 6.0374]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 6.72 seconds to finish estimate with resulting utilities: [6.6658 6.4456]
Computing meta_strategies
Exited RRD with total regret 0.43094663168328573 that was less than regret lambda 0.4310344827586208 after 2365 iterations 
NEW LAMBDA 0.41379310344827597
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0     1.84           3.01           3.19           3.18           3.24           3.44      
    1     7.37           5.64           4.80           5.13           5.71           5.17      
    2     7.50           6.62           4.16           5.84           6.93           6.78      
    3     7.27           6.66           5.46           5.64           6.64           6.53      
    4     7.37           6.37           5.89           6.65           6.84           6.04      
    5     7.34           6.56           5.73           6.93           7.08           6.56      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0     1.84           7.37           7.50           7.27           7.37           7.34      
    1     3.01           5.64           6.62           6.66           6.37           6.56      
    2     3.19           4.80           4.16           5.46           5.89           5.73      
    3     3.18           5.13           5.84           5.64           6.65           6.93      
    4     3.24           5.71           6.93           6.64           6.84           7.08      
    5     3.44           5.17           6.78           6.53           6.04           6.56      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0     3.68          10.38          10.69          10.45          10.61          10.77      
    1    10.38          11.28          11.42          11.79          12.08          11.73      
    2    10.69          11.42           8.33          11.30          12.82          12.51      
    3    10.45          11.79          11.30          11.27          13.29          13.45      
    4    10.61          12.08          12.82          13.29          13.67          13.12      
    5    10.77          11.73          12.51          13.45          13.12          13.11      

 

Metagame probabilities: 
Player #0: 0.0001  0.0193  0.1527  0.1683  0.2342  0.4255  
Player #1: 0.0001  0.0193  0.1527  0.1683  0.2342  0.4255  
Iteration : 5
Time so far: 28390.357483386993
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 07:46:18.631519: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11811 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06144450643885535 5.904964640269908 3.0681446561148 6.747640809643641 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06344246642634861 5.983956033878757 3.059964895248413 6.825943053683395 420034 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06421312171978265 5.882382279451746 3.0517061021206153 6.904138909222003 820066 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0639487014408275 5.780592568377231 3.0417602667571804 6.983356208260603 1220098 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06274487536890806 5.681674811757844 3.029282209067509 7.06104674872078 1620130 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.061223301700299375 5.597117160694711 3.0162206542891945 7.13555353407932 2020168 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.059773786133540424 5.527607376933877 3.0013778541914 7.208238760755345 2420214 0


Pure best response payoff estimated to be 7.0226 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 5.88 seconds to finish estimate with resulting utilities: [7.4042 3.6108]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 7.61 seconds to finish estimate with resulting utilities: [6.4842 5.588 ]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 8.1 seconds to finish estimate with resulting utilities: [5.903  6.7794]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 6.43 seconds to finish estimate with resulting utilities: [6.9366 6.6162]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 5.99 seconds to finish estimate with resulting utilities: [6.8156 6.05  ]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 6.04 seconds to finish estimate with resulting utilities: [6.6448 6.2476]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 6.74 seconds to finish estimate with resulting utilities: [6.7068 6.5358]
Computing meta_strategies
Exited RRD with total regret 0.4137801755960693 that was less than regret lambda 0.41379310344827597 after 1971 iterations 
NEW LAMBDA 0.39655172413793116
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0     1.84           3.01           3.19           3.18           3.24           3.44           3.61      
    1     7.37           5.64           4.80           5.13           5.71           5.17           5.59      
    2     7.50           6.62           4.16           5.84           6.93           6.78           6.78      
    3     7.27           6.66           5.46           5.64           6.64           6.53           6.62      
    4     7.37           6.37           5.89           6.65           6.84           6.04           6.05      
    5     7.34           6.56           5.73           6.93           7.08           6.56           6.25      
    6     7.40           6.48           5.90           6.94           6.82           6.64           6.62      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0     1.84           7.37           7.50           7.27           7.37           7.34           7.40      
    1     3.01           5.64           6.62           6.66           6.37           6.56           6.48      
    2     3.19           4.80           4.16           5.46           5.89           5.73           5.90      
    3     3.18           5.13           5.84           5.64           6.65           6.93           6.94      
    4     3.24           5.71           6.93           6.64           6.84           7.08           6.82      
    5     3.44           5.17           6.78           6.53           6.04           6.56           6.64      
    6     3.61           5.59           6.78           6.62           6.05           6.25           6.62      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0     3.68          10.38          10.69          10.45          10.61          10.77          11.02      
    1    10.38          11.28          11.42          11.79          12.08          11.73          12.07      
    2    10.69          11.42           8.33          11.30          12.82          12.51          12.68      
    3    10.45          11.79          11.30          11.27          13.29          13.45          13.55      
    4    10.61          12.08          12.82          13.29          13.67          13.12          12.87      
    5    10.77          11.73          12.51          13.45          13.12          13.11          12.89      
    6    11.02          12.07          12.68          13.55          12.87          12.89          13.24      

 

Metagame probabilities: 
Player #0: 0.0004  0.0251  0.1432  0.1511  0.1558  0.2415  0.283  
Player #1: 0.0004  0.0251  0.1432  0.1511  0.1558  0.2415  0.283  
Iteration : 6
Time so far: 34980.30129694939
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-08-02 09:36:08.873253: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13801 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.059221785038181075 5.5350722999572755 2.9934960474198866 7.2452227345132085 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06057035049594629 5.592974426461466 2.9820382686830915 7.2908622271463575 420046 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06103078230433869 5.53586926635789 2.9706420753338585 7.332041519142003 820086 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06069787379639787 5.4828229163506785 2.958854911427298 7.37709402381339 1220121 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05963308906245024 5.431027341697648 2.945952673069915 7.426120354613716 1620153 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05832676412421029 5.377171660614014 2.93006928830828 7.4846004827230335 2020185 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05708185204822201 5.321654022232781 2.91097908488865 7.547758475682258 2420211 0
Recovering previous policy with expected return of 6.663336663336663. Long term value was 6.7158 and short term was 6.629.


Pure best response payoff estimated to be 7.0492 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 5.9 seconds to finish estimate with resulting utilities: [7.3578 3.6274]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 7.53 seconds to finish estimate with resulting utilities: [6.4922 5.4924]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 8.2 seconds to finish estimate with resulting utilities: [5.8494 6.739 ]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 6.41 seconds to finish estimate with resulting utilities: [6.8684 6.6278]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 6.06 seconds to finish estimate with resulting utilities: [6.7232 6.1014]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 6.04 seconds to finish estimate with resulting utilities: [6.6236 6.2084]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 6.72 seconds to finish estimate with resulting utilities: [6.6776 6.5538]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 6.65 seconds to finish estimate with resulting utilities: [6.6964 6.5122]
Computing meta_strategies
Exited RRD with total regret 0.396461127527453 that was less than regret lambda 0.39655172413793116 after 1365 iterations 
NEW LAMBDA 0.3793103448275863
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0     1.84           3.01           3.19           3.18           3.24           3.44           3.61           3.63      
    1     7.37           5.64           4.80           5.13           5.71           5.17           5.59           5.49      
    2     7.50           6.62           4.16           5.84           6.93           6.78           6.78           6.74      
    3     7.27           6.66           5.46           5.64           6.64           6.53           6.62           6.63      
    4     7.37           6.37           5.89           6.65           6.84           6.04           6.05           6.10      
    5     7.34           6.56           5.73           6.93           7.08           6.56           6.25           6.21      
    6     7.40           6.48           5.90           6.94           6.82           6.64           6.62           6.55      
    7     7.36           6.49           5.85           6.87           6.72           6.62           6.68           6.60      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0     1.84           7.37           7.50           7.27           7.37           7.34           7.40           7.36      
    1     3.01           5.64           6.62           6.66           6.37           6.56           6.48           6.49      
    2     3.19           4.80           4.16           5.46           5.89           5.73           5.90           5.85      
    3     3.18           5.13           5.84           5.64           6.65           6.93           6.94           6.87      
    4     3.24           5.71           6.93           6.64           6.84           7.08           6.82           6.72      
    5     3.44           5.17           6.78           6.53           6.04           6.56           6.64           6.62      
    6     3.61           5.59           6.78           6.62           6.05           6.25           6.62           6.68      
    7     3.63           5.49           6.74           6.63           6.10           6.21           6.55           6.60      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0     3.68          10.38          10.69          10.45          10.61          10.77          11.02          10.99      
    1    10.38          11.28          11.42          11.79          12.08          11.73          12.07          11.98      
    2    10.69          11.42           8.33          11.30          12.82          12.51          12.68          12.59      
    3    10.45          11.79          11.30          11.27          13.29          13.45          13.55          13.50      
    4    10.61          12.08          12.82          13.29          13.67          13.12          12.87          12.82      
    5    10.77          11.73          12.51          13.45          13.12          13.11          12.89          12.83      
    6    11.02          12.07          12.68          13.55          12.87          12.89          13.24          13.23      
    7    10.99          11.98          12.59          13.50          12.82          12.83          13.23          13.21      

 

Metagame probabilities: 
Player #0: 0.0022  0.039  0.1343  0.138  0.129  0.1679  0.1969  0.1928  
Player #1: 0.0022  0.039  0.1343  0.138  0.129  0.1679  0.1969  0.1928  
Iteration : 7
Time so far: 41635.75688004494
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-08-02 11:27:04.379836: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15791 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.057043596613348076 5.350714121735707 2.9083433543387924 7.556625656455359 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.058189780231687645 5.412640422673527 2.8987040350639743 7.596320957088589 420030 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.058538631919986546 5.374364828504225 2.8875035218592644 7.635507767541387 820071 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05819789939829541 5.339757474130778 2.874828495635867 7.678989233702175 1220107 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05727353203955985 5.304243966894403 2.860906835567732 7.7290026788578094 1620151 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0561383760250789 5.270567348247062 2.8463966094420288 7.78474572330803 2020186 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.055039643553872405 5.236836206514849 2.8289298936285534 7.849052120145331 2420227 0


Pure best response payoff estimated to be 7.0268 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (8, 0)
Current player 0 and current strategies (8, 0) took 6.05 seconds to finish estimate with resulting utilities: [7.4248 3.5042]
Estimating current strategies:  (8, 1)
Current player 0 and current strategies (8, 1) took 7.77 seconds to finish estimate with resulting utilities: [6.5012 5.5726]
Estimating current strategies:  (8, 2)
Current player 0 and current strategies (8, 2) took 9.19 seconds to finish estimate with resulting utilities: [5.5894 6.405 ]
Estimating current strategies:  (8, 3)
Current player 0 and current strategies (8, 3) took 7.01 seconds to finish estimate with resulting utilities: [6.7438 6.1638]
Estimating current strategies:  (8, 4)
Current player 0 and current strategies (8, 4) took 6.26 seconds to finish estimate with resulting utilities: [6.8638 5.6516]
Estimating current strategies:  (8, 5)
Current player 0 and current strategies (8, 5) took 5.93 seconds to finish estimate with resulting utilities: [7.1942 6.189 ]
Estimating current strategies:  (8, 6)
Current player 0 and current strategies (8, 6) took 5.93 seconds to finish estimate with resulting utilities: [7.3468 6.4842]
Estimating current strategies:  (8, 7)
Current player 0 and current strategies (8, 7) took 5.87 seconds to finish estimate with resulting utilities: [7.384  6.4036]
Estimating current strategies:  (8, 8)
Current player 0 and current strategies (8, 8) took 5.36 seconds to finish estimate with resulting utilities: [7.3396 6.3078]
Computing meta_strategies
Exited RRD with total regret 0.3792183740672588 that was less than regret lambda 0.3793103448275863 after 4610 iterations 
NEW LAMBDA 0.3620689655172415
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0     1.84           3.01           3.19           3.18           3.24           3.44           3.61           3.63           3.50      
    1     7.37           5.64           4.80           5.13           5.71           5.17           5.59           5.49           5.57      
    2     7.50           6.62           4.16           5.84           6.93           6.78           6.78           6.74           6.41      
    3     7.27           6.66           5.46           5.64           6.64           6.53           6.62           6.63           6.16      
    4     7.37           6.37           5.89           6.65           6.84           6.04           6.05           6.10           5.65      
    5     7.34           6.56           5.73           6.93           7.08           6.56           6.25           6.21           6.19      
    6     7.40           6.48           5.90           6.94           6.82           6.64           6.62           6.55           6.48      
    7     7.36           6.49           5.85           6.87           6.72           6.62           6.68           6.60           6.40      
    8     7.42           6.50           5.59           6.74           6.86           7.19           7.35           7.38           6.82      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0     1.84           7.37           7.50           7.27           7.37           7.34           7.40           7.36           7.42      
    1     3.01           5.64           6.62           6.66           6.37           6.56           6.48           6.49           6.50      
    2     3.19           4.80           4.16           5.46           5.89           5.73           5.90           5.85           5.59      
    3     3.18           5.13           5.84           5.64           6.65           6.93           6.94           6.87           6.74      
    4     3.24           5.71           6.93           6.64           6.84           7.08           6.82           6.72           6.86      
    5     3.44           5.17           6.78           6.53           6.04           6.56           6.64           6.62           7.19      
    6     3.61           5.59           6.78           6.62           6.05           6.25           6.62           6.68           7.35      
    7     3.63           5.49           6.74           6.63           6.10           6.21           6.55           6.60           7.38      
    8     3.50           5.57           6.41           6.16           5.65           6.19           6.48           6.40           6.82      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8      
    0     3.68          10.38          10.69          10.45          10.61          10.77          11.02          10.99          10.93      
    1    10.38          11.28          11.42          11.79          12.08          11.73          12.07          11.98          12.07      
    2    10.69          11.42           8.33          11.30          12.82          12.51          12.68          12.59          11.99      
    3    10.45          11.79          11.30          11.27          13.29          13.45          13.55          13.50          12.91      
    4    10.61          12.08          12.82          13.29          13.67          13.12          12.87          12.82          12.52      
    5    10.77          11.73          12.51          13.45          13.12          13.11          12.89          12.83          13.38      
    6    11.02          12.07          12.68          13.55          12.87          12.89          13.24          13.23          13.83      
    7    10.99          11.98          12.59          13.50          12.82          12.83          13.23          13.21          13.79      
    8    10.93          12.07          11.99          12.91          12.52          13.38          13.83          13.79          13.65      

 

Metagame probabilities: 
Player #0: 0.0001  0.0007  0.0489  0.0357  0.0111  0.0465  0.1235  0.1069  0.6266  
Player #1: 0.0001  0.0007  0.0489  0.0357  0.0111  0.0465  0.1235  0.1069  0.6266  
Iteration : 8
Time so far: 47945.034071683884
Approximating Best Response
Training best response:  True 0.06
rtg values check:  (?,) (?, 1)
2023-08-02 13:12:13.876824: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_62/weights_2/Adam_1/Assign' id:17781 op device:{requested: '', assigned: ''} def:{{{node mlp_62/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_62/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_62/weights_2/Adam_1, mlp_62/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05486414344798737 5.268252940191901 2.8238729005451892 7.8663961868251775 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05570209974229689 5.331885519146576 2.812418685572031 7.919742026013875 420031 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05601372197303205 5.285014868343314 2.8018557578801886 7.968880305196303 820061 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05582837587115352 5.232889026992319 2.792435674698179 8.01544102583373 1220095 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05511663839313205 5.179717479720509 2.7831489174990685 8.059136549862592 1620133 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.054207218707256144 5.128513808369318 2.772716434895727 8.103668885728078 2020181 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.053288205344399715 5.07892180100305 2.7587319499029994 8.15174354768514 2420215 0
Recovering previous policy with expected return of 7.003996003996004. Long term value was 6.9938 and short term was 6.98.


Pure best response payoff estimated to be 7.2966 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (9, 0)
Current player 0 and current strategies (9, 0) took 6.65 seconds to finish estimate with resulting utilities: [7.4656 3.5644]
Estimating current strategies:  (9, 1)
Current player 0 and current strategies (9, 1) took 8.68 seconds to finish estimate with resulting utilities: [6.477  5.5418]
Estimating current strategies:  (9, 2)
Current player 0 and current strategies (9, 2) took 9.97 seconds to finish estimate with resulting utilities: [5.642 6.46 ]
Estimating current strategies:  (9, 3)
Current player 0 and current strategies (9, 3) took 7.81 seconds to finish estimate with resulting utilities: [6.656 6.075]
Estimating current strategies:  (9, 4)
Current player 0 and current strategies (9, 4) took 7.03 seconds to finish estimate with resulting utilities: [6.8268 5.6264]
Estimating current strategies:  (9, 5)
Current player 0 and current strategies (9, 5) took 6.49 seconds to finish estimate with resulting utilities: [7.2104 6.1958]
Estimating current strategies:  (9, 6)
Current player 0 and current strategies (9, 6) took 6.48 seconds to finish estimate with resulting utilities: [7.3318 6.4918]
Estimating current strategies:  (9, 7)
Current player 0 and current strategies (9, 7) took 6.36 seconds to finish estimate with resulting utilities: [7.366  6.4566]
Estimating current strategies:  (9, 8)
Current player 0 and current strategies (9, 8) took 5.93 seconds to finish estimate with resulting utilities: [7.2916 6.2314]
Estimating current strategies:  (9, 9)
Current player 0 and current strategies (9, 9) took 5.69 seconds to finish estimate with resulting utilities: [7.3666 6.1978]
Computing meta_strategies
Exited RRD with total regret 0.36203314862871494 that was less than regret lambda 0.3620689655172415 after 5063 iterations 
NEW LAMBDA 0.3448275862068967
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     1.84           3.01           3.19           3.18           3.24           3.44           3.61           3.63           3.50           3.56      
    1     7.37           5.64           4.80           5.13           5.71           5.17           5.59           5.49           5.57           5.54      
    2     7.50           6.62           4.16           5.84           6.93           6.78           6.78           6.74           6.41           6.46      
    3     7.27           6.66           5.46           5.64           6.64           6.53           6.62           6.63           6.16           6.08      
    4     7.37           6.37           5.89           6.65           6.84           6.04           6.05           6.10           5.65           5.63      
    5     7.34           6.56           5.73           6.93           7.08           6.56           6.25           6.21           6.19           6.20      
    6     7.40           6.48           5.90           6.94           6.82           6.64           6.62           6.55           6.48           6.49      
    7     7.36           6.49           5.85           6.87           6.72           6.62           6.68           6.60           6.40           6.46      
    8     7.42           6.50           5.59           6.74           6.86           7.19           7.35           7.38           6.82           6.23      
    9     7.47           6.48           5.64           6.66           6.83           7.21           7.33           7.37           7.29           6.78      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     1.84           7.37           7.50           7.27           7.37           7.34           7.40           7.36           7.42           7.47      
    1     3.01           5.64           6.62           6.66           6.37           6.56           6.48           6.49           6.50           6.48      
    2     3.19           4.80           4.16           5.46           5.89           5.73           5.90           5.85           5.59           5.64      
    3     3.18           5.13           5.84           5.64           6.65           6.93           6.94           6.87           6.74           6.66      
    4     3.24           5.71           6.93           6.64           6.84           7.08           6.82           6.72           6.86           6.83      
    5     3.44           5.17           6.78           6.53           6.04           6.56           6.64           6.62           7.19           7.21      
    6     3.61           5.59           6.78           6.62           6.05           6.25           6.62           6.68           7.35           7.33      
    7     3.63           5.49           6.74           6.63           6.10           6.21           6.55           6.60           7.38           7.37      
    8     3.50           5.57           6.41           6.16           5.65           6.19           6.48           6.40           6.82           7.29      
    9     3.56           5.54           6.46           6.08           5.63           6.20           6.49           6.46           6.23           6.78      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     3.68          10.38          10.69          10.45          10.61          10.77          11.02          10.99          10.93          11.03      
    1    10.38          11.28          11.42          11.79          12.08          11.73          12.07          11.98          12.07          12.02      
    2    10.69          11.42           8.33          11.30          12.82          12.51          12.68          12.59          11.99          12.10      
    3    10.45          11.79          11.30          11.27          13.29          13.45          13.55          13.50          12.91          12.73      
    4    10.61          12.08          12.82          13.29          13.67          13.12          12.87          12.82          12.52          12.45      
    5    10.77          11.73          12.51          13.45          13.12          13.11          12.89          12.83          13.38          13.41      
    6    11.02          12.07          12.68          13.55          12.87          12.89          13.24          13.23          13.83          13.82      
    7    10.99          11.98          12.59          13.50          12.82          12.83          13.23          13.21          13.79          13.82      
    8    10.93          12.07          11.99          12.91          12.52          13.38          13.83          13.79          13.65          13.52      
    9    11.03          12.02          12.10          12.73          12.45          13.41          13.82          13.82          13.52          13.56      

 

Metagame probabilities: 
Player #0: 0.0001  0.0003  0.0342  0.0159  0.0035  0.0233  0.0771  0.0669  0.164  0.6148  
Player #1: 0.0001  0.0003  0.0342  0.0159  0.0035  0.0233  0.0771  0.0669  0.164  0.6148  
Iteration : 9
Time so far: 55172.54302763939
Approximating Best Response
Training best response:  True 0.05500000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 15:12:42.051708: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_69/weights_2/Adam_1/Assign' id:19771 op device:{requested: '', assigned: ''} def:{{{node mlp_69/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_69/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_69/weights_2/Adam_1, mlp_69/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.053189245774841305 5.109672172503982 2.755775022278065 8.16220223585652 20003 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05426585797413227 5.184671614572546 2.752614308608937 8.192316799850525 420033 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05468745777496066 5.133807709805586 2.7466040034201735 8.22426661633627 820068 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05452371740312126 5.084875418948258 2.738562917017996 8.261437366378388 1220102 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0539417684461944 5.03583127745012 2.732592254765362 8.297079070992314 1620141 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.053175112512414774 4.987969611908975 2.7261906767426822 8.331065968006683 2020174 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05243439856639943 4.940348968584893 2.71692582204197 8.365689599881186 2420216 0
Recovering previous policy with expected return of 7.317682317682317. Long term value was 7.2244 and short term was 7.207.


Pure best response payoff estimated to be 7.533 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (10, 0)
Current player 0 and current strategies (10, 0) took 6.14 seconds to finish estimate with resulting utilities: [7.3638 3.6308]
Estimating current strategies:  (10, 1)
Current player 0 and current strategies (10, 1) took 8.08 seconds to finish estimate with resulting utilities: [6.4038 5.4894]
Estimating current strategies:  (10, 2)
Current player 0 and current strategies (10, 2) took 9.16 seconds to finish estimate with resulting utilities: [5.6234 6.3922]
Estimating current strategies:  (10, 3)
Current player 0 and current strategies (10, 3) took 7.15 seconds to finish estimate with resulting utilities: [6.6892 6.1202]
Estimating current strategies:  (10, 4)
Current player 0 and current strategies (10, 4) took 6.35 seconds to finish estimate with resulting utilities: [6.8476 5.6628]
Estimating current strategies:  (10, 5)
Current player 0 and current strategies (10, 5) took 6.06 seconds to finish estimate with resulting utilities: [7.2164 6.0944]
Estimating current strategies:  (10, 6)
Current player 0 and current strategies (10, 6) took 5.95 seconds to finish estimate with resulting utilities: [7.4216 6.444 ]
Estimating current strategies:  (10, 7)
Current player 0 and current strategies (10, 7) took 5.99 seconds to finish estimate with resulting utilities: [7.3584 6.4434]
Estimating current strategies:  (10, 8)
Current player 0 and current strategies (10, 8) took 5.54 seconds to finish estimate with resulting utilities: [7.3388 6.2336]
Estimating current strategies:  (10, 9)
Current player 0 and current strategies (10, 9) took 5.48 seconds to finish estimate with resulting utilities: [7.3372 6.2366]
Estimating current strategies:  (10, 10)
Current player 0 and current strategies (10, 10) took 5.46 seconds to finish estimate with resulting utilities: [7.3248 6.245 ]
Computing meta_strategies
Exited RRD with total regret 0.34476107962782976 that was less than regret lambda 0.3448275862068967 after 5374 iterations 
NEW LAMBDA 0.3275862068965518
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     1.84           3.01           3.19           3.18           3.24           3.44           3.61           3.63           3.50           3.56           3.63      
    1     7.37           5.64           4.80           5.13           5.71           5.17           5.59           5.49           5.57           5.54           5.49      
    2     7.50           6.62           4.16           5.84           6.93           6.78           6.78           6.74           6.41           6.46           6.39      
    3     7.27           6.66           5.46           5.64           6.64           6.53           6.62           6.63           6.16           6.08           6.12      
    4     7.37           6.37           5.89           6.65           6.84           6.04           6.05           6.10           5.65           5.63           5.66      
    5     7.34           6.56           5.73           6.93           7.08           6.56           6.25           6.21           6.19           6.20           6.09      
    6     7.40           6.48           5.90           6.94           6.82           6.64           6.62           6.55           6.48           6.49           6.44      
    7     7.36           6.49           5.85           6.87           6.72           6.62           6.68           6.60           6.40           6.46           6.44      
    8     7.42           6.50           5.59           6.74           6.86           7.19           7.35           7.38           6.82           6.23           6.23      
    9     7.47           6.48           5.64           6.66           6.83           7.21           7.33           7.37           7.29           6.78           6.24      
   10     7.36           6.40           5.62           6.69           6.85           7.22           7.42           7.36           7.34           7.34           6.78      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     1.84           7.37           7.50           7.27           7.37           7.34           7.40           7.36           7.42           7.47           7.36      
    1     3.01           5.64           6.62           6.66           6.37           6.56           6.48           6.49           6.50           6.48           6.40      
    2     3.19           4.80           4.16           5.46           5.89           5.73           5.90           5.85           5.59           5.64           5.62      
    3     3.18           5.13           5.84           5.64           6.65           6.93           6.94           6.87           6.74           6.66           6.69      
    4     3.24           5.71           6.93           6.64           6.84           7.08           6.82           6.72           6.86           6.83           6.85      
    5     3.44           5.17           6.78           6.53           6.04           6.56           6.64           6.62           7.19           7.21           7.22      
    6     3.61           5.59           6.78           6.62           6.05           6.25           6.62           6.68           7.35           7.33           7.42      
    7     3.63           5.49           6.74           6.63           6.10           6.21           6.55           6.60           7.38           7.37           7.36      
    8     3.50           5.57           6.41           6.16           5.65           6.19           6.48           6.40           6.82           7.29           7.34      
    9     3.56           5.54           6.46           6.08           5.63           6.20           6.49           6.46           6.23           6.78           7.34      
   10     3.63           5.49           6.39           6.12           5.66           6.09           6.44           6.44           6.23           6.24           6.78      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     3.68          10.38          10.69          10.45          10.61          10.77          11.02          10.99          10.93          11.03          10.99      
    1    10.38          11.28          11.42          11.79          12.08          11.73          12.07          11.98          12.07          12.02          11.89      
    2    10.69          11.42           8.33          11.30          12.82          12.51          12.68          12.59          11.99          12.10          12.02      
    3    10.45          11.79          11.30          11.27          13.29          13.45          13.55          13.50          12.91          12.73          12.81      
    4    10.61          12.08          12.82          13.29          13.67          13.12          12.87          12.82          12.52          12.45          12.51      
    5    10.77          11.73          12.51          13.45          13.12          13.11          12.89          12.83          13.38          13.41          13.31      
    6    11.02          12.07          12.68          13.55          12.87          12.89          13.24          13.23          13.83          13.82          13.87      
    7    10.99          11.98          12.59          13.50          12.82          12.83          13.23          13.21          13.79          13.82          13.80      
    8    10.93          12.07          11.99          12.91          12.52          13.38          13.83          13.79          13.65          13.52          13.57      
    9    11.03          12.02          12.10          12.73          12.45          13.41          13.82          13.82          13.52          13.56          13.57      
   10    10.99          11.89          12.02          12.81          12.51          13.31          13.87          13.80          13.57          13.57          13.57      

 

Metagame probabilities: 
Player #0: 0.0001  0.0002  0.0223  0.0093  0.0015  0.0113  0.0479  0.0439  0.0657  0.1385  0.6593  
Player #1: 0.0001  0.0002  0.0223  0.0093  0.0015  0.0113  0.0479  0.0439  0.0657  0.1385  0.6593  
Iteration : 10
Time so far: 62288.06859469414
Approximating Best Response
Training best response:  True 0.05
rtg values check:  (?,) (?, 1)
2023-08-02 17:11:17.141076: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_76/weights_2/Adam_1/Assign' id:21761 op device:{requested: '', assigned: ''} def:{{{node mlp_76/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_76/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_76/weights_2/Adam_1, mlp_76/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05235644060115626 4.9618246845707565 2.7146134475007195 8.373460909140618 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.053146011266020116 5.003436990988337 2.707951064390512 8.405518514883095 420028 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05350570244488385 4.963747306237414 2.7015741732173253 8.43657259508297 820060 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05348316324377636 4.921542262045805 2.6976407742339092 8.46431763510723 1220093 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05300868696500425 4.880265136421565 2.694629575780901 8.489163395142794 1620130 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.052354334124749206 4.839490006058691 2.689773365374815 8.514926743942357 2020171 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0517100068577007 4.79961592849427 2.6806408736278993 8.542662985902696 2420202 0


slurmstepd: error: *** JOB 57045985 ON gl3326 CANCELLED AT 2023-08-02T18:18:52 ***
