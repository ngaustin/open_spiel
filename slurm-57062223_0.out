Job Id listed below:
57062227

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

/var/spool/slurmd.spool/job57062227/slurm_script: line 29: regret_lambda: command not found
/var/spool/slurmd.spool/job57062227/slurm_script: line 30: regret_steps: command not found
2023-08-02 12:24:58.394503: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-02 12:24:59.761498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0802 12:25:02.201256 22444954884992 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x1469964fed70>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x1469964fed70>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-08-02 12:25:02.503825: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-08-02 12:25:02.785064: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.42 seconds to finish estimate with resulting utilities: [48.965 49.405]
Exited RRD with total regret 0.0 that was less than regret lambda 2.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    49.19      

 

Player 1 Payoff matrix: 

           0      
    0    49.19      

 

Social Welfare Sum Matrix: 

           0      
    0    98.37      

 

Iteration : 0
Time so far: 0.0001773834228515625
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-08-02 12:25:22.109168: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.11937852501869202 30.491953468322755 2.066206955909729 0.0012298552435822784 10591 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.1016301117837429 15.609110832214355 1.8614010691642762 0.2720035552978516 216040 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08994308337569237 14.048922061920166 1.847943329811096 0.4118946731090546 418108 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08593078851699829 13.60645341873169 1.8453334331512452 0.5374707102775573 619737 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08046011850237847 13.632942581176758 1.773612666130066 0.7399715065956116 821625 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06993255987763405 15.786378479003906 1.7487152695655823 0.814331567287445 1024725 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06444735676050187 15.10423822402954 1.7045852780342101 0.9251473009586334 1226702 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0589962936937809 16.434498500823974 1.6793923497200012 1.0599309980869294 1429921 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05538683049380779 18.29423246383667 1.6635866522789002 1.1996373534202576 1634981 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04899446666240692 18.444155788421632 1.6004082322120667 1.3042457938194274 1840331 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.04286691769957542 19.522645950317383 1.529369580745697 1.5604859948158265 2048019 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034632261283695696 20.0373966217041 1.448473823070526 1.6900640606880188 2252912 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02784613464027643 18.926334381103516 1.3707732677459716 1.8606805324554443 2461317 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022732965275645257 17.962204265594483 1.2987229585647584 2.01053409576416 2673256 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020206383801996707 19.147906875610353 1.2891990900039674 2.238301420211792 2883797 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015397100057452918 19.682696437835695 1.2047589063644408 2.287987565994263 3093121 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01163471508771181 21.94363498687744 1.1327091455459595 2.471802592277527 3302611 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008755845809355378 24.765044593811034 1.0672906637191772 2.654301381111145 3513451 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003888259781524539 23.835698699951173 1.0076610326766968 2.7276001930236817 3725402 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002056558267213404 18.21888189315796 0.9734861016273498 3.078063178062439 3937211 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006708424800308422 21.49817352294922 0.912379390001297 3.292073154449463 4147819 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015834065299713985 22.797372245788573 0.8529621422290802 3.492429184913635 4363326 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014561145034804213 20.902713584899903 0.7519414842128753 3.8739272117614747 4576998 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010097282502101734 22.959944915771484 0.7025600731372833 4.393938446044922 4791696 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016107184579595923 25.6471248626709 0.6623266816139222 4.773878908157348 5007905 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000230161151921493 25.013149642944335 0.6131985783576965 4.945316123962402 5223471 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011871958500705659 21.224115753173827 0.5863903403282166 5.199428558349609 5439441 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009650036110542715 24.91471748352051 0.544731855392456 5.47218279838562 5657191 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008190186286810786 22.99265022277832 0.5329450100660325 5.612822675704956 5875785 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -5.3203488641884177e-05 26.652080726623534 0.5050479471683502 6.141967391967773 6092274 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011615259893005714 19.636509895324707 0.5039384156465531 5.984713745117188 6308855 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0021674943942343815 23.658430290222167 0.5205642610788346 6.192528438568115 6527916 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016872841579242959 25.98486385345459 0.4885080397129059 6.328464555740356 6747276 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010302037640940397 23.12240505218506 0.46423585116863253 6.337957048416138 6967176 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016621867864159868 24.014599609375 0.4260746717453003 6.7473217964172365 7183567 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011666616217553382 24.606069374084473 0.45135186314582826 6.458014297485351 7402612 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014606131502659992 25.265623092651367 0.4351475417613983 6.517431306838989 7619071 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009368481580168009 26.16721305847168 0.4318142294883728 6.642650318145752 7836426 0


Pure best response payoff estimated to be 192.485 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 79.95 seconds to finish estimate with resulting utilities: [188.83   4.24]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 131.41 seconds to finish estimate with resulting utilities: [96.675 95.645]
Computing meta_strategies
Exited RRD with total regret 1.8244952481845473 that was less than regret lambda 2.0 after 44 iterations 
NEW LAMBDA 1.9310344827586208
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    49.19           4.24      
    1    188.83          96.16      

 

Player 1 Payoff matrix: 

           0              1      
    0    49.19          188.83      
    1     4.24          96.16      

 

Social Welfare Sum Matrix: 

           0              1      
    0    98.37          193.07      
    1    193.07          192.32      

 

Metagame probabilities: 
Player #0: 0.0099  0.9901  
Player #1: 0.0099  0.9901  
Iteration : 1
Time so far: 6310.771522521973
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-08-02 14:10:32.980800: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028021957166492938 111.88510131835938 0.5430233895778656 8.152966737747192 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03691090475767851 21.25203800201416 0.7556979179382324 6.813243579864502 230391 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0389830406755209 18.762966632843018 0.8346907377243042 6.272286224365234 448924 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03908397816121578 19.84993381500244 0.9220324456691742 6.234835433959961 665578 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03190048411488533 23.494733238220213 0.7870266377925873 6.842905759811401 879913 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.034778105840086934 15.705728816986085 0.9039448916912078 5.907462930679321 1093040 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03264998383820057 14.139626789093018 0.9413697361946106 5.431445550918579 1305141 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03153766859322786 18.488459396362305 1.002915185689926 4.5859356880187985 1513655 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03243421819061041 15.72590217590332 1.0359169960021972 4.520097637176514 1727446 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026694317534565925 15.376980018615722 0.9304276943206787 4.840257453918457 1940371 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.022758058086037636 15.94504508972168 0.9106483399868012 4.8129056930542 2150716 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019792210683226584 14.285894680023194 0.9087710618972779 4.990285778045655 2359663 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018371921963989735 16.549701595306395 0.9107394993305207 5.119629907608032 2571880 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013194002024829388 18.43159065246582 0.814594441652298 5.653326082229614 2779962 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01147614922374487 18.32413806915283 0.8400485336780548 5.456689739227295 2990890 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009688120428472757 17.42048921585083 0.838644015789032 5.5739359855651855 3201592 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.007983192941173911 14.400234985351563 0.8816253900527954 5.378158950805664 3413099 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005179589684121311 19.089190006256104 0.8284980952739716 5.477228260040283 3628645 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002625180594623089 21.682901191711426 0.7577461779117585 5.916660404205322 3842145 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009586363099515438 17.799432277679443 0.8012735724449158 5.522457313537598 4051274 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008931427029892803 19.743663215637206 0.4569098025560379 6.283419561386109 4267086 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019237429369241 14.46363935470581 0.6536596059799195 6.173033332824707 4484632 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003373099229065701 19.72192974090576 0.6301507830619812 6.244756603240967 4703161 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006576599498657743 26.627706718444824 0.5288537442684174 6.438680982589721 4921524 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00033071527213905937 19.873236083984374 0.4693577438592911 6.614917182922364 5140271 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  3.3464694570284334e-05 17.95966625213623 0.45859613716602327 7.307743549346924 5358724 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0018125553702702745 17.872478866577147 0.45706323683261874 6.734427404403687 5575170 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00019440101459622383 18.912825965881346 0.4126447945833206 6.951366329193116 5792871 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009645273952628486 24.909913063049316 0.3986000955104828 7.370907974243164 6012075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001552182511659339 22.179745292663576 0.5158848226070404 7.354769372940064 6232075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009154909494100139 16.75333414077759 0.45622378289699556 7.535313987731934 6449935 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005012441863073037 21.169276618957518 0.36405144035816195 8.098794984817506 6669077 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00047399687173310667 17.075926971435546 0.36206042766571045 7.864267301559448 6888121 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006158341238915455 15.697434139251708 0.3305689483880997 8.769933986663819 7107131 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00035754391283262523 15.051989459991455 0.36898740828037263 8.487613582611084 7327131 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013004326741793194 20.272768020629883 0.394110181927681 8.290878868103027 7546367 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014651400892034872 23.06547031402588 0.23462676256895065 9.377023124694825 7766367 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000507181194552686 17.724436950683593 0.2909956634044647 9.331614685058593 7986127 0


Pure best response payoff estimated to be 129.41 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 79.63 seconds to finish estimate with resulting utilities: [169.87   2.72]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 132.37 seconds to finish estimate with resulting utilities: [126.345  45.775]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 133.79 seconds to finish estimate with resulting utilities: [26.295 24.985]
Computing meta_strategies
Exited RRD with total regret 1.9163890632418088 that was less than regret lambda 1.9310344827586208 after 93 iterations 
NEW LAMBDA 1.8620689655172415
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    49.19           4.24           2.72      
    1    188.83          96.16          45.77      
    2    169.87          126.34          25.64      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    49.19          188.83          169.87      
    1     4.24          96.16          126.34      
    2     2.72          45.77          25.64      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    98.37          193.07          172.59      
    1    193.07          192.32          172.12      
    2    172.59          172.12          51.28      

 

Metagame probabilities: 
Player #0: 0.0005  0.4418  0.5577  
Player #1: 0.0005  0.4418  0.5577  
Iteration : 2
Time so far: 14784.482700824738
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 16:31:46.796657: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00971031819935888 43.37391624450684 0.16590648517012596 12.433968639373779 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03516160789877176 18.07927360534668 0.7366998195648193 7.289439105987549 229795 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03343052659183741 19.38772048950195 0.7196385025978088 7.292775392532349 446162 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026019673235714434 31.044553565979005 0.6006277740001679 7.527169466018677 658971 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032630471885204314 20.177566909790038 0.8016056299209595 6.825154829025268 874352 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02702925819903612 23.086620330810547 0.7280816256999969 7.296562004089355 1092381 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028285409696400164 18.16251106262207 0.8076607048511505 6.586124467849731 1310417 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025147066451609135 21.141158676147462 0.7684103190898895 7.080697536468506 1529392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025961507856845856 18.446934509277344 0.8837601065635681 6.368644332885742 1749135 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.021502499282360078 17.018804454803465 0.8057940483093262 6.830813932418823 1966788 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.018696432560682298 22.17705593109131 0.7672706544399261 6.651647615432739 2182280 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015823415387421845 22.04700469970703 0.7673345983028412 6.92496395111084 2401153 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015646892599761486 19.461858558654786 0.8529288589954376 6.411892509460449 2619197 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.013625828456133604 17.484514427185058 0.9008784115314483 6.212911701202392 2834118 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.010833893716335297 15.323117637634278 0.8303682923316955 6.623262548446656 3051604 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00826059808023274 24.329853439331053 0.8361700475215912 6.3539056301116945 3269512 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005370879964902997 17.713892269134522 0.8160440146923065 6.540459871292114 3485964 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003645669075194746 14.873382949829102 0.8301399171352386 6.509197854995728 3700309 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016494548704940825 13.73231897354126 0.7711299359798431 6.6473925590515135 3913940 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001374541877885349 17.66952257156372 0.6293892800807953 7.508847284317016 4127228 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003861419569147984 16.45574369430542 0.581974458694458 7.089921379089356 4343259 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  3.430699580349028e-05 18.17688684463501 0.6312311887741089 6.665019559860229 4556742 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017147651844425127 15.6644455909729 0.4359697550535202 7.1133264064788815 4771444 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004605429287039442 13.748146629333496 0.2876872032880783 7.477125215530395 4985416 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005784564607893116 19.29953079223633 0.5728755176067353 7.5854887008667 5200018 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010042076028184965 23.724517822265625 0.44094773530960085 7.743404150009155 5415586 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.0010583225994196255 23.55690803527832 0.42041831016540526 7.366222858428955 5627466 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012536178051959722 19.211977577209474 0.6056852042675018 7.377333784103394 5840175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009174462698865682 14.757530212402344 0.5833570092916489 7.3358207702636715 6051352 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004286190464654283 20.106486320495605 0.5700367093086243 7.59825587272644 6263457 0
