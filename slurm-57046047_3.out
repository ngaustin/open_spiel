Job Id listed below:
57046051

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

/var/spool/slurmd.spool/job57046051/slurm_script: line 36: regret_lambda: command not found
2023-08-01 23:54:29.999807: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-01 23:54:30.885486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0801 23:54:32.317720 23331929533312 rl_environment.py:187] Using game instance: bargaining
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 32
discount_factor: 0.99
double: True
epsilon_decay_duration: 1000
hidden_layers_sizes: [50, 50]
learn_every: 10
learning_rate: 0.01
min_buffer_size_to_learn: 1000
num_actions: 121
optimizer_str: adam
replay_buffer_capacity: 10000
session: <tensorflow.python.client.session.Session object at 0x15380d5f6d70>
state_representation_size: 93
symmetric: True
update_target_network_every: 1000


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.8
epochs_ppo: 5
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 20000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 1000
regret_calculation_steps: 200000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x15380d5f6d70>
sims_per_entry: 5000
state_representation_size: 93
steps_fine_tune: 2000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-08-01 23:54:32.635139: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-08-01 23:54:32.913278: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1079 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 5000 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 2.7 seconds to finish estimate with resulting utilities: [1.8438 1.843 ]
Exited RRD with total regret 0.0 that was less than regret lambda 0.2 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0     1.84      

 

Player 1 Payoff matrix: 

           0      
    0     1.84      

 

Social Welfare Sum Matrix: 

           0      
    0     3.69      

 

Iteration : 0
Time so far: 0.0001838207244873047
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-08-01 23:54:36.728814: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1941 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.24522630572319032 7.2684245109558105 4.784281826019287 0.0005634857880068012 20005 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.22035317648024785 9.727373926980155 4.655039378574917 0.15319560883446165 420049 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.18554400583592856 8.450666246181582 4.47299238530601 0.3579231931131245 820092 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.15291299262984853 7.6336801779074746 4.269592725253496 0.6405087478952454 1220130 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.12380942123263707 7.2248639507058225 4.103639591476064 1.0272599541206513 1620169 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.10214469888164561 7.036205383338551 3.95407183241136 1.4942540024843622 2020204 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08756342592903159 6.943013686187997 3.81818726555375 1.9956532200570012 2420244 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07701094739456125 6.888868819568174 3.698126929533397 2.5008557980842676 2820283 0


Pure best response payoff estimated to be 7.3954 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 6.15 seconds to finish estimate with resulting utilities: [7.4094 2.7628]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 9.13 seconds to finish estimate with resulting utilities: [4.5852 5.1978]
Computing meta_strategies
Exited RRD with total regret 0.1998143892351658 that was less than regret lambda 0.2 after 1106 iterations 
NEW LAMBDA 0.19310344827586207
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0     1.84           2.76      
    1     7.41           4.89      

 

Player 1 Payoff matrix: 

           0              1      
    0     1.84           7.41      
    1     2.76           4.89      

 

Social Welfare Sum Matrix: 

           0              1      
    0     3.69          10.17      
    1    10.17           9.78      

 

Metagame probabilities: 
Player #0: 0.0438  0.9562  
Player #1: 0.0438  0.9562  
Iteration : 1
Time so far: 5282.172680139542
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-08-02 01:22:39.167116: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3851 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07772931852443538 7.040568107954213 3.693952228317798 2.5196582617948646 20004 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09133446690522962 7.5130161255966 3.6870701990009827 2.79534823556484 420050 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09632005701557948 7.433622201458438 3.6842702684821664 3.0438255188271937 820090 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09454601853532661 7.385031214799032 3.6697328237023683 3.2866712389644457 1220134 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08866420154174505 7.3911993606670485 3.644057847358085 3.5307964766382667 1620175 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08191744393860528 7.410954346932655 3.605714215128875 3.7681644387513678 2020211 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0760606838804683 7.432979048663423 3.536519643368612 4.030253793541032 2420259 0


Pure best response payoff estimated to be 5.9332 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 6.02 seconds to finish estimate with resulting utilities: [7.3948 3.0968]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 10.07 seconds to finish estimate with resulting utilities: [5.6566 4.2092]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 10.94 seconds to finish estimate with resulting utilities: [4.3324 5.0348]
Computing meta_strategies
Exited RRD with total regret 0.19309074519783564 that was less than regret lambda 0.19310344827586207 after 2851 iterations 
NEW LAMBDA 0.18620689655172412
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0     1.84           2.76           3.10      
    1     7.41           4.89           4.21      
    2     7.39           5.66           4.68      

 

Player 1 Payoff matrix: 

           0              1              2      
    0     1.84           7.41           7.39      
    1     2.76           4.89           5.66      
    2     3.10           4.21           4.68      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0     3.69          10.17          10.49      
    1    10.17           9.78           9.87      
    2    10.49           9.87           9.37      

 

Metagame probabilities: 
Player #0: 0.0015  0.1785  0.82  
Player #1: 0.0015  0.1785  0.82  
Iteration : 2
Time so far: 10387.453790187836
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 02:47:44.736512: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5841 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0753349176607481 7.502141187074925 3.5168201903725387 4.095897319435853 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08240820155847121 7.62780297741242 3.523339137964548 4.225606491261038 420045 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08474871512248934 7.490380475109485 3.524598140281652 4.350977891977333 820084 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08363216203698477 7.36705843053462 3.514183408404709 4.492802715522618 1220130 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.08004348583976398 7.243183879137726 3.4954395687202213 4.646596572380762 1620173 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07585808535500725 7.121539181909379 3.465576519303491 4.812044815343636 2020215 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07207001478996074 6.997930988661695 3.4273000092469443 4.973690187162979 2420260 0


Pure best response payoff estimated to be 5.4936 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (3, 0)
Current player 0 and current strategies (3, 0) took 5.77 seconds to finish estimate with resulting utilities: [7.2904 3.3072]
Estimating current strategies:  (3, 1)
Current player 0 and current strategies (3, 1) took 8.83 seconds to finish estimate with resulting utilities: [5.0786 5.3002]
Estimating current strategies:  (3, 2)
Current player 0 and current strategies (3, 2) took 8.16 seconds to finish estimate with resulting utilities: [5.3898 6.0664]
Estimating current strategies:  (3, 3)
Current player 0 and current strategies (3, 3) took 6.29 seconds to finish estimate with resulting utilities: [7.8032 4.3844]
Computing meta_strategies
Exited RRD with total regret 0.18619695932271796 that was less than regret lambda 0.18620689655172412 after 4002 iterations 
NEW LAMBDA 0.1793103448275862
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3      
    0     1.84           2.76           3.10           3.31      
    1     7.41           4.89           4.21           5.30      
    2     7.39           5.66           4.68           6.07      
    3     7.29           5.08           5.39           6.09      

 

Player 1 Payoff matrix: 

           0              1              2              3      
    0     1.84           7.41           7.39           7.29      
    1     2.76           4.89           5.66           5.08      
    2     3.10           4.21           4.68           5.39      
    3     3.31           5.30           6.07           6.09      

 

Social Welfare Sum Matrix: 

           0              1              2              3      
    0     3.69          10.17          10.49          10.60      
    1    10.17           9.78           9.87          10.38      
    2    10.49           9.87           9.37          11.46      
    3    10.60          10.38          11.46          12.19      

 

Metagame probabilities: 
Player #0: 0.0001  0.024  0.3145  0.6613  
Player #1: 0.0001  0.024  0.3145  0.6613  
Iteration : 3
Time so far: 15650.166719675064
Approximating Best Response
Training best response:  True 0.085
rtg values check:  (?,) (?, 1)
2023-08-02 04:15:27.330014: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_27/weights_2/Adam_1/Assign' id:7831 op device:{requested: '', assigned: ''} def:{{{node mlp_27/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_27/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_27/weights_2/Adam_1, mlp_27/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07223811088662781 7.084234833102865 3.42477031503756 4.983474864113732 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07600469296081389 7.244349850860297 3.4020512854351717 5.12619205988111 420040 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07784282473668869 7.1124438321479015 3.383956477575213 5.268964750690579 820085 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07746624036505015 6.994571461634976 3.364561218342611 5.412217911174921 1220114 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07534450953257193 6.870805597916627 3.342090928554535 5.557922908130494 1620159 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07271204689624516 6.756683808369715 3.315682413832086 5.707254712260773 2020196 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07021460722319763 6.65126987292072 3.28567896255358 5.854897309055128 2420229 0
Recovering previous policy with expected return of 7.027972027972028. Long term value was 6.6888 and short term was 6.747.


Pure best response payoff estimated to be 7.0874 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (4, 0)
Current player 0 and current strategies (4, 0) took 5.89 seconds to finish estimate with resulting utilities: [7.3338 3.3914]
Estimating current strategies:  (4, 1)
Current player 0 and current strategies (4, 1) took 8.86 seconds to finish estimate with resulting utilities: [5.1648 5.2904]
Estimating current strategies:  (4, 2)
Current player 0 and current strategies (4, 2) took 8.32 seconds to finish estimate with resulting utilities: [5.3756 6.1682]
Estimating current strategies:  (4, 3)
Current player 0 and current strategies (4, 3) took 6.38 seconds to finish estimate with resulting utilities: [7.8034 4.313 ]
Estimating current strategies:  (4, 4)
Current player 0 and current strategies (4, 4) took 6.35 seconds to finish estimate with resulting utilities: [7.7914 4.3738]
Computing meta_strategies
Exited RRD with total regret 0.17919562553012014 that was less than regret lambda 0.1793103448275862 after 3260 iterations 
NEW LAMBDA 0.1724137931034483
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4      
    0     1.84           2.76           3.10           3.31           3.39      
    1     7.41           4.89           4.21           5.30           5.29      
    2     7.39           5.66           4.68           6.07           6.17      
    3     7.29           5.08           5.39           6.09           4.31      
    4     7.33           5.16           5.38           7.80           6.08      

 

Player 1 Payoff matrix: 

           0              1              2              3              4      
    0     1.84           7.41           7.39           7.29           7.33      
    1     2.76           4.89           5.66           5.08           5.16      
    2     3.10           4.21           4.68           5.39           5.38      
    3     3.31           5.30           6.07           6.09           7.80      
    4     3.39           5.29           6.17           4.31           6.08      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4      
    0     3.69          10.17          10.49          10.60          10.73      
    1    10.17           9.78           9.87          10.38          10.46      
    2    10.49           9.87           9.37          11.46          11.54      
    3    10.60          10.38          11.46          12.19          12.12      
    4    10.73          10.46          11.54          12.12          12.17      

 

Metagame probabilities: 
Player #0: 0.0001  0.0252  0.2665  0.0211  0.6871  
Player #1: 0.0001  0.0252  0.2665  0.0211  0.6871  
Iteration : 4
Time so far: 21619.991187095642
Approximating Best Response
Training best response:  True 0.08000000000000002
rtg values check:  (?,) (?, 1)
2023-08-02 05:54:57.211054: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_34/weights_2/Adam_1/Assign' id:9821 op device:{requested: '', assigned: ''} def:{{{node mlp_34/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_34/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_34/weights_2/Adam_1, mlp_34/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0692354591264431 6.682982915616864 3.268884395908665 5.927019087183546 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07211561023623712 6.8701672571742405 3.255416549537262 6.009498768979844 420037 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07350947459100574 6.768506892265812 3.2456600090081547 6.092050577621414 820075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07326304398079816 6.666647835668808 3.230682005931762 6.189870371333147 1220120 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07165990821595841 6.566802420185561 3.210036626946567 6.3025915362377445 1620159 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06966496885465505 6.486990217792178 3.1853146893306845 6.421840150419274 2020190 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06766659800786283 6.408448606748192 3.152829145675169 6.545075277214608 2420227 0
Recovering previous policy with expected return of 7.185814185814186. Long term value was 6.8792 and short term was 6.911.


Pure best response payoff estimated to be 7.3214 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (5, 0)
Current player 0 and current strategies (5, 0) took 5.98 seconds to finish estimate with resulting utilities: [7.3852 3.338 ]
Estimating current strategies:  (5, 1)
Current player 0 and current strategies (5, 1) took 8.91 seconds to finish estimate with resulting utilities: [5.2076 5.296 ]
Estimating current strategies:  (5, 2)
Current player 0 and current strategies (5, 2) took 8.28 seconds to finish estimate with resulting utilities: [5.379  6.1372]
Estimating current strategies:  (5, 3)
Current player 0 and current strategies (5, 3) took 6.44 seconds to finish estimate with resulting utilities: [7.8138 4.3858]
Estimating current strategies:  (5, 4)
Current player 0 and current strategies (5, 4) took 6.44 seconds to finish estimate with resulting utilities: [7.7964 4.3378]
Estimating current strategies:  (5, 5)
Current player 0 and current strategies (5, 5) took 6.37 seconds to finish estimate with resulting utilities: [7.8066 4.3258]
Computing meta_strategies
Exited RRD with total regret 0.1723993561379249 that was less than regret lambda 0.1724137931034483 after 3254 iterations 
NEW LAMBDA 0.16551724137931034
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5      
    0     1.84           2.76           3.10           3.31           3.39           3.34      
    1     7.41           4.89           4.21           5.30           5.29           5.30      
    2     7.39           5.66           4.68           6.07           6.17           6.14      
    3     7.29           5.08           5.39           6.09           4.31           4.39      
    4     7.33           5.16           5.38           7.80           6.08           4.34      
    5     7.39           5.21           5.38           7.81           7.80           6.07      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5      
    0     1.84           7.41           7.39           7.29           7.33           7.39      
    1     2.76           4.89           5.66           5.08           5.16           5.21      
    2     3.10           4.21           4.68           5.39           5.38           5.38      
    3     3.31           5.30           6.07           6.09           7.80           7.81      
    4     3.39           5.29           6.17           4.31           6.08           7.80      
    5     3.34           5.30           6.14           4.39           4.34           6.07      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5      
    0     3.69          10.17          10.49          10.60          10.73          10.72      
    1    10.17           9.78           9.87          10.38          10.46          10.50      
    2    10.49           9.87           9.37          11.46          11.54          11.52      
    3    10.60          10.38          11.46          12.19          12.12          12.20      
    4    10.73          10.46          11.54          12.12          12.17          12.13      
    5    10.72          10.50          11.52          12.20          12.13          12.13      

 

Metagame probabilities: 
Player #0: 0.0001  0.0189  0.2094  0.0082  0.0224  0.741  
Player #1: 0.0001  0.0189  0.2094  0.0082  0.0224  0.741  
Iteration : 5
Time so far: 28077.940210819244
Approximating Best Response
Training best response:  True 0.07500000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 07:42:35.174693: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_41/weights_2/Adam_1/Assign' id:11811 op device:{requested: '', assigned: ''} def:{{{node mlp_41/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_41/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_41/weights_2/Adam_1, mlp_41/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06623039576285343 6.410352179549123 3.121870053196681 6.645334232382466 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06866219330616918 6.569898496557165 3.113878268701059 6.688597320522469 420034 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06996414888794167 6.48764412248735 3.1086359507059877 6.734453660259402 820069 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06998761929320824 6.407151628974434 3.103215899300742 6.783877182908243 1220100 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06881460177005669 6.327059135696515 3.0929675492786224 6.84329963616068 1620140 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.067231163042381 6.24626318830528 3.0802256251960403 6.910588462874827 2020180 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06564360427353172 6.169702443399737 3.0636689242086104 6.98163653746059 2420219 0


Pure best response payoff estimated to be 7.4482 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (6, 0)
Current player 0 and current strategies (6, 0) took 6.89 seconds to finish estimate with resulting utilities: [7.411  3.0036]
Estimating current strategies:  (6, 1)
Current player 0 and current strategies (6, 1) took 10.47 seconds to finish estimate with resulting utilities: [4.8094 5.1988]
Estimating current strategies:  (6, 2)
Current player 0 and current strategies (6, 2) took 9.37 seconds to finish estimate with resulting utilities: [5.3602 5.9266]
Estimating current strategies:  (6, 3)
Current player 0 and current strategies (6, 3) took 5.99 seconds to finish estimate with resulting utilities: [7.9168 4.594 ]
Estimating current strategies:  (6, 4)
Current player 0 and current strategies (6, 4) took 6.02 seconds to finish estimate with resulting utilities: [7.8272 4.5742]
Estimating current strategies:  (6, 5)
Current player 0 and current strategies (6, 5) took 5.95 seconds to finish estimate with resulting utilities: [7.9102 4.6074]
Estimating current strategies:  (6, 6)
Current player 0 and current strategies (6, 6) took 9.17 seconds to finish estimate with resulting utilities: [5.9744 3.9772]
Computing meta_strategies
Exited RRD with total regret 0.16551119973993522 that was less than regret lambda 0.16551724137931034 after 7525 iterations 
NEW LAMBDA 0.1586206896551724
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6      
    0     1.84           2.76           3.10           3.31           3.39           3.34           3.00      
    1     7.41           4.89           4.21           5.30           5.29           5.30           5.20      
    2     7.39           5.66           4.68           6.07           6.17           6.14           5.93      
    3     7.29           5.08           5.39           6.09           4.31           4.39           4.59      
    4     7.33           5.16           5.38           7.80           6.08           4.34           4.57      
    5     7.39           5.21           5.38           7.81           7.80           6.07           4.61      
    6     7.41           4.81           5.36           7.92           7.83           7.91           4.98      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6      
    0     1.84           7.41           7.39           7.29           7.33           7.39           7.41      
    1     2.76           4.89           5.66           5.08           5.16           5.21           4.81      
    2     3.10           4.21           4.68           5.39           5.38           5.38           5.36      
    3     3.31           5.30           6.07           6.09           7.80           7.81           7.92      
    4     3.39           5.29           6.17           4.31           6.08           7.80           7.83      
    5     3.34           5.30           6.14           4.39           4.34           6.07           7.91      
    6     3.00           5.20           5.93           4.59           4.57           4.61           4.98      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6      
    0     3.69          10.17          10.49          10.60          10.73          10.72          10.41      
    1    10.17           9.78           9.87          10.38          10.46          10.50          10.01      
    2    10.49           9.87           9.37          11.46          11.54          11.52          11.29      
    3    10.60          10.38          11.46          12.19          12.12          12.20          12.51      
    4    10.73          10.46          11.54          12.12          12.17          12.13          12.40      
    5    10.72          10.50          11.52          12.20          12.13          12.13          12.52      
    6    10.41          10.01          11.29          12.51          12.40          12.52           9.95      

 

Metagame probabilities: 
Player #0: 0.0001  0.0027  0.4462  0.0014  0.0032  0.0296  0.5168  
Player #1: 0.0001  0.0027  0.4462  0.0014  0.0032  0.0296  0.5168  
Iteration : 6
Time so far: 34623.842609643936
Approximating Best Response
Training best response:  True 0.06999999999999999
rtg values check:  (?,) (?, 1)
2023-08-02 09:31:41.461051: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_48/weights_2/Adam_1/Assign' id:13801 op device:{requested: '', assigned: ''} def:{{{node mlp_48/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_48/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_48/weights_2/Adam_1, mlp_48/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06458521173278156 6.147867237421415 3.0460445593642222 7.0430724317304865 20001 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0664076662672484 6.247438104878812 3.0445596935716717 7.07692569325304 420039 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06688881504090749 6.235829725104836 3.0351971627716265 7.119894547555207 820073 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.066483972734729 6.215717656968203 3.023014532917395 7.1735482001565885 1220108 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06536131005836535 6.177689530791974 3.010925638826241 7.232306127160412 1620149 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06393968813516113 6.137575529186279 2.9981609523497044 7.2965804492593085 2020197 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06259371234052488 6.093292640985433 2.9867059833524516 7.3617276548431825 2420224 0


Pure best response payoff estimated to be 6.5444 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (7, 0)
Current player 0 and current strategies (7, 0) took 6.82 seconds to finish estimate with resulting utilities: [7.297  3.0988]
Estimating current strategies:  (7, 1)
Current player 0 and current strategies (7, 1) took 9.06 seconds to finish estimate with resulting utilities: [5.0794 6.0294]
Estimating current strategies:  (7, 2)
Current player 0 and current strategies (7, 2) took 7.74 seconds to finish estimate with resulting utilities: [5.6068 6.6914]
Estimating current strategies:  (7, 3)
Current player 0 and current strategies (7, 3) took 5.61 seconds to finish estimate with resulting utilities: [8.0186 5.2926]
Estimating current strategies:  (7, 4)
Current player 0 and current strategies (7, 4) took 5.58 seconds to finish estimate with resulting utilities: [8.048  5.2652]
Estimating current strategies:  (7, 5)
Current player 0 and current strategies (7, 5) took 5.55 seconds to finish estimate with resulting utilities: [7.9586 5.3108]
Estimating current strategies:  (7, 6)
Current player 0 and current strategies (7, 6) took 7.02 seconds to finish estimate with resulting utilities: [6.2764 4.5178]
Estimating current strategies:  (7, 7)
Current player 0 and current strategies (7, 7) took 6.72 seconds to finish estimate with resulting utilities: [6.8184 4.4102]
Computing meta_strategies
Exited RRD with total regret 0.1585934279897261 that was less than regret lambda 0.1586206896551724 after 6762 iterations 
NEW LAMBDA 0.15172413793103448
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0     1.84           2.76           3.10           3.31           3.39           3.34           3.00           3.10      
    1     7.41           4.89           4.21           5.30           5.29           5.30           5.20           6.03      
    2     7.39           5.66           4.68           6.07           6.17           6.14           5.93           6.69      
    3     7.29           5.08           5.39           6.09           4.31           4.39           4.59           5.29      
    4     7.33           5.16           5.38           7.80           6.08           4.34           4.57           5.27      
    5     7.39           5.21           5.38           7.81           7.80           6.07           4.61           5.31      
    6     7.41           4.81           5.36           7.92           7.83           7.91           4.98           4.52      
    7     7.30           5.08           5.61           8.02           8.05           7.96           6.28           5.61      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7      
    0     1.84           7.41           7.39           7.29           7.33           7.39           7.41           7.30      
    1     2.76           4.89           5.66           5.08           5.16           5.21           4.81           5.08      
    2     3.10           4.21           4.68           5.39           5.38           5.38           5.36           5.61      
    3     3.31           5.30           6.07           6.09           7.80           7.81           7.92           8.02      
    4     3.39           5.29           6.17           4.31           6.08           7.80           7.83           8.05      
    5     3.34           5.30           6.14           4.39           4.34           6.07           7.91           7.96      
    6     3.00           5.20           5.93           4.59           4.57           4.61           4.98           6.28      
    7     3.10           6.03           6.69           5.29           5.27           5.31           4.52           5.61      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7      
    0     3.69          10.17          10.49          10.60          10.73          10.72          10.41          10.40      
    1    10.17           9.78           9.87          10.38          10.46          10.50          10.01          11.11      
    2    10.49           9.87           9.37          11.46          11.54          11.52          11.29          12.30      
    3    10.60          10.38          11.46          12.19          12.12          12.20          12.51          13.31      
    4    10.73          10.46          11.54          12.12          12.17          12.13          12.40          13.31      
    5    10.72          10.50          11.52          12.20          12.13          12.13          12.52          13.27      
    6    10.41          10.01          11.29          12.51          12.40          12.52           9.95          10.79      
    7    10.40          11.11          12.30          13.31          13.31          13.27          10.79          11.23      

 

Metagame probabilities: 
Player #0: 0.0001  0.0057  0.4384  0.0021  0.0041  0.0195  0.0047  0.5254  
Player #1: 0.0001  0.0057  0.4384  0.0021  0.0041  0.0195  0.0047  0.5254  
Iteration : 7
Time so far: 40877.72986960411
Approximating Best Response
Training best response:  True 0.065
rtg values check:  (?,) (?, 1)
2023-08-02 11:15:55.651572: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_55/weights_2/Adam_1/Assign' id:15791 op device:{requested: '', assigned: ''} def:{{{node mlp_55/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_55/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_55/weights_2/Adam_1, mlp_55/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06208330469662107 6.109975985155303 2.980977012306031 7.393994613605848 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06314318652587497 6.190210000868178 2.9710273003603525 7.4450666415205555 420035 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06344957725505508 6.147527164227013 2.9605010355424435 7.491751581697944 820071 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06308376736169094 6.10668726307658 2.9498685230416504 7.541854735301358 1220103 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06209438960267045 6.064855442752133 2.938304530157076 7.598768633801323 1620138 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.060889096282191545 6.021519198123435 2.927716823913208 7.657771449388709 2020177 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05973527334526318 5.97556537968052 2.917431102705964 7.7187468939861015 2420212 0
Recovering previous policy with expected return of 6.208791208791209. Long term value was 5.9618 and short term was 5.938.


Pure best response payoff estimated to be 6.7332 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (8, 0)
Current player 0 and current strategies (8, 0) took 6.73 seconds to finish estimate with resulting utilities: [7.2932 3.1368]
Estimating current strategies:  (8, 1)
Current player 0 and current strategies (8, 1) took 9.14 seconds to finish estimate with resulting utilities: [5.027  6.1074]
Estimating current strategies:  (8, 2)
Current player 0 and current strategies (8, 2) took 7.82 seconds to finish estimate with resulting utilities: [5.6196 6.6616]
Estimating current strategies:  (8, 3)
Current player 0 and current strategies (8, 3) took 5.62 seconds to finish estimate with resulting utilities: [8.0208 5.3   ]
Estimating current strategies:  (8, 4)
Current player 0 and current strategies (8, 4) took 5.63 seconds to finish estimate with resulting utilities: [7.997  5.2692]
Estimating current strategies:  (8, 5)
Current player 0 and current strategies (8, 5) took 5.59 seconds to finish estimate with resulting utilities: [8.0052 5.2922]
Estimating current strategies:  (8, 6)
Current player 0 and current strategies (8, 6) took 7.07 seconds to finish estimate with resulting utilities: [6.2488 4.5392]
Estimating current strategies:  (8, 7)
Current player 0 and current strategies (8, 7) took 6.87 seconds to finish estimate with resulting utilities: [6.8262 4.3202]
Estimating current strategies:  (8, 8)
Current player 0 and current strategies (8, 8) took 6.8 seconds to finish estimate with resulting utilities: [6.8442 4.3402]
Computing meta_strategies
Exited RRD with total regret 0.15169182408576365 that was less than regret lambda 0.15172413793103448 after 6641 iterations 
NEW LAMBDA 0.14482758620689656
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0     1.84           2.76           3.10           3.31           3.39           3.34           3.00           3.10           3.14      
    1     7.41           4.89           4.21           5.30           5.29           5.30           5.20           6.03           6.11      
    2     7.39           5.66           4.68           6.07           6.17           6.14           5.93           6.69           6.66      
    3     7.29           5.08           5.39           6.09           4.31           4.39           4.59           5.29           5.30      
    4     7.33           5.16           5.38           7.80           6.08           4.34           4.57           5.27           5.27      
    5     7.39           5.21           5.38           7.81           7.80           6.07           4.61           5.31           5.29      
    6     7.41           4.81           5.36           7.92           7.83           7.91           4.98           4.52           4.54      
    7     7.30           5.08           5.61           8.02           8.05           7.96           6.28           5.61           4.32      
    8     7.29           5.03           5.62           8.02           8.00           8.01           6.25           6.83           5.59      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8      
    0     1.84           7.41           7.39           7.29           7.33           7.39           7.41           7.30           7.29      
    1     2.76           4.89           5.66           5.08           5.16           5.21           4.81           5.08           5.03      
    2     3.10           4.21           4.68           5.39           5.38           5.38           5.36           5.61           5.62      
    3     3.31           5.30           6.07           6.09           7.80           7.81           7.92           8.02           8.02      
    4     3.39           5.29           6.17           4.31           6.08           7.80           7.83           8.05           8.00      
    5     3.34           5.30           6.14           4.39           4.34           6.07           7.91           7.96           8.01      
    6     3.00           5.20           5.93           4.59           4.57           4.61           4.98           6.28           6.25      
    7     3.10           6.03           6.69           5.29           5.27           5.31           4.52           5.61           6.83      
    8     3.14           6.11           6.66           5.30           5.27           5.29           4.54           4.32           5.59      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8      
    0     3.69          10.17          10.49          10.60          10.73          10.72          10.41          10.40          10.43      
    1    10.17           9.78           9.87          10.38          10.46          10.50          10.01          11.11          11.13      
    2    10.49           9.87           9.37          11.46          11.54          11.52          11.29          12.30          12.28      
    3    10.60          10.38          11.46          12.19          12.12          12.20          12.51          13.31          13.32      
    4    10.73          10.46          11.54          12.12          12.17          12.13          12.40          13.31          13.27      
    5    10.72          10.50          11.52          12.20          12.13          12.13          12.52          13.27          13.30      
    6    10.41          10.01          11.29          12.51          12.40          12.52           9.95          10.79          10.79      
    7    10.40          11.11          12.30          13.31          13.31          13.27          10.79          11.23          11.15      
    8    10.43          11.13          12.28          13.32          13.27          13.30          10.79          11.15          11.18      

 

Metagame probabilities: 
Player #0: 0.0001  0.0087  0.4396  0.0026  0.0046  0.0161  0.0026  0.0071  0.5186  
Player #1: 0.0001  0.0087  0.4396  0.0026  0.0046  0.0161  0.0026  0.0071  0.5186  
Iteration : 8
Time so far: 47513.75672292709
Approximating Best Response
Training best response:  True 0.06
rtg values check:  (?,) (?, 1)
2023-08-02 13:06:31.954231: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_62/weights_2/Adam_1/Assign' id:17781 op device:{requested: '', assigned: ''} def:{{{node mlp_62/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_62/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_62/weights_2/Adam_1, mlp_62/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05940687843316971 5.989372870851631 2.9137393813910544 7.742488366315034 20004 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06039568019206677 6.062025608672953 2.906168679481806 7.785712580574518 420041 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.060736218043273595 6.023623023457873 2.898721837099809 7.823978160980594 820083 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06046270374907376 5.988810351950505 2.8901501231412827 7.868202334315257 1220123 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0596159266782952 5.953324326125788 2.8798782492444195 7.919015870525787 1620151 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05858805514832401 5.914283628314345 2.86951320125706 7.97174508297791 2020185 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.057603613952613285 5.875896181827738 2.859447182030429 8.02397011525683 2420215 0
Recovering previous policy with expected return of 6.371628371628372. Long term value was 6.2382 and short term was 6.168.


Pure best response payoff estimated to be 6.8998 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (9, 0)
Current player 0 and current strategies (9, 0) took 6.86 seconds to finish estimate with resulting utilities: [7.323  3.1074]
Estimating current strategies:  (9, 1)
Current player 0 and current strategies (9, 1) took 9.11 seconds to finish estimate with resulting utilities: [5.0522 6.135 ]
Estimating current strategies:  (9, 2)
Current player 0 and current strategies (9, 2) took 7.84 seconds to finish estimate with resulting utilities: [5.604  6.7158]
Estimating current strategies:  (9, 3)
Current player 0 and current strategies (9, 3) took 5.74 seconds to finish estimate with resulting utilities: [7.9572 5.3394]
Estimating current strategies:  (9, 4)
Current player 0 and current strategies (9, 4) took 5.71 seconds to finish estimate with resulting utilities: [7.9868 5.239 ]
Estimating current strategies:  (9, 5)
Current player 0 and current strategies (9, 5) took 5.67 seconds to finish estimate with resulting utilities: [7.9754 5.2794]
Estimating current strategies:  (9, 6)
Current player 0 and current strategies (9, 6) took 7.14 seconds to finish estimate with resulting utilities: [6.3106 4.4414]
Estimating current strategies:  (9, 7)
Current player 0 and current strategies (9, 7) took 6.94 seconds to finish estimate with resulting utilities: [6.8276 4.3916]
Estimating current strategies:  (9, 8)
Current player 0 and current strategies (9, 8) took 6.85 seconds to finish estimate with resulting utilities: [6.8228 4.378 ]
Estimating current strategies:  (9, 9)
Current player 0 and current strategies (9, 9) took 6.82 seconds to finish estimate with resulting utilities: [6.846  4.3694]
Computing meta_strategies
Exited RRD with total regret 0.14481922157167748 that was less than regret lambda 0.14482758620689656 after 6602 iterations 
NEW LAMBDA 0.13793103448275862
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     1.84           2.76           3.10           3.31           3.39           3.34           3.00           3.10           3.14           3.11      
    1     7.41           4.89           4.21           5.30           5.29           5.30           5.20           6.03           6.11           6.13      
    2     7.39           5.66           4.68           6.07           6.17           6.14           5.93           6.69           6.66           6.72      
    3     7.29           5.08           5.39           6.09           4.31           4.39           4.59           5.29           5.30           5.34      
    4     7.33           5.16           5.38           7.80           6.08           4.34           4.57           5.27           5.27           5.24      
    5     7.39           5.21           5.38           7.81           7.80           6.07           4.61           5.31           5.29           5.28      
    6     7.41           4.81           5.36           7.92           7.83           7.91           4.98           4.52           4.54           4.44      
    7     7.30           5.08           5.61           8.02           8.05           7.96           6.28           5.61           4.32           4.39      
    8     7.29           5.03           5.62           8.02           8.00           8.01           6.25           6.83           5.59           4.38      
    9     7.32           5.05           5.60           7.96           7.99           7.98           6.31           6.83           6.82           5.61      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     1.84           7.41           7.39           7.29           7.33           7.39           7.41           7.30           7.29           7.32      
    1     2.76           4.89           5.66           5.08           5.16           5.21           4.81           5.08           5.03           5.05      
    2     3.10           4.21           4.68           5.39           5.38           5.38           5.36           5.61           5.62           5.60      
    3     3.31           5.30           6.07           6.09           7.80           7.81           7.92           8.02           8.02           7.96      
    4     3.39           5.29           6.17           4.31           6.08           7.80           7.83           8.05           8.00           7.99      
    5     3.34           5.30           6.14           4.39           4.34           6.07           7.91           7.96           8.01           7.98      
    6     3.00           5.20           5.93           4.59           4.57           4.61           4.98           6.28           6.25           6.31      
    7     3.10           6.03           6.69           5.29           5.27           5.31           4.52           5.61           6.83           6.83      
    8     3.14           6.11           6.66           5.30           5.27           5.29           4.54           4.32           5.59           6.82      
    9     3.11           6.13           6.72           5.34           5.24           5.28           4.44           4.39           4.38           5.61      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9      
    0     3.69          10.17          10.49          10.60          10.73          10.72          10.41          10.40          10.43          10.43      
    1    10.17           9.78           9.87          10.38          10.46          10.50          10.01          11.11          11.13          11.19      
    2    10.49           9.87           9.37          11.46          11.54          11.52          11.29          12.30          12.28          12.32      
    3    10.60          10.38          11.46          12.19          12.12          12.20          12.51          13.31          13.32          13.30      
    4    10.73          10.46          11.54          12.12          12.17          12.13          12.40          13.31          13.27          13.23      
    5    10.72          10.50          11.52          12.20          12.13          12.13          12.52          13.27          13.30          13.25      
    6    10.41          10.01          11.29          12.51          12.40          12.52           9.95          10.79          10.79          10.75      
    7    10.40          11.11          12.30          13.31          13.31          13.27          10.79          11.23          11.15          11.22      
    8    10.43          11.13          12.28          13.32          13.27          13.30          10.79          11.15          11.18          11.20      
    9    10.43          11.19          12.32          13.30          13.23          13.25          10.75          11.22          11.20          11.22      

 

Metagame probabilities: 
Player #0: 0.0001  0.0093  0.4581  0.003  0.0042  0.0127  0.0012  0.0034  0.0098  0.4982  
Player #1: 0.0001  0.0093  0.4581  0.003  0.0042  0.0127  0.0012  0.0034  0.0098  0.4982  
Iteration : 9
Time so far: 54690.82280135155
Approximating Best Response
Training best response:  True 0.05500000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 15:06:09.269081: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_69/weights_2/Adam_1/Assign' id:19771 op device:{requested: '', assigned: ''} def:{{{node mlp_69/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_69/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_69/weights_2/Adam_1, mlp_69/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05719240103389477 5.882567956609241 2.854243709495512 8.051337095338056 20002 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05811849174554375 5.944354997595151 2.848756323158741 8.082658785800785 420036 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05847400369671259 5.915580159601618 2.84318546902938 8.111487690015553 820075 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.058318498453055444 5.886982017793963 2.837364960043661 8.143143517260254 1220105 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05762465716409592 5.856375363970559 2.8309775960445402 8.17817273047297 1620135 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05675348140486349 5.8240262068808075 2.8246250192262234 8.213620509040217 2020170 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05591231974239879 5.7879561074330255 2.815880127008145 8.249586060432884 2420201 0
Recovering previous policy with expected return of 6.422577422577422. Long term value was 6.183 and short term was 6.166.


Pure best response payoff estimated to be 6.9058 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (10, 0)
Current player 0 and current strategies (10, 0) took 6.78 seconds to finish estimate with resulting utilities: [7.3    3.1256]
Estimating current strategies:  (10, 1)
Current player 0 and current strategies (10, 1) took 9.0 seconds to finish estimate with resulting utilities: [5.0766 6.1052]
Estimating current strategies:  (10, 2)
Current player 0 and current strategies (10, 2) took 7.79 seconds to finish estimate with resulting utilities: [5.649  6.6804]
Estimating current strategies:  (10, 3)
Current player 0 and current strategies (10, 3) took 5.69 seconds to finish estimate with resulting utilities: [7.9942 5.2134]
Estimating current strategies:  (10, 4)
Current player 0 and current strategies (10, 4) took 5.69 seconds to finish estimate with resulting utilities: [7.992  5.2782]
Estimating current strategies:  (10, 5)
Current player 0 and current strategies (10, 5) took 5.7 seconds to finish estimate with resulting utilities: [7.9974 5.2578]
Estimating current strategies:  (10, 6)
Current player 0 and current strategies (10, 6) took 7.21 seconds to finish estimate with resulting utilities: [6.236 4.523]
Estimating current strategies:  (10, 7)
Current player 0 and current strategies (10, 7) took 6.88 seconds to finish estimate with resulting utilities: [6.841  4.3368]
Estimating current strategies:  (10, 8)
Current player 0 and current strategies (10, 8) took 6.89 seconds to finish estimate with resulting utilities: [6.8178 4.4044]
Estimating current strategies:  (10, 9)
Current player 0 and current strategies (10, 9) took 6.86 seconds to finish estimate with resulting utilities: [6.8256 4.3334]
Estimating current strategies:  (10, 10)
Current player 0 and current strategies (10, 10) took 6.87 seconds to finish estimate with resulting utilities: [6.765  4.3082]
Computing meta_strategies
Exited RRD with total regret 0.1378973777857322 that was less than regret lambda 0.13793103448275862 after 6581 iterations 
NEW LAMBDA 0.13103448275862067
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     1.84           2.76           3.10           3.31           3.39           3.34           3.00           3.10           3.14           3.11           3.13      
    1     7.41           4.89           4.21           5.30           5.29           5.30           5.20           6.03           6.11           6.13           6.11      
    2     7.39           5.66           4.68           6.07           6.17           6.14           5.93           6.69           6.66           6.72           6.68      
    3     7.29           5.08           5.39           6.09           4.31           4.39           4.59           5.29           5.30           5.34           5.21      
    4     7.33           5.16           5.38           7.80           6.08           4.34           4.57           5.27           5.27           5.24           5.28      
    5     7.39           5.21           5.38           7.81           7.80           6.07           4.61           5.31           5.29           5.28           5.26      
    6     7.41           4.81           5.36           7.92           7.83           7.91           4.98           4.52           4.54           4.44           4.52      
    7     7.30           5.08           5.61           8.02           8.05           7.96           6.28           5.61           4.32           4.39           4.34      
    8     7.29           5.03           5.62           8.02           8.00           8.01           6.25           6.83           5.59           4.38           4.40      
    9     7.32           5.05           5.60           7.96           7.99           7.98           6.31           6.83           6.82           5.61           4.33      
   10     7.30           5.08           5.65           7.99           7.99           8.00           6.24           6.84           6.82           6.83           5.54      

 

Player 1 Payoff matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     1.84           7.41           7.39           7.29           7.33           7.39           7.41           7.30           7.29           7.32           7.30      
    1     2.76           4.89           5.66           5.08           5.16           5.21           4.81           5.08           5.03           5.05           5.08      
    2     3.10           4.21           4.68           5.39           5.38           5.38           5.36           5.61           5.62           5.60           5.65      
    3     3.31           5.30           6.07           6.09           7.80           7.81           7.92           8.02           8.02           7.96           7.99      
    4     3.39           5.29           6.17           4.31           6.08           7.80           7.83           8.05           8.00           7.99           7.99      
    5     3.34           5.30           6.14           4.39           4.34           6.07           7.91           7.96           8.01           7.98           8.00      
    6     3.00           5.20           5.93           4.59           4.57           4.61           4.98           6.28           6.25           6.31           6.24      
    7     3.10           6.03           6.69           5.29           5.27           5.31           4.52           5.61           6.83           6.83           6.84      
    8     3.14           6.11           6.66           5.30           5.27           5.29           4.54           4.32           5.59           6.82           6.82      
    9     3.11           6.13           6.72           5.34           5.24           5.28           4.44           4.39           4.38           5.61           6.83      
   10     3.13           6.11           6.68           5.21           5.28           5.26           4.52           4.34           4.40           4.33           5.54      

 

Social Welfare Sum Matrix: 

           0              1              2              3              4              5              6              7              8              9             10      
    0     3.69          10.17          10.49          10.60          10.73          10.72          10.41          10.40          10.43          10.43          10.43      
    1    10.17           9.78           9.87          10.38          10.46          10.50          10.01          11.11          11.13          11.19          11.18      
    2    10.49           9.87           9.37          11.46          11.54          11.52          11.29          12.30          12.28          12.32          12.33      
    3    10.60          10.38          11.46          12.19          12.12          12.20          12.51          13.31          13.32          13.30          13.21      
    4    10.73          10.46          11.54          12.12          12.17          12.13          12.40          13.31          13.27          13.23          13.27      
    5    10.72          10.50          11.52          12.20          12.13          12.13          12.52          13.27          13.30          13.25          13.26      
    6    10.41          10.01          11.29          12.51          12.40          12.52           9.95          10.79          10.79          10.75          10.76      
    7    10.40          11.11          12.30          13.31          13.31          13.27          10.79          11.23          11.15          11.22          11.18      
    8    10.43          11.13          12.28          13.32          13.27          13.30          10.79          11.15          11.18          11.20          11.22      
    9    10.43          11.19          12.32          13.30          13.23          13.25          10.75          11.22          11.20          11.22          11.16      
   10    10.43          11.18          12.33          13.21          13.27          13.26          10.76          11.18          11.22          11.16          11.07      

 

Metagame probabilities: 
Player #0: 0.0001  0.0099  0.4612  0.0025  0.0048  0.0111  0.0011  0.0019  0.0045  0.0109  0.4921  
Player #1: 0.0001  0.0099  0.4612  0.0025  0.0048  0.0111  0.0011  0.0019  0.0045  0.0109  0.4921  
Iteration : 10
Time so far: 61205.85540008545
Approximating Best Response
Training best response:  True 0.05
rtg values check:  (?,) (?, 1)
2023-08-02 16:54:45.034041: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_76/weights_2/Adam_1/Assign' id:21761 op device:{requested: '', assigned: ''} def:{{{node mlp_76/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_76/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_76/weights_2/Adam_1, mlp_76/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 2000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05564375153986542 5.796822401182264 2.8114734399764933 8.26604796262863 20005 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05653245243085978 5.85216761166391 2.8071775236911933 8.289973373051287 420034 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05693107988092805 5.8292298334629296 2.8033319855726058 8.309503856723843 820065 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05681886601288111 5.8031296436415705 2.798537400972242 8.333756606487091 1220099 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05622965044039374 5.777951803578157 2.7936791328666364 8.36043417141423 1620139 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.055479631323456235 5.751997505510166 2.7885821354989893 8.389842346207525 2020170 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05475179912531883 5.725229575502197 2.783391271495085 8.421546287333616 2420203 0


slurmstepd: error: *** JOB 57046051 ON gl3049 CANCELLED AT 2023-08-02T18:18:41 ***
