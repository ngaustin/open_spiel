Job Id listed below:
57062221

Lmod is automatically replacing "clang/2022.1.2" with "intel/2022.1.2".


Lmod is automatically replacing "gcc/10.3.0" with "clang/2022.1.2".

-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: gcc/10.3.0 (required by: python/3.10.4)
-------------------------------------------------------------------------------

Lmod is automatically replacing "clang/2022.1.2" with "gcc/10.3.0".

/var/spool/slurmd.spool/job57062221/slurm_script: line 29: regret_lambda: command not found
/var/spool/slurmd.spool/job57062221/slurm_script: line 30: regret_steps: command not found
2023-08-02 12:24:43.094536: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-02 12:24:47.772793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/anrigu/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0802 12:24:59.511124 22668800547712 rl_environment.py:187] Using game instance: harvest
Num cpu cores:  36
 
 Did not find gpu device. Using cpu. 
 
Agent Arguments: 
batch_size: 64
discount_factor: 0.99
double: True
epsilon_decay_duration: 100000
hidden_layers_sizes: [50, 50]
learn_every: 1
learning_rate: 0.0003
min_buffer_size_to_learn: 20000
num_actions: 8
optimizer_str: adam
replay_buffer_capacity: 100000
session: <tensorflow.python.client.session.Session object at 0x149db48ded70>
state_representation_size: 84
symmetric: True
update_target_network_every: 500


Consensus Arguments: 
alpha: 5.0
batch_size: 128
beta: 0.5
clear_trajectories: False
consensus_imitation: False
consensus_oracle: trajectory_deep
deep_network_lr: 0.0003
device: /cpu:0
discount: 0.99
entropy_decay_duration: 0.5
epochs_ppo: 10
eps_clip: 0.1
eps_clip_value: 0.5
eta: 0.05
fine_tune: True
fine_tune_policy_lr: 3e-05
fine_tune_value_lr: 0.0003
hidden_layer_size: 50
imitation_mode: prob_action
joint_action: False
max_buffer_size_fine_tune: 100000
min_buffer_size_fine_tune: 10000
minibatches_ppo: 50
minimum_entropy: 0.8
n_hidden_layers: 2
num_players: 2
num_simulations_fit: 1
perturb_all: False
policy_constraint: 0.1
policy_constraint_steps: 20
ppo_entropy_regularization: 0.05
pretrained_policy_steps: 500
recovery_window: 200
regret_calculation_steps: 1500000
rewards_joint: False
session: <tensorflow.python.client.session.Session object at 0x149db48ded70>
sims_per_entry: 200
state_representation_size: 84
steps_fine_tune: 8000000
symmetric: True
tau: 0.001
training_epochs: 1
training_steps: 1000
transfer_policy_minimum_entropy: 0.0
update_target_every: 1


2023-08-02 12:25:00.076591: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2023-08-02 12:25:00.696756: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_2/bias_2/Adam_1/Assign' id:1071 op device:{requested: '', assigned: ''} def:{{{node mlp_2/bias_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_2/bias_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_2/bias_2/Adam_1, mlp_2/bias_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Using 200 sims per entry.
Rectifier : 
Perturbating oracle outputs : False
Sampling from marginals : True
Using prd_collab as strategy method.
Using probabilistic as training strategy selector.
Estimating current strategies:  (0, 0)
Current player 0 and current strategies (0, 0) took 18.43 seconds to finish estimate with resulting utilities: [50.095 50.35 ]
Exited RRD with total regret 0.0 that was less than regret lambda 5.0 after 0 iterations 

 Metagame display:
Player 0 Payoff matrix: 

           0      
    0    50.22      

 

Player 1 Payoff matrix: 

           0      
    0    50.22      

 

Social Welfare Sum Matrix: 

           0      
    0    100.44      

 

Iteration : 0
Time so far: 0.00018072128295898438
Approximating Best Response
Training best response:  True 0.1
rtg values check:  (?,) (?, 1)
2023-08-02 12:25:20.042571: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_6/weights_2/Adam_1/Assign' id:1925 op device:{requested: '', assigned: ''} def:{{{node mlp_6/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_6/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_6/weights_2/Adam_1, mlp_6/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading policies from offline learning: 

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.1161905623972416 25.32016944885254 2.031481146812439 0.001088519774202723 10857 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0996354304254055 16.009810543060304 1.8706928730010985 0.20189774334430693 216851 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.09176600947976113 13.325875186920166 1.8143980145454406 0.29234582781791685 419055 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.085963936150074 18.001611709594727 1.7847891211509705 0.37327142953872683 620857 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07947744578123092 16.36190023422241 1.7798556089401245 0.422093865275383 823701 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.07067807763814926 17.206412887573244 1.716420555114746 0.5172106385231018 1027018 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.06344852745532989 15.899342727661132 1.6678475618362427 0.6081423938274384 1229672 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.05985639803111553 18.361756896972658 1.6394084811210632 0.7092456638813018 1433077 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.058490054309368135 20.40766544342041 1.6379856944084168 0.8105637729167938 1637282 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.047508306056261065 17.8990891456604 1.5857412219047546 0.9771310925483704 1842694 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.037542277947068214 19.047557830810547 1.510078203678131 1.117053496837616 2051573 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03440734669566155 19.40003890991211 1.4821443557739258 1.2382941842079163 2258602 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.028175190277397632 22.976125717163086 1.426220214366913 1.3392076730728149 2466104 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02461601197719574 18.656444549560547 1.387855875492096 1.3925114631652833 2675636 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02057555438950658 18.04794063568115 1.37738037109375 1.629653525352478 2885176 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016756858117878436 24.67704544067383 1.2986971020698548 1.8278637170791625 3095340 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01305552776902914 21.943556213378905 1.242164170742035 2.0509097576141357 3307601 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00830474914982915 20.191365814208986 1.1636394262313843 2.0616000413894655 3520017 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00545276440680027 22.252714920043946 1.0769004225730896 2.3890796899795532 3731843 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019502029230352492 26.211563110351562 0.9953046917915345 2.6240198850631713 3946180 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009916705676005222 24.55956325531006 0.9308702170848846 2.8094117403030396 4160426 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0011463310685940087 24.84463424682617 0.8417306840419769 3.3315134763717653 4374985 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015455846711120102 26.667346572875978 0.7818159520626068 3.580411171913147 4590020 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001213442670996301 21.837757873535157 0.7846771955490113 3.69096577167511 4807601 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003324164834339172 24.368952560424805 0.7615925669670105 3.85409836769104 5024753 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002153008489403874 24.54848117828369 0.6936024069786072 4.155879783630371 5243826 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0019110841138171963 23.974798965454102 0.6097173273563385 4.612738800048828 5460773 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0012631322722882032 21.793356513977052 0.574005514383316 4.860170030593872 5677027 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002604669604124865 18.093749523162842 0.5664350867271424 5.068063163757325 5894782 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010600405483273789 24.095689582824708 0.5404987186193466 5.354977798461914 6110861 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004640555904188659 21.360534286499025 0.5068005532026291 5.559613275527954 6328556 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017297565762419253 22.471619606018066 0.49355501532554624 5.392927408218384 6543208 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008579140485380776 23.726075744628908 0.5059621751308441 5.597796297073364 6759859 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0010302232200047 24.649771690368652 0.39613227248191835 6.081221008300782 6976618 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0013281644263770432 23.978801918029784 0.3971647650003433 5.952179670333862 7192849 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00075937506990158 22.33769702911377 0.421149942278862 6.040927171707153 7409197 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003487799782305956 25.088565063476562 0.40193946063518526 6.088729906082153 7627623 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017058981342415792 30.367514610290527 0.36645915508270266 6.296652984619141 7846821 0


Pure best response payoff estimated to be 192.905 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (1, 0)
Current player 0 and current strategies (1, 0) took 78.04 seconds to finish estimate with resulting utilities: [188.86   4.32]
Estimating current strategies:  (1, 1)
Current player 0 and current strategies (1, 1) took 130.36 seconds to finish estimate with resulting utilities: [92.24 94.05]
Computing meta_strategies
Exited RRD with total regret 4.635146450708788 that was less than regret lambda 5.0 after 35 iterations 
NEW LAMBDA 4.827586206896552
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1      
    0    50.22           4.32      
    1    188.86          93.14      

 

Player 1 Payoff matrix: 

           0              1      
    0    50.22          188.86      
    1     4.32          93.14      

 

Social Welfare Sum Matrix: 

           0              1      
    0    100.44          193.18      
    1    193.18          186.29      

 

Metagame probabilities: 
Player #0: 0.0257  0.9743  
Player #1: 0.0257  0.9743  
Iteration : 1
Time so far: 6069.437269687653
Approximating Best Response
Training best response:  True 0.095
rtg values check:  (?,) (?, 1)
2023-08-02 14:06:29.566084: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_13/weights_2/Adam_1/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node mlp_13/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_13/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_13/weights_2/Adam_1, mlp_13/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029574557580053805 89.9859733581543 0.5639703810214997 7.907943677902222 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029770574904978274 18.25658760070801 0.6259719371795655 6.872636365890503 229059 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030200532265007497 17.366728115081788 0.6729122996330261 6.335143089294434 448420 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.029970853216946126 25.153435897827148 0.7014596462249756 5.62603530883789 667320 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.032841391116380694 18.03234920501709 0.7907736301422119 5.324055767059326 880496 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03516890443861485 19.822335720062256 0.9163590192794799 4.770871591567993 1095068 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.033108991011977194 18.435456657409667 0.9575677812099457 4.815206575393677 1309551 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.031068437546491624 18.24410810470581 0.9450801193714142 4.417028570175171 1521065 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.027896026149392128 18.348780155181885 0.9554276585578918 4.748412799835205 1733330 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026019100658595563 23.326113891601562 0.9892676532268524 4.727933168411255 1946394 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.025666068121790887 19.643191146850587 1.0332303643226624 4.653287076950074 2157142 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.020877561531960964 22.270335960388184 1.0069905281066895 4.635458993911743 2370286 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017833624687045812 16.873651695251464 0.9685503363609314 4.867393493652344 2584410 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01682205619290471 21.418687438964845 0.9355614662170411 4.782108879089355 2797233 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.015043152682483196 19.980940437316896 0.9579156517982483 4.871387624740601 3012068 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.009705008007586002 17.06755828857422 0.906041830778122 5.351099824905395 3228296 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008125117141753435 19.95011806488037 0.9271464407444 5.061193799972534 3444180 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.005071116797626019 22.777785110473634 0.8831750810146332 5.169517469406128 3660228 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.002741943625733256 19.142957496643067 0.8756449043750762 5.2543848037719725 3877361 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0007470053213182836 17.147747802734376 0.8256065905094147 5.265851020812988 4094334 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0014870938859530725 12.909608364105225 0.8156994342803955 5.4702362537384035 4311339 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0008669763366924599 15.67771282196045 0.7143205642700196 5.526012277603149 4527958 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00026751353507279416 26.253407096862794 0.6309964597225189 5.939644289016724 4744863 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.000539359837421216 16.704943466186524 0.647965931892395 6.382371664047241 4963727 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00044470908760558815 24.707357788085936 0.4788381189107895 6.304473447799682 5180975 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  0.00011500652035465465 21.88130989074707 0.4433607876300812 6.225753355026245 5400257 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005058056049165316 18.658632469177245 0.4957014322280884 6.73052945137024 5619194 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006637960061198101 18.477098655700683 0.4616735428571701 6.679834794998169 5839194 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0015002803582319757 20.63060245513916 0.34526250660419466 7.23028244972229 6056705 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009723242146719713 16.030217361450195 0.485345321893692 6.946487331390381 6275992 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0001391362282447517 18.79778594970703 0.41190881431102755 7.198600625991821 6494462 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0002886571273847949 17.402277660369872 0.4724334329366684 6.823221826553345 6712805 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001104055308678653 15.603351211547851 0.4524138420820236 7.2046843528747555 6932161 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006576565057912376 16.22731943130493 0.4138397932052612 7.6190589427947994 7151935 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -8.075155349160923e-05 17.19688787460327 0.385963636636734 7.735311222076416 7370529 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.003071753599942895 25.798018074035646 0.3638834595680237 8.184388017654419 7589368 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0004391414317069575 18.996216201782225 0.3926600724458694 7.852970790863037 7809259 0


Pure best response payoff estimated to be 131.695 for player 0. 
No returns found for player 1. Defaulting to 0 for pure best response estimate



Updating Empirical Game
Estimating current strategies:  (2, 0)
Current player 0 and current strategies (2, 0) took 79.55 seconds to finish estimate with resulting utilities: [179.13    2.805]
Estimating current strategies:  (2, 1)
Current player 0 and current strategies (2, 1) took 131.08 seconds to finish estimate with resulting utilities: [129.97  48.88]
Estimating current strategies:  (2, 2)
Current player 0 and current strategies (2, 2) took 132.0 seconds to finish estimate with resulting utilities: [54.775 49.94 ]
Computing meta_strategies
Exited RRD with total regret 4.819034492901693 that was less than regret lambda 4.827586206896552 after 92 iterations 
NEW LAMBDA 4.655172413793103
New explore mss minimum:  0.0

 Metagame display:
Player 0 Payoff matrix: 

           0              1              2      
    0    50.22           4.32           2.81      
    1    188.86          93.14          48.88      
    2    179.13          129.97          52.36      

 

Player 1 Payoff matrix: 

           0              1              2      
    0    50.22          188.86          179.13      
    1     4.32          93.14          129.97      
    2     2.81          48.88          52.36      

 

Social Welfare Sum Matrix: 

           0              1              2      
    0    100.44          193.18          181.94      
    1    193.18          186.29          178.85      
    2    181.94          178.85          104.72      

 

Metagame probabilities: 
Player #0: 0.0004  0.2202  0.7794  
Player #1: 0.0004  0.2202  0.7794  
Iteration : 2
Time so far: 14301.20145893097
Approximating Best Response
Training best response:  True 0.09000000000000001
rtg values check:  (?,) (?, 1)
2023-08-02 16:23:41.450681: W tensorflow/c/c_api.cc:300] Operation '{name:'mlp_20/weights_2/Adam_1/Assign' id:5801 op device:{requested: '', assigned: ''} def:{{{node mlp_20/weights_2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=["loc:@mlp_20/weights_2"], _has_manual_control_dependencies=true, use_locking=true, validate_shape=true](mlp_20/weights_2/Adam_1, mlp_20/weights_2/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Setting to fine tune mode: 
Loading previous PPO policy with minimum entropy 0.0

Training each of the policies for 8000000 steps. 
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.01883637374266982 62.49576683044434 0.36775552928447724 9.701421070098878 11000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.039855380356311795 12.802448844909668 0.8326302230358124 6.314605474472046 230826 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03126613218337297 13.690646839141845 0.6838386297225952 6.835869836807251 450023 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03389662280678749 17.398656368255615 0.7921157419681549 6.295159530639649 666462 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.03531137388199568 11.84439582824707 0.8701186299324035 6.029953956604004 883671 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030179838836193084 12.870230007171632 0.7990886628627777 6.397632789611817 1101641 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.030138052254915237 15.081500053405762 0.8556051611900329 5.988663578033448 1317752 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02912008427083492 12.979728031158448 0.9092929482460022 5.7171289920806885 1535971 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.026041755452752113 15.262874794006347 0.8883585691452026 5.724351119995117 1752069 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.02468548994511366 13.761489391326904 0.864309698343277 5.919824457168579 1967048 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.019732592813670635 12.515208625793457 0.7904911041259766 6.165813875198364 2184686 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.017915593553334476 15.714090633392335 0.8628180682659149 5.631813621520996 2400096 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.016537155583500864 13.6335524559021 0.8689646780490875 5.657795333862305 2614110 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011506369523704051 15.495897388458252 0.7143370926380157 6.726194095611572 2829764 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.011781212128698825 12.098474788665772 0.8641033113002777 5.8826151371002195 3048155 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.008436500187963247 12.417679405212402 0.778499549627304 6.002996635437012 3261894 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0068314581410959365 11.783266353607178 0.7492931544780731 6.579085111618042 3476392 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0034658326534554363 18.26429452896118 0.7264747679233551 6.643715476989746 3691007 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0017063887789845467 14.271061038970947 0.7057214796543121 6.630020475387573 3907000 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009974722779588774 15.112708377838135 0.722642433643341 6.514621210098267 4122074 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001142676707240753 13.065742492675781 0.6494941890239716 6.860821914672852 4334925 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.00058090680395253 12.862421798706055 0.5535180330276489 6.9945361614227295 4547809 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006536142274853774 15.723659229278564 0.5242965191602706 7.042530059814453 4757181 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001347905517854997 15.703082180023193 0.5985972940921783 6.741227722167968 4965069 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009288448294682894 16.54461803436279 0.4685764104127884 7.090427017211914 5176024 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006746012935764156 13.727994918823242 0.2531396567821503 7.928305292129517 5387157 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -6.440811758920973e-05 19.719416236877443 0.18078619986772537 8.644625186920166 5598840 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  8.611456432845443e-05 18.273081016540527 0.20413326919078828 7.928412628173828 5810776 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0009395807959663216 13.498461532592774 0.19803527742624283 8.508361721038819 6022930 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0006944738372112625 17.754051399230956 0.19048754423856734 8.296561431884765 6232280 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0003046458346943837 13.613980484008788 0.22359588295221328 8.426385307312012 6444505 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0016061759164585966 18.542007064819337 0.2745542749762535 8.062099695205688 6654536 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.0005475103003846016 14.384571266174316 0.27122092694044114 8.448091125488281 6867859 0
Mean PPO Actor + Value losses, entropy, and kl last 20 updates...and num env steps...and policy constraint weight:  -0.001145087420809432 16.078303623199464 0.20303100794553758 8.89375696182251 7081567 0
